{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating and Error Components Model via GMM\n",
    "### by [Jason DeBacker](https://jasondebacker.com), September 2025\n",
    "\n",
    "This Jupyter Notebook illustrates how estimate the stationary error components model used in DeBacker, Heim, Panousi, Ramnath, and Vidangos (*Brookings Papers on Economic Activity*, Spring 2013) (hereafter, DHPRV).\n",
    "\n",
    "## Moment conditions\n",
    "\n",
    "The theoretical moment conditions are given by:\n",
    "\n",
    "$$\n",
    "g_{a,t,k}(\\Theta_{0}) = E \\left[cov(\\xi^{i}_{a,t},\\xi^{i}_{a+k,t+k})\\right] -cov(a,t,k; \\Theta_{0}) = 0\n",
    "$$\n",
    "\n",
    "Where $cov(a,t,k)$ are the theoretical covariances for individuals of age $a$, in year $t$, at lead $k$.  These are given as:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "cov(a,t,k; \\Theta)  = & \\sigma^{2}_{\\alpha} + \\psi^{k}var(p^{i}_{a,t}) + \\mathbf{1}[k=0](1+\\mathbf{1}[a\\geq2]\\theta^{2}_{1}+\\mathbf{1}[a\\geq3]\\theta^{2}_{2})\\sigma^{2}_{\\varepsilon} + \\\\\n",
    "& \\mathbf{1}[k=1](\\theta_{1}+\\mathbf{1}[a\\geq2]\\theta_{1}\\theta_{2})\\sigma^{2}_{\\varepsilon} + \\\\\n",
    "& \\mathbf{1}[k=2]\\theta_{2}\\sigma^{2}_{\\varepsilon}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The sample analogue to these conditions are:\n",
    "\n",
    "$$\n",
    "\\tilde{g}_{a,t,k}(\\Theta_{0}) = \\frac{\\sum_{i=1}^{n}(\\xi^{i}_{a,t}\\xi^{i}_{a+k,t+k})}{n-1} -cov(a,t,k; \\Theta_{0}) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "Thus the first step in our method of moments estimator is to compute the covariances from the data.\n",
    "\n",
    "## The data\n",
    "\n",
    "We'll begin the exercise in this notebook by reading in a Stata data file that contains the computed sample covariances.  These were constructed from a panel of tax returns spanning the 1987 to 2009 period.  The unit of analysis are males, aged 25-60 with at least \\$2,575 (in 2005\\$) in labor income.  Labor income is defined as wages and salaries plus self-employment income.\n",
    "\n",
    "The covariances are computed on the residuals from regressing the log of labor income on dummy variables for age, separately by year.\n",
    "\n",
    "The covariances are then computed for all possible combinations of age, year, and lead.\n",
    "\n",
    "Let's read in the data and look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages we'll use here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yrvar</th>\n",
       "      <th>hvar</th>\n",
       "      <th>jvar</th>\n",
       "      <th>hpjvar</th>\n",
       "      <th>covvar</th>\n",
       "      <th>wgtvar</th>\n",
       "      <th>covvarfd</th>\n",
       "      <th>wgtvarfd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.438955</td>\n",
       "      <td>329.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.289566</td>\n",
       "      <td>306.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.261525</td>\n",
       "      <td>302.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1987</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.235245</td>\n",
       "      <td>290.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.177744</td>\n",
       "      <td>283.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yrvar  hvar  jvar  hpjvar    covvar  wgtvar  covvarfd  wgtvarfd\n",
       "0   1987     1     0       1  0.438955   329.0       NaN       NaN\n",
       "1   1987     1     1       2  0.289566   306.0       NaN       NaN\n",
       "2   1987     1     2       3  0.261525   302.0       NaN       NaN\n",
       "3   1987     1     3       4  0.235245   290.0       NaN       NaN\n",
       "4   1987     1     4       5  0.177744   283.0       NaN       NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the covariance data\n",
    "covs_data = pd.read_stata('CovsData_labinc_CAL.dta')\n",
    "covs_data.head(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yrvar</th>\n",
       "      <th>hvar</th>\n",
       "      <th>jvar</th>\n",
       "      <th>hpjvar</th>\n",
       "      <th>covvar</th>\n",
       "      <th>wgtvar</th>\n",
       "      <th>covvarfd</th>\n",
       "      <th>wgtvarfd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7084.000000</td>\n",
       "      <td>7084.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1994.866279</td>\n",
       "      <td>15.366279</td>\n",
       "      <td>6.267442</td>\n",
       "      <td>21.633721</td>\n",
       "      <td>0.420291</td>\n",
       "      <td>232.583298</td>\n",
       "      <td>0.013806</td>\n",
       "      <td>213.861374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.652031</td>\n",
       "      <td>9.071330</td>\n",
       "      <td>5.107145</td>\n",
       "      <td>9.071330</td>\n",
       "      <td>0.142618</td>\n",
       "      <td>51.062534</td>\n",
       "      <td>0.059922</td>\n",
       "      <td>45.461811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1987.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089425</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>-0.117507</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1990.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.318830</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>-0.011552</td>\n",
       "      <td>186.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1994.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.403844</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>-0.001709</td>\n",
       "      <td>218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.501245</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>0.009143</td>\n",
       "      <td>245.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.031453</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>0.377932</td>\n",
       "      <td>342.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             yrvar         hvar         jvar       hpjvar       covvar  \\\n",
       "count  7912.000000  7912.000000  7912.000000  7912.000000  7912.000000   \n",
       "mean   1994.866279    15.366279     6.267442    21.633721     0.420291   \n",
       "std       5.652031     9.071330     5.107145     9.071330     0.142618   \n",
       "min    1987.000000     1.000000     0.000000     1.000000     0.089425   \n",
       "25%    1990.000000     8.000000     2.000000    15.000000     0.318830   \n",
       "50%    1994.000000    15.000000     5.000000    22.000000     0.403844   \n",
       "75%    1999.000000    22.000000    10.000000    29.000000     0.501245   \n",
       "max    2009.000000    36.000000    22.000000    36.000000     1.031453   \n",
       "\n",
       "            wgtvar     covvarfd     wgtvarfd  \n",
       "count  7912.000000  7084.000000  7084.000000  \n",
       "mean    232.583298     0.013806   213.861374  \n",
       "std      51.062534     0.059922    45.461811  \n",
       "min      81.000000    -0.117507    76.000000  \n",
       "25%     201.000000    -0.011552   186.000000  \n",
       "50%     239.000000    -0.001709   218.000000  \n",
       "75%     268.000000     0.009143   245.000000  \n",
       "max     367.000000     0.377932   342.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covs_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are 7,912 different covariances measured from the data (we'll ignore the \"fd\", or first-differenced\" covariances, and just estimate the model for the levels here).\n",
    "\n",
    "The variables names are as follows:\n",
    "* `yrvar` = year\n",
    "* `hvar` = age\n",
    "* `jvar` = lead\n",
    "* `hpjvar` = age + lead\n",
    "* `covvar` = covariance\n",
    "* `wgtvar` = sample weights = number of observations used to compute the sample covariance.\n",
    "\n",
    "# Clean the data\n",
    "\n",
    "Now let's make a few adjustments to these data made in DHPRV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yrvar</th>\n",
       "      <th>hvar</th>\n",
       "      <th>jvar</th>\n",
       "      <th>hpjvar</th>\n",
       "      <th>covvar</th>\n",
       "      <th>wgtvar</th>\n",
       "      <th>covvarfd</th>\n",
       "      <th>wgtvarfd</th>\n",
       "      <th>cohort</th>\n",
       "      <th>yearpj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7084.000000</td>\n",
       "      <td>7084.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "      <td>7912.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1994.866279</td>\n",
       "      <td>15.366279</td>\n",
       "      <td>6.267442</td>\n",
       "      <td>21.633721</td>\n",
       "      <td>0.420291</td>\n",
       "      <td>232.583298</td>\n",
       "      <td>0.013806</td>\n",
       "      <td>213.861374</td>\n",
       "      <td>1980.500000</td>\n",
       "      <td>2001.133721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.652031</td>\n",
       "      <td>9.071330</td>\n",
       "      <td>5.107145</td>\n",
       "      <td>9.071330</td>\n",
       "      <td>0.142618</td>\n",
       "      <td>51.062534</td>\n",
       "      <td>0.059922</td>\n",
       "      <td>45.461811</td>\n",
       "      <td>10.059474</td>\n",
       "      <td>5.652031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1987.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089425</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>-0.117507</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1990.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.318830</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>-0.011552</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1994.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.403844</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>-0.001709</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>1980.500000</td>\n",
       "      <td>2002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.501245</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>0.009143</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>1988.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.031453</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>0.377932</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             yrvar         hvar         jvar       hpjvar       covvar  \\\n",
       "count  7912.000000  7912.000000  7912.000000  7912.000000  7912.000000   \n",
       "mean   1994.866279    15.366279     6.267442    21.633721     0.420291   \n",
       "std       5.652031     9.071330     5.107145     9.071330     0.142618   \n",
       "min    1987.000000     1.000000     0.000000     1.000000     0.089425   \n",
       "25%    1990.000000     8.000000     2.000000    15.000000     0.318830   \n",
       "50%    1994.000000    15.000000     5.000000    22.000000     0.403844   \n",
       "75%    1999.000000    22.000000    10.000000    29.000000     0.501245   \n",
       "max    2009.000000    36.000000    22.000000    36.000000     1.031453   \n",
       "\n",
       "            wgtvar     covvarfd     wgtvarfd       cohort       yearpj  \n",
       "count  7912.000000  7084.000000  7084.000000  7912.000000  7912.000000  \n",
       "mean    232.583298     0.013806   213.861374  1980.500000  2001.133721  \n",
       "std      51.062534     0.059922    45.461811    10.059474     5.652031  \n",
       "min      81.000000    -0.117507    76.000000  1952.000000  1987.000000  \n",
       "25%     201.000000    -0.011552   186.000000  1973.000000  1997.000000  \n",
       "50%     239.000000    -0.001709   218.000000  1980.500000  2002.000000  \n",
       "75%     268.000000     0.009143   245.000000  1988.000000  2006.000000  \n",
       "max     367.000000     0.377932   342.000000  2009.000000  2009.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the data\n",
    "# drop if missing value for the covariance\n",
    "covs_data = covs_data[covs_data.covvar.isnull() != True ]\n",
    "\n",
    "\n",
    "# Define sample period, range of ages, etc.\n",
    "year1 = 1987\n",
    "year2 = 1988\n",
    "yearT = 2009\n",
    "minAge = 25\n",
    "maxAge = 60\n",
    "# Define cohorts - those who were 25 in 1952 were 60 in 1987\n",
    "cohort1 = 1952  # first cohort\n",
    "cohortT = 2009  # last cohort\n",
    "# Put these parameters of the data in a tuple\n",
    "sample_params = (year1, yearT, cohort1, cohortT, minAge, maxAge)\n",
    "\n",
    "# * Create dummy variable Dj0, which equals 1 if jvar==0 and equals 0 otherwise\n",
    "covs_data['Dj0'] = covs_data['jvar'] == 0\n",
    "\n",
    "# ********************************************************************************\n",
    "# * Pick observations used for estimation\n",
    "# ********************************************************************************\n",
    "covs_data['cohort'] = covs_data['yrvar'] - covs_data['hvar'] + 1\n",
    "covs_data[(covs_data['cohort'] >= cohort1) & (covs_data['cohort'] <= cohortT)]\n",
    "covs_data[(covs_data['yrvar'] >= year1) & (covs_data['yrvar'] <= yearT)]\n",
    "\n",
    "# * Make sure that \"future\" years used to compute autocovariances do not fall\n",
    "# * outside of [year1,yearT] range:\n",
    "covs_data['yearpj'] = covs_data['cohort'] -1 + covs_data['hvar'] + covs_data['jvar']\n",
    "covs_data[(covs_data['yearpj'] >= year1) & (covs_data['yearpj'] <= yearT)]\n",
    "covs_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears these restrictions make no difference here - we had no missing values, no observations outside the potential ranges of values.\n",
    "\n",
    "## Theoretical covariances\n",
    "\n",
    "Now, let's define those theoretical covariances.  Recall, these are given by:\n",
    "\n",
    "  \\begin{equation*}\n",
    "    \\begin{split}\n",
    "      cov(a,t,k; \\Theta) & = \\sigma^{2}_{\\alpha} + \\psi^{k}var(p^{i}_{a,t}) + \\\\ & \\quad\\quad\\quad \\mathbf{1}[k=0](1+\\mathbf{1}[a\\geq2]\\theta^{2}_{1}+\\mathbf{1}[a\\geq3]\\theta^{2}_{2})\\sigma^{2}_{\\varepsilon} + \\\\\n",
    "      & \\quad\\quad\\quad \\mathbf{1}[k=1](\\theta_{1}+\\mathbf{1}[a\\geq2]\\theta_{1}\\theta_{2})\\sigma^{2}_{\\varepsilon} + \\\\\n",
    "      & \\quad\\quad\\quad \\mathbf{1}[k=2]\\theta_{2}\\sigma^{2}_{\\varepsilon}\n",
    "    \\end{split}\n",
    "  \\end{equation*}\n",
    "  \n",
    "There is a little more detail needed to compute $var(p^{i}_{a,t})$ (you can refer to DHPRV for more detail on this, but it can also be seen in the function we'll define below).  Let's call this function that returns the theoretical covariances `model_cov` and define it as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute theoretical covariances.\n",
    "def model_cov(Theta, sample_params):\n",
    "    '''\n",
    "    Compute model implied covariances given parameters.\n",
    "\n",
    "    Args:\n",
    "        Theta: A length 6 tuple, model parameters (sigma2_alpha,\n",
    "               sigma2_eta, sigma2_eps, psi, theta1, theta2)\n",
    "        args: A length X tuple, integers for age ranges\n",
    "\n",
    "    Returns:\n",
    "        covs_model: A dataframe with covariances for all years (t),\n",
    "                    ages (h), and leads (k)\n",
    "\n",
    "    '''\n",
    "\n",
    "    sigma2_alpha, sigma2_eta, sigma2_eps, psi, theta1, theta2 = Theta\n",
    "    year1, yearT, cohort1, cohortT, minAge, maxAge = sample_params\n",
    "\n",
    "    # Define age range - ages 25-60, inclusive is 36 years\n",
    "    ageRange = maxAge - minAge + 1\n",
    "\n",
    "    # compute var(p^{i}_{a,t})\n",
    "    varp = np.empty((cohortT - cohort1, ageRange + 1))\n",
    "    # add one to age range becaue python index 0, but use ages from data that start at 1\n",
    "    for cohort in range(cohort1, cohortT + 1):\n",
    "        for age in range(1, ageRange + 1):\n",
    "            yr = cohort + age - 1\n",
    "            yrm1  = yr - 1\n",
    "            yrm2  = yr - 2\n",
    "            agem1 = age - 1\n",
    "            if yr <= yearT:\n",
    "                if yr == year1:\n",
    "                    varp[yr - year1, age] = sigma2_eta * (1 - psi ** (2 * age)) / (1 - psi ** 2)\n",
    "                if age == 1:\n",
    "                    varp[yr - year1, age] = sigma2_eta\n",
    "                if age == 2:\n",
    "                    varp[yr - year1, age] = (psi ** 2) * varp[yrm1 - year1, agem1] + sigma2_eta\n",
    "                if age >= 3:\n",
    "                    varp[yr - year1, age] = (psi ** 2) * varp[yrm1 - year1, agem1] + sigma2_eta\n",
    "\n",
    "    # Compute theoretical covariancea\n",
    "    covs_dict = {'year': [], 'age': [], 'lead': [], 'cov': []}\n",
    "    for t in range(year1, yearT + 1):\n",
    "        maxK = yearT - t\n",
    "        for h in range(1, ageRange + 1):\n",
    "            for k in range(maxK + 1):\n",
    "                cov = (sigma2_alpha + ((psi ** k) * varp[t - year1, h]) +\n",
    "                       ((k==0) * (1 + ((h>=2) * (theta1 ** 2)) + ((h>=3) * (theta2 ** 2))) * sigma2_eps) +\n",
    "                       ((k==1) * (theta1 + ((h>=2) * theta1 * theta2)) * sigma2_eps) +\n",
    "                       ((k==2) * theta2 * sigma2_eps))\n",
    "                       # update dictionary\n",
    "                covs_dict['cov'].append(cov)\n",
    "                covs_dict['year'].append(t)\n",
    "                covs_dict['age'].append(h)\n",
    "                covs_dict['lead'].append(k)\n",
    "\n",
    "    # create dataframe with dictionary\n",
    "    covs_model = pd.DataFrame(covs_dict)\n",
    "\n",
    "    return covs_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the statistical objective function\n",
    "\n",
    "Now we have the covariances from the data and a function that returns the model covariances for a given set of parameters.  Now we just need to define a statistical objective function that can be minimized.  In our case, the moment conditions can be represented by a set of residuals (the difference between the data and model moments).  We can thus use a nonlinear least squares estimator.  \n",
    "\n",
    "To estimate the model parameters via nonlinear least squares, we'll call the `scipy.optimize.least_squares()` method.  The function that is passed as an argument to this method is one that returns the vector of residuals. So we'll need our function to return the errors in these moment conditions.\n",
    "\n",
    "We name this function `resids()` and define it as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to return residuals - for nonlinear least squares estimator\n",
    "def resids(Theta, covs_data, sample_params):\n",
    "    '''\n",
    "    Compute the (weighted) residuals from the moment conditions.\n",
    "\n",
    "    Args:\n",
    "        covs_data: Dataframe of covariances from the data\n",
    "        covs_model: Dataframe of model implied covariances\n",
    "\n",
    "    Returns:\n",
    "        resids_wgt: Vector of weighted differences between the model and data moments\n",
    "    '''\n",
    "    # Find the model covariances impllied by parameter vector Theta\n",
    "    covs_model = model_cov(Theta, sample_params)\n",
    "    # rename the covariance variable in the model dataframe\n",
    "    covs_model = covs_model.rename(columns={'cov': 'cov_model'})\n",
    "\n",
    "    # merge the two dataframes together\n",
    "    data_model = covs_data.merge(covs_model, how='left', left_on=['yrvar', 'hvar', 'jvar'],\n",
    "                                 right_on=['year', 'age', 'lead'], copy=True, indicator=False)\n",
    "\n",
    "    # compute the weighted differences between data and model covariances\n",
    "    # Note that use sqrt of the weight since it will be squares in f(x)=r^Tr\n",
    "    resids_wgt = np.array(((data_model['covvar'] - data_model['cov_model']) *\n",
    "                          (data_model['wgtvar'] ** (1 / 2))).values)\n",
    "\n",
    "\n",
    "    return resids_wgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the optimization routine\n",
    "\n",
    "With the moment conditions not defined, we are almost ready to call our least squares estimator.  But before we do, we'll need to provide our initial guesses and set any bounds on the parameter estimates (if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial guesses at the parameter values\n",
    "# Parameters are in order, sigma2_alpha, sigma2_eta, sigma2_eps, psi, theta1, theta2\n",
    "Theta0 = (0.1, 0.001, 0.1, 0.9, 0.0, 0.0)\n",
    "\n",
    "# Set bounds on parameters\n",
    "bnds = ([0, 0, 0, -np.inf, -np.inf, -np.inf],\n",
    "        [np.inf, np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "\n",
    "# Call minimzer to minimize the sum of square residuals\n",
    "# note the comma after covs_data - this is critical to pass the argument in the right way\n",
    "nls_results = opt.least_squares(resids, Theta0, bounds=bnds, method='trf', args=(covs_data, sample_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We'll now view the results.  The object returned from `scipy.optimize.least_squares()` is a dictionary with 12 key value pairs.  The dictionary includes the parameter estimates (key = `'x'`) and a lot of other information about the estimation such as whether the estimator converged, the number of function evaluations, the gradient and jacobian at the parameters estimate, and more.\n",
    "\n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19679008, 0.02927674, 0.18261447, 0.96234267, 0.22857777,\n",
       "       0.12308686])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show nonlinear least squares estimation results\n",
    "nls_results['x']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An alternative estimator\n",
    "\n",
    "We can also use a minimizer that is not specifically for a least squares problem.  In this case, we'll need to define our statistical objective function that we want to minimize.  We'll call this `ssr()` since it's the sum of squared residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the statistical objective function\n",
    "def ssr(Theta, covs_data, sample_params):\n",
    "    '''\n",
    "    Compute the (weighted) sum of squared residuals from the moment conditions.\n",
    "\n",
    "    Args:\n",
    "        covs_data: Dataframe of covariances from the data\n",
    "        covs_model: Dataframe of model implied covariances\n",
    "\n",
    "    Returns:\n",
    "        sumsq: Weighted sum of squared residuals showing difference\n",
    "                between data and model covariances.\n",
    "    '''\n",
    "\n",
    "    # Find the model covariances impllied by parameter vector Theta\n",
    "    covs_model = model_cov(Theta, sample_params)\n",
    "\n",
    "    # merge the two dataframes together\n",
    "    data_model = covs_data.merge(covs_model, how='left', left_on=['yrvar', 'hvar', 'jvar'],\n",
    "                                 right_on=['year', 'age', 'lead'], copy=True, indicator=False)\n",
    "\n",
    "    # compute differences between data and model covariances\n",
    "    data_model['resid'] = data_model['covvar'] - data_model['cov']\n",
    "\n",
    "    # compute weighted SSR\n",
    "    sumsq = ((data_model['resid'] ** 2) * (data_model['wgtvar'])).sum()\n",
    "\n",
    "    return sumsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to call the `ssr()` function through a minimization routine.  Here, we'll use a modified BFGS algorithm that will allow for bounds on the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 8055.905176093184\n",
       "        x: [ 1.968e-01  2.928e-02  1.826e-01  9.623e-01  2.286e-01\n",
       "             1.231e-01]\n",
       "      nit: 46\n",
       "      jac: [ 2.456e-03  8.731e-03  1.364e-03  3.183e-03  3.638e-04\n",
       "             1.819e-04]\n",
       "     nfev: 406\n",
       "     njev: 58\n",
       " hess_inv: <6x6 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial guesses at the parameter values\n",
    "Theta0 = (0.1, 0.001, 0.1, 0.9, 0.0, 0.0)\n",
    "\n",
    "# Set bounds on parameters\n",
    "bnds = ((0, None), (0, None), (0, None),\n",
    "        (None, None), (None, None), (None, None))\n",
    "\n",
    "# Use a general purpose minimizer to minimize the SSR\n",
    "min_results = opt.minimize(ssr, Theta0, args=(covs_data, sample_params),\n",
    "                           method=\"L-BFGS-B\", bounds=bnds, tol=1e-15)\n",
    "\n",
    "min_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      " & Estimate \\\\\n",
      "\\midrule\n",
      "\\sigma^2_\\alpha & 0.196790 \\\\\n",
      "$\\sigma^2_\\eta$ & 0.029277 \\\\\n",
      "$\\sigma^2_\\varepsilon$ & 0.182615 \\\\\n",
      "$\\psi$ & 0.962343 \\\\\n",
      "$\\theta_1$ & 0.228578 \\\\\n",
      "$\\theta_2$ & 0.123085 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_results['x']\n",
    "# put in a dict and then Pandas DataFrame\n",
    "var_name_list = [r'\\sigma^2_\\alpha', r'$\\sigma^2_\\eta$', r'$\\sigma^2_\\varepsilon$', r'$\\psi$', r'$\\theta_1$', r'$\\theta_2$']\n",
    "# create dictionary\n",
    "est_dict = {}\n",
    "for i, name in enumerate(var_name_list):\n",
    "    est_dict[name] = min_results['x'][i]\n",
    "# create DataFrame\n",
    "est_df = pd.DataFrame(est_dict, index=['Estimate']).T\n",
    "print(est_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We'll omit the discussion of standard errors, but they can be computed using either the theoretical standard errors (which will rely on the Jacobian of the function) or via boostrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Estimate',\n",
       " 'T',\n",
       " '_AXIS_LEN',\n",
       " '_AXIS_ORDERS',\n",
       " '_AXIS_TO_AXIS_NUMBER',\n",
       " '_HANDLED_TYPES',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__annotations__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_ufunc__',\n",
       " '__arrow_c_stream__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__dataframe__',\n",
       " '__dataframe_consortium_standard__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__divmod__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__finalize__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__ifloordiv__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pandas_priority__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdivmod__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__round__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_accessors',\n",
       " '_accum_func',\n",
       " '_agg_examples_doc',\n",
       " '_agg_see_also_doc',\n",
       " '_align_for_op',\n",
       " '_align_frame',\n",
       " '_align_series',\n",
       " '_append',\n",
       " '_arith_method',\n",
       " '_arith_method_with_reindex',\n",
       " '_as_manager',\n",
       " '_attrs',\n",
       " '_box_col_values',\n",
       " '_can_fast_transpose',\n",
       " '_check_inplace_and_allows_duplicate_labels',\n",
       " '_check_is_chained_assignment_possible',\n",
       " '_check_label_or_level_ambiguity',\n",
       " '_check_setitem_copy',\n",
       " '_clear_item_cache',\n",
       " '_clip_with_one_bound',\n",
       " '_clip_with_scalar',\n",
       " '_cmp_method',\n",
       " '_combine_frame',\n",
       " '_consolidate',\n",
       " '_consolidate_inplace',\n",
       " '_construct_axes_dict',\n",
       " '_construct_result',\n",
       " '_constructor',\n",
       " '_constructor_from_mgr',\n",
       " '_constructor_sliced',\n",
       " '_constructor_sliced_from_mgr',\n",
       " '_create_data_for_split_and_tight_to_dict',\n",
       " '_data',\n",
       " '_deprecate_downcast',\n",
       " '_dir_additions',\n",
       " '_dir_deletions',\n",
       " '_dispatch_frame_op',\n",
       " '_drop_axis',\n",
       " '_drop_labels_or_levels',\n",
       " '_ensure_valid_index',\n",
       " '_find_valid_index',\n",
       " '_flags',\n",
       " '_flex_arith_method',\n",
       " '_flex_cmp_method',\n",
       " '_from_arrays',\n",
       " '_from_mgr',\n",
       " '_get_agg_axis',\n",
       " '_get_axis',\n",
       " '_get_axis_name',\n",
       " '_get_axis_number',\n",
       " '_get_axis_resolvers',\n",
       " '_get_block_manager_axis',\n",
       " '_get_bool_data',\n",
       " '_get_cleaned_column_resolvers',\n",
       " '_get_column_array',\n",
       " '_get_index_resolvers',\n",
       " '_get_item_cache',\n",
       " '_get_label_or_level_values',\n",
       " '_get_numeric_data',\n",
       " '_get_value',\n",
       " '_get_values_for_csv',\n",
       " '_getitem_bool_array',\n",
       " '_getitem_multilevel',\n",
       " '_getitem_nocopy',\n",
       " '_getitem_slice',\n",
       " '_gotitem',\n",
       " '_hidden_attrs',\n",
       " '_indexed_same',\n",
       " '_info_axis',\n",
       " '_info_axis_name',\n",
       " '_info_axis_number',\n",
       " '_info_repr',\n",
       " '_init_mgr',\n",
       " '_inplace_method',\n",
       " '_internal_names',\n",
       " '_internal_names_set',\n",
       " '_is_copy',\n",
       " '_is_homogeneous_type',\n",
       " '_is_label_or_level_reference',\n",
       " '_is_label_reference',\n",
       " '_is_level_reference',\n",
       " '_is_mixed_type',\n",
       " '_is_view',\n",
       " '_is_view_after_cow_rules',\n",
       " '_iset_item',\n",
       " '_iset_item_mgr',\n",
       " '_iset_not_inplace',\n",
       " '_item_cache',\n",
       " '_iter_column_arrays',\n",
       " '_ixs',\n",
       " '_logical_func',\n",
       " '_logical_method',\n",
       " '_maybe_align_series_as_frame',\n",
       " '_maybe_cache_changed',\n",
       " '_maybe_update_cacher',\n",
       " '_metadata',\n",
       " '_mgr',\n",
       " '_min_count_stat_function',\n",
       " '_needs_reindex_multi',\n",
       " '_pad_or_backfill',\n",
       " '_protect_consolidate',\n",
       " '_reduce',\n",
       " '_reduce_axis1',\n",
       " '_reindex_axes',\n",
       " '_reindex_multi',\n",
       " '_reindex_with_indexers',\n",
       " '_rename',\n",
       " '_replace_columnwise',\n",
       " '_repr_data_resource_',\n",
       " '_repr_fits_horizontal_',\n",
       " '_repr_fits_vertical_',\n",
       " '_repr_html_',\n",
       " '_repr_latex_',\n",
       " '_reset_cache',\n",
       " '_reset_cacher',\n",
       " '_sanitize_column',\n",
       " '_series',\n",
       " '_set_axis',\n",
       " '_set_axis_name',\n",
       " '_set_axis_nocheck',\n",
       " '_set_is_copy',\n",
       " '_set_item',\n",
       " '_set_item_frame_value',\n",
       " '_set_item_mgr',\n",
       " '_set_value',\n",
       " '_setitem_array',\n",
       " '_setitem_frame',\n",
       " '_setitem_slice',\n",
       " '_shift_with_freq',\n",
       " '_should_reindex_frame_op',\n",
       " '_slice',\n",
       " '_stat_function',\n",
       " '_stat_function_ddof',\n",
       " '_take_with_is_copy',\n",
       " '_to_dict_of_blocks',\n",
       " '_to_latex_via_styler',\n",
       " '_typ',\n",
       " '_update_inplace',\n",
       " '_validate_dtype',\n",
       " '_values',\n",
       " '_where',\n",
       " 'abs',\n",
       " 'add',\n",
       " 'add_prefix',\n",
       " 'add_suffix',\n",
       " 'agg',\n",
       " 'aggregate',\n",
       " 'align',\n",
       " 'all',\n",
       " 'any',\n",
       " 'apply',\n",
       " 'applymap',\n",
       " 'asfreq',\n",
       " 'asof',\n",
       " 'assign',\n",
       " 'astype',\n",
       " 'at',\n",
       " 'at_time',\n",
       " 'attrs',\n",
       " 'axes',\n",
       " 'backfill',\n",
       " 'between_time',\n",
       " 'bfill',\n",
       " 'bool',\n",
       " 'boxplot',\n",
       " 'clip',\n",
       " 'columns',\n",
       " 'combine',\n",
       " 'combine_first',\n",
       " 'compare',\n",
       " 'convert_dtypes',\n",
       " 'copy',\n",
       " 'corr',\n",
       " 'corrwith',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'describe',\n",
       " 'diff',\n",
       " 'div',\n",
       " 'divide',\n",
       " 'dot',\n",
       " 'drop',\n",
       " 'drop_duplicates',\n",
       " 'droplevel',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'duplicated',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'equals',\n",
       " 'eval',\n",
       " 'ewm',\n",
       " 'expanding',\n",
       " 'explode',\n",
       " 'ffill',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'first_valid_index',\n",
       " 'flags',\n",
       " 'floordiv',\n",
       " 'from_dict',\n",
       " 'from_records',\n",
       " 'ge',\n",
       " 'get',\n",
       " 'groupby',\n",
       " 'gt',\n",
       " 'head',\n",
       " 'hist',\n",
       " 'iat',\n",
       " 'idxmax',\n",
       " 'idxmin',\n",
       " 'iloc',\n",
       " 'index',\n",
       " 'infer_objects',\n",
       " 'info',\n",
       " 'insert',\n",
       " 'interpolate',\n",
       " 'isetitem',\n",
       " 'isin',\n",
       " 'isna',\n",
       " 'isnull',\n",
       " 'items',\n",
       " 'iterrows',\n",
       " 'itertuples',\n",
       " 'join',\n",
       " 'keys',\n",
       " 'kurt',\n",
       " 'kurtosis',\n",
       " 'last',\n",
       " 'last_valid_index',\n",
       " 'le',\n",
       " 'loc',\n",
       " 'lt',\n",
       " 'map',\n",
       " 'mask',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'melt',\n",
       " 'memory_usage',\n",
       " 'merge',\n",
       " 'min',\n",
       " 'mod',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'multiply',\n",
       " 'ndim',\n",
       " 'ne',\n",
       " 'nlargest',\n",
       " 'notna',\n",
       " 'notnull',\n",
       " 'nsmallest',\n",
       " 'nunique',\n",
       " 'pad',\n",
       " 'pct_change',\n",
       " 'pipe',\n",
       " 'pivot',\n",
       " 'pivot_table',\n",
       " 'plot',\n",
       " 'pop',\n",
       " 'pow',\n",
       " 'prod',\n",
       " 'product',\n",
       " 'quantile',\n",
       " 'query',\n",
       " 'radd',\n",
       " 'rank',\n",
       " 'rdiv',\n",
       " 'reindex',\n",
       " 'reindex_like',\n",
       " 'rename',\n",
       " 'rename_axis',\n",
       " 'reorder_levels',\n",
       " 'replace',\n",
       " 'resample',\n",
       " 'reset_index',\n",
       " 'rfloordiv',\n",
       " 'rmod',\n",
       " 'rmul',\n",
       " 'rolling',\n",
       " 'round',\n",
       " 'rpow',\n",
       " 'rsub',\n",
       " 'rtruediv',\n",
       " 'sample',\n",
       " 'select_dtypes',\n",
       " 'sem',\n",
       " 'set_axis',\n",
       " 'set_flags',\n",
       " 'set_index',\n",
       " 'shape',\n",
       " 'shift',\n",
       " 'size',\n",
       " 'skew',\n",
       " 'sort_index',\n",
       " 'sort_values',\n",
       " 'squeeze',\n",
       " 'stack',\n",
       " 'std',\n",
       " 'style',\n",
       " 'sub',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'swaplevel',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'to_clipboard',\n",
       " 'to_csv',\n",
       " 'to_dict',\n",
       " 'to_excel',\n",
       " 'to_feather',\n",
       " 'to_gbq',\n",
       " 'to_hdf',\n",
       " 'to_html',\n",
       " 'to_json',\n",
       " 'to_latex',\n",
       " 'to_markdown',\n",
       " 'to_numpy',\n",
       " 'to_orc',\n",
       " 'to_parquet',\n",
       " 'to_period',\n",
       " 'to_pickle',\n",
       " 'to_records',\n",
       " 'to_sql',\n",
       " 'to_stata',\n",
       " 'to_string',\n",
       " 'to_timestamp',\n",
       " 'to_xarray',\n",
       " 'to_xml',\n",
       " 'transform',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncate',\n",
       " 'tz_convert',\n",
       " 'tz_localize',\n",
       " 'unstack',\n",
       " 'update',\n",
       " 'value_counts',\n",
       " 'values',\n",
       " 'var',\n",
       " 'where',\n",
       " 'xs']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(est_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "est_df.plot(kind='barh', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scipy.optimize in scipy:\n",
      "\n",
      "NAME\n",
      "    scipy.optimize\n",
      "\n",
      "DESCRIPTION\n",
      "    =====================================================\n",
      "    Optimization and root finding (:mod:`scipy.optimize`)\n",
      "    =====================================================\n",
      "\n",
      "    .. currentmodule:: scipy.optimize\n",
      "\n",
      "    .. toctree::\n",
      "       :hidden:\n",
      "\n",
      "       optimize.cython_optimize\n",
      "\n",
      "    SciPy ``optimize`` provides functions for minimizing (or maximizing)\n",
      "    objective functions, possibly subject to constraints. It includes\n",
      "    solvers for nonlinear problems (with support for both local and global\n",
      "    optimization algorithms), linear programming, constrained\n",
      "    and nonlinear least-squares, root finding, and curve fitting.\n",
      "\n",
      "    Common functions and objects, shared across different solvers, are:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       show_options - Show specific options optimization solvers.\n",
      "       OptimizeResult - The optimization result returned by some optimizers.\n",
      "       OptimizeWarning - The optimization encountered problems.\n",
      "\n",
      "\n",
      "    Optimization\n",
      "    ============\n",
      "\n",
      "    Scalar functions optimization\n",
      "    -----------------------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       minimize_scalar - Interface for minimizers of univariate functions\n",
      "\n",
      "    The `minimize_scalar` function supports the following methods:\n",
      "\n",
      "    .. toctree::\n",
      "\n",
      "       optimize.minimize_scalar-brent\n",
      "       optimize.minimize_scalar-bounded\n",
      "       optimize.minimize_scalar-golden\n",
      "\n",
      "    Local (multivariate) optimization\n",
      "    ---------------------------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       minimize - Interface for minimizers of multivariate functions.\n",
      "\n",
      "    The `minimize` function supports the following methods:\n",
      "\n",
      "    .. toctree::\n",
      "\n",
      "       optimize.minimize-neldermead\n",
      "       optimize.minimize-powell\n",
      "       optimize.minimize-cg\n",
      "       optimize.minimize-bfgs\n",
      "       optimize.minimize-newtoncg\n",
      "       optimize.minimize-lbfgsb\n",
      "       optimize.minimize-tnc\n",
      "       optimize.minimize-cobyla\n",
      "       optimize.minimize-cobyqa\n",
      "       optimize.minimize-slsqp\n",
      "       optimize.minimize-trustconstr\n",
      "       optimize.minimize-dogleg\n",
      "       optimize.minimize-trustncg\n",
      "       optimize.minimize-trustkrylov\n",
      "       optimize.minimize-trustexact\n",
      "\n",
      "    Constraints are passed to `minimize` function as a single object or\n",
      "    as a list of objects from the following classes:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       NonlinearConstraint - Class defining general nonlinear constraints.\n",
      "       LinearConstraint - Class defining general linear constraints.\n",
      "\n",
      "    Simple bound constraints are handled separately and there is a special class\n",
      "    for them:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       Bounds - Bound constraints.\n",
      "\n",
      "    Quasi-Newton strategies implementing `HessianUpdateStrategy`\n",
      "    interface can be used to approximate the Hessian in `minimize`\n",
      "    function (available only for the 'trust-constr' method). Available\n",
      "    quasi-Newton methods implementing this interface are:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       BFGS - Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n",
      "       SR1 - Symmetric-rank-1 Hessian update strategy.\n",
      "\n",
      "    .. _global_optimization:\n",
      "\n",
      "    Global optimization\n",
      "    -------------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       basinhopping - Basinhopping stochastic optimizer.\n",
      "       brute - Brute force searching optimizer.\n",
      "       differential_evolution - Stochastic optimizer using differential evolution.\n",
      "\n",
      "       shgo - Simplicial homology global optimizer.\n",
      "       dual_annealing - Dual annealing stochastic optimizer.\n",
      "       direct - DIRECT (Dividing Rectangles) optimizer.\n",
      "\n",
      "    Least-squares and curve fitting\n",
      "    ===============================\n",
      "\n",
      "    Nonlinear least-squares\n",
      "    -----------------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       least_squares - Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "\n",
      "    Linear least-squares\n",
      "    --------------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       nnls - Linear least-squares problem with non-negativity constraint.\n",
      "       lsq_linear - Linear least-squares problem with bound constraints.\n",
      "       isotonic_regression - Least squares problem of isotonic regression via PAVA.\n",
      "\n",
      "    Curve fitting\n",
      "    -------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       curve_fit -- Fit curve to a set of points.\n",
      "\n",
      "    Root finding\n",
      "    ============\n",
      "\n",
      "    Scalar functions\n",
      "    ----------------\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       root_scalar - Unified interface for nonlinear solvers of scalar functions.\n",
      "       brentq - quadratic interpolation Brent method.\n",
      "       brenth - Brent method, modified by Harris with hyperbolic extrapolation.\n",
      "       ridder - Ridder's method.\n",
      "       bisect - Bisection method.\n",
      "       newton - Newton's method (also Secant and Halley's methods).\n",
      "       toms748 - Alefeld, Potra & Shi Algorithm 748.\n",
      "       RootResults - The root finding result returned by some root finders.\n",
      "\n",
      "    The `root_scalar` function supports the following methods:\n",
      "\n",
      "    .. toctree::\n",
      "\n",
      "       optimize.root_scalar-brentq\n",
      "       optimize.root_scalar-brenth\n",
      "       optimize.root_scalar-bisect\n",
      "       optimize.root_scalar-ridder\n",
      "       optimize.root_scalar-newton\n",
      "       optimize.root_scalar-toms748\n",
      "       optimize.root_scalar-secant\n",
      "       optimize.root_scalar-halley\n",
      "\n",
      "\n",
      "\n",
      "    The table below lists situations and appropriate methods, along with\n",
      "    *asymptotic* convergence rates per iteration (and per function evaluation)\n",
      "    for successful convergence to a simple root(*).\n",
      "    Bisection is the slowest of them all, adding one bit of accuracy for each\n",
      "    function evaluation, but is guaranteed to converge.\n",
      "    The other bracketing methods all (eventually) increase the number of accurate\n",
      "    bits by about 50% for every function evaluation.\n",
      "    The derivative-based methods, all built on `newton`, can converge quite quickly\n",
      "    if the initial value is close to the root.  They can also be applied to\n",
      "    functions defined on (a subset of) the complex plane.\n",
      "\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | Domain of f | Bracket? |    Derivatives?      | Solvers     |        Convergence           |\n",
      "    +             +          +----------+-----------+             +-------------+----------------+\n",
      "    |             |          | `fprime` | `fprime2` |             | Guaranteed? |  Rate(s)(*)    |\n",
      "    +=============+==========+==========+===========+=============+=============+================+\n",
      "    | `R`         | Yes      | N/A      | N/A       | - bisection | - Yes       | - 1 \"Linear\"   |\n",
      "    |             |          |          |           | - brentq    | - Yes       | - >=1, <= 1.62 |\n",
      "    |             |          |          |           | - brenth    | - Yes       | - >=1, <= 1.62 |\n",
      "    |             |          |          |           | - ridder    | - Yes       | - 2.0 (1.41)   |\n",
      "    |             |          |          |           | - toms748   | - Yes       | - 2.7 (1.65)   |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | No       | No        | secant      | No          | 1.62 (1.62)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | Yes      | No        | newton      | No          | 2.00 (1.41)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | Yes      | Yes       | halley      | No          | 3.00 (1.44)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "\n",
      "    .. seealso::\n",
      "\n",
      "       `scipy.optimize.cython_optimize` -- Typed Cython versions of root finding functions\n",
      "\n",
      "    Fixed point finding:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       fixed_point - Single-variable fixed-point solver.\n",
      "\n",
      "    Multidimensional\n",
      "    ----------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       root - Unified interface for nonlinear solvers of multivariate functions.\n",
      "\n",
      "    The `root` function supports the following methods:\n",
      "\n",
      "    .. toctree::\n",
      "\n",
      "       optimize.root-hybr\n",
      "       optimize.root-lm\n",
      "       optimize.root-broyden1\n",
      "       optimize.root-broyden2\n",
      "       optimize.root-anderson\n",
      "       optimize.root-linearmixing\n",
      "       optimize.root-diagbroyden\n",
      "       optimize.root-excitingmixing\n",
      "       optimize.root-krylov\n",
      "       optimize.root-dfsane\n",
      "\n",
      "    Elementwise Minimization and Root Finding\n",
      "    =========================================\n",
      "\n",
      "    .. toctree::\n",
      "       :maxdepth: 3\n",
      "\n",
      "       optimize.elementwise\n",
      "\n",
      "    Linear programming / MILP\n",
      "    =========================\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       milp -- Mixed integer linear programming.\n",
      "       linprog -- Unified interface for minimizers of linear programming problems.\n",
      "\n",
      "    The `linprog` function supports the following methods:\n",
      "\n",
      "    .. toctree::\n",
      "\n",
      "       optimize.linprog-simplex\n",
      "       optimize.linprog-interior-point\n",
      "       optimize.linprog-revised_simplex\n",
      "       optimize.linprog-highs-ipm\n",
      "       optimize.linprog-highs-ds\n",
      "       optimize.linprog-highs\n",
      "\n",
      "    The simplex, interior-point, and revised simplex methods support callback\n",
      "    functions, such as:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       linprog_verbose_callback -- Sample callback function for linprog (simplex).\n",
      "\n",
      "    Assignment problems\n",
      "    ===================\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       linear_sum_assignment -- Solves the linear-sum assignment problem.\n",
      "       quadratic_assignment -- Solves the quadratic assignment problem.\n",
      "\n",
      "    The `quadratic_assignment` function supports the following methods:\n",
      "\n",
      "    .. toctree::\n",
      "\n",
      "       optimize.qap-faq\n",
      "       optimize.qap-2opt\n",
      "\n",
      "    Utilities\n",
      "    =========\n",
      "\n",
      "    Finite-difference approximation\n",
      "    -------------------------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       approx_fprime - Approximate the gradient of a scalar function.\n",
      "       check_grad - Check the supplied derivative using finite differences.\n",
      "\n",
      "\n",
      "    Line search\n",
      "    -----------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       bracket - Bracket a minimum, given two starting points.\n",
      "       line_search - Return a step that satisfies the strong Wolfe conditions.\n",
      "\n",
      "    Hessian approximation\n",
      "    ---------------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       LbfgsInvHessProduct - Linear operator for L-BFGS approximate inverse Hessian.\n",
      "       HessianUpdateStrategy - Interface for implementing Hessian update strategies\n",
      "\n",
      "    Benchmark problems\n",
      "    ------------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       rosen - The Rosenbrock function.\n",
      "       rosen_der - The derivative of the Rosenbrock function.\n",
      "       rosen_hess - The Hessian matrix of the Rosenbrock function.\n",
      "       rosen_hess_prod - Product of the Rosenbrock Hessian with a vector.\n",
      "\n",
      "    Legacy functions\n",
      "    ================\n",
      "\n",
      "    The functions below are not recommended for use in new scripts;\n",
      "    all of these methods are accessible via a newer, more consistent\n",
      "    interfaces, provided by the interfaces above.\n",
      "\n",
      "    Optimization\n",
      "    ------------\n",
      "\n",
      "    General-purpose multivariate methods:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       fmin - Nelder-Mead Simplex algorithm.\n",
      "       fmin_powell - Powell's (modified) conjugate direction method.\n",
      "       fmin_cg - Non-linear (Polak-Ribiere) conjugate gradient algorithm.\n",
      "       fmin_bfgs - Quasi-Newton method (Broydon-Fletcher-Goldfarb-Shanno).\n",
      "       fmin_ncg - Line-search Newton Conjugate Gradient.\n",
      "\n",
      "    Constrained multivariate methods:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       fmin_l_bfgs_b - Zhu, Byrd, and Nocedal's constrained optimizer.\n",
      "       fmin_tnc - Truncated Newton code.\n",
      "       fmin_cobyla - Constrained optimization by linear approximation.\n",
      "       fmin_slsqp - Minimization using sequential least-squares programming.\n",
      "\n",
      "    Univariate (scalar) minimization methods:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       fminbound - Bounded minimization of a scalar function.\n",
      "       brent - 1-D function minimization using Brent method.\n",
      "       golden - 1-D function minimization using Golden Section method.\n",
      "\n",
      "    Least-squares\n",
      "    -------------\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       leastsq - Minimize the sum of squares of M equations in N unknowns.\n",
      "\n",
      "    Root finding\n",
      "    ------------\n",
      "\n",
      "    General nonlinear solvers:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       fsolve - Non-linear multivariable equation solver.\n",
      "       broyden1 - Broyden's first method.\n",
      "       broyden2 - Broyden's second method.\n",
      "       NoConvergence -  Exception raised when nonlinear solver does not converge.\n",
      "\n",
      "    Large-scale nonlinear solvers:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       newton_krylov\n",
      "       anderson\n",
      "\n",
      "       BroydenFirst\n",
      "       InverseJacobian\n",
      "       KrylovJacobian\n",
      "\n",
      "    Simple iteration solvers:\n",
      "\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "\n",
      "       excitingmixing\n",
      "       linearmixing\n",
      "       diagbroyden\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _basinhopping\n",
      "    _bglu_dense\n",
      "    _bracket\n",
      "    _chandrupatla\n",
      "    _cobyla_py\n",
      "    _cobyqa_py\n",
      "    _constraints\n",
      "    _dcsrch\n",
      "    _differentiable_functions\n",
      "    _differentialevolution\n",
      "    _direct\n",
      "    _direct_py\n",
      "    _dual_annealing\n",
      "    _elementwise\n",
      "    _group_columns\n",
      "    _hessian_update_strategy\n",
      "    _highspy (package)\n",
      "    _isotonic\n",
      "    _lbfgsb\n",
      "    _lbfgsb_py\n",
      "    _linesearch\n",
      "    _linprog\n",
      "    _linprog_doc\n",
      "    _linprog_highs\n",
      "    _linprog_ip\n",
      "    _linprog_rs\n",
      "    _linprog_simplex\n",
      "    _linprog_util\n",
      "    _lsap\n",
      "    _lsq (package)\n",
      "    _milp\n",
      "    _minimize\n",
      "    _minpack\n",
      "    _minpack_py\n",
      "    _moduleTNC\n",
      "    _nnls\n",
      "    _nonlin\n",
      "    _numdiff\n",
      "    _optimize\n",
      "    _pava_pybind\n",
      "    _qap\n",
      "    _remove_redundancy\n",
      "    _root\n",
      "    _root_scalar\n",
      "    _shgo\n",
      "    _shgo_lib (package)\n",
      "    _slsqp_py\n",
      "    _slsqplib\n",
      "    _spectral\n",
      "    _tnc\n",
      "    _trlib (package)\n",
      "    _trustregion\n",
      "    _trustregion_constr (package)\n",
      "    _trustregion_dogleg\n",
      "    _trustregion_exact\n",
      "    _trustregion_krylov\n",
      "    _trustregion_ncg\n",
      "    _tstutils\n",
      "    _zeros\n",
      "    _zeros_py\n",
      "    cobyla\n",
      "    cython_optimize (package)\n",
      "    elementwise\n",
      "    lbfgsb\n",
      "    linesearch\n",
      "    minpack\n",
      "    minpack2\n",
      "    moduleTNC\n",
      "    nonlin\n",
      "    optimize\n",
      "    slsqp\n",
      "    tnc\n",
      "    zeros\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        scipy.optimize._nonlin.NoConvergence\n",
      "    builtins.UserWarning(builtins.Warning)\n",
      "        scipy.optimize._optimize.OptimizeWarning\n",
      "    builtins.object\n",
      "        scipy.optimize._constraints.Bounds\n",
      "        scipy.optimize._constraints.LinearConstraint\n",
      "        scipy.optimize._constraints.NonlinearConstraint\n",
      "        scipy.optimize._hessian_update_strategy.HessianUpdateStrategy\n",
      "        scipy.optimize._nonlin.InverseJacobian\n",
      "    scipy._lib._util._RichResult(builtins.dict)\n",
      "        scipy.optimize._optimize.OptimizeResult\n",
      "            scipy.optimize._zeros_py.RootResults\n",
      "    scipy.optimize._hessian_update_strategy.FullHessianUpdateStrategy(scipy.optimize._hessian_update_strategy.HessianUpdateStrategy)\n",
      "        scipy.optimize._hessian_update_strategy.BFGS\n",
      "        scipy.optimize._hessian_update_strategy.SR1\n",
      "    scipy.optimize._nonlin.GenericBroyden(scipy.optimize._nonlin.Jacobian)\n",
      "        scipy.optimize._nonlin.BroydenFirst\n",
      "    scipy.optimize._nonlin.Jacobian(builtins.object)\n",
      "        scipy.optimize._nonlin.KrylovJacobian\n",
      "    scipy.sparse.linalg._interface.LinearOperator(builtins.object)\n",
      "        scipy.optimize._lbfgsb_py.LbfgsInvHessProduct\n",
      "\n",
      "    class BFGS(FullHessianUpdateStrategy)\n",
      "     |  BFGS(exception_strategy='skip_update', min_curvature=None, init_scale='auto')\n",
      "     |\n",
      "     |  Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  exception_strategy : {'skip_update', 'damp_update'}, optional\n",
      "     |      Define how to proceed when the curvature condition is violated.\n",
      "     |      Set it to 'skip_update' to just skip the update. Or, alternatively,\n",
      "     |      set it to 'damp_update' to interpolate between the actual BFGS\n",
      "     |      result and the unmodified matrix. Both exceptions strategies\n",
      "     |      are explained  in [1]_, p.536-537.\n",
      "     |  min_curvature : float\n",
      "     |      This number, scaled by a normalization factor, defines the\n",
      "     |      minimum curvature ``dot(delta_grad, delta_x)`` allowed to go\n",
      "     |      unaffected by the exception strategy. By default is equal to\n",
      "     |      1e-8 when ``exception_strategy = 'skip_update'`` and equal\n",
      "     |      to 0.2 when ``exception_strategy = 'damp_update'``.\n",
      "     |  init_scale : {float, np.array, 'auto'}\n",
      "     |      This parameter can be used to initialize the Hessian or its\n",
      "     |      inverse. When a float is given, the relevant array is initialized\n",
      "     |      to ``np.eye(n) * init_scale``, where ``n`` is the problem dimension.\n",
      "     |      Alternatively, if a precisely ``(n, n)`` shaped, symmetric array is given,\n",
      "     |      this array will be used. Otherwise an error is generated.\n",
      "     |      Set it to 'auto' in order to use an automatic heuristic for choosing\n",
      "     |      the initial scale. The heuristic is described in [1]_, p.143.\n",
      "     |      The default is 'auto'.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The update is based on the description in [1]_, p.140.\n",
      "     |\n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n",
      "     |         Second Edition (2006).\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      BFGS\n",
      "     |      FullHessianUpdateStrategy\n",
      "     |      HessianUpdateStrategy\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, exception_strategy='skip_update', min_curvature=None, init_scale='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FullHessianUpdateStrategy:\n",
      "     |\n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |\n",
      "     |  get_matrix(self)\n",
      "     |      Return the current internal matrix.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      M : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian or its inverse\n",
      "     |          (depending on how `approx_type` was defined).\n",
      "     |\n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |\n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |\n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |\n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from HessianUpdateStrategy:\n",
      "     |\n",
      "     |  __matmul__(self, p)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from HessianUpdateStrategy:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class Bounds(builtins.object)\n",
      "     |  Bounds(lb=-inf, ub=inf, keep_feasible=False)\n",
      "     |\n",
      "     |  Bounds constraint on the variables.\n",
      "     |\n",
      "     |  The constraint has the general inequality form::\n",
      "     |\n",
      "     |      lb <= x <= ub\n",
      "     |\n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  lb, ub : dense array_like, optional\n",
      "     |      Lower and upper bounds on independent variables. `lb`, `ub`, and\n",
      "     |      `keep_feasible` must be the same shape or broadcastable.\n",
      "     |      Set components of `lb` and `ub` equal\n",
      "     |      to fix a variable. Use ``np.inf`` with an appropriate sign to disable\n",
      "     |      bounds on all or some variables. Note that you can mix constraints of\n",
      "     |      different types: interval, one-sided or equality, by setting different\n",
      "     |      components of `lb` and `ub` as necessary. Defaults to ``lb = -np.inf``\n",
      "     |      and ``ub = np.inf`` (no bounds).\n",
      "     |  keep_feasible : dense array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. Must be broadcastable with `lb` and `ub`.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, lb=-inf, ub=inf, keep_feasible=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  residual(self, x)\n",
      "     |      Calculate the residual (slack) between the input and the bounds\n",
      "     |\n",
      "     |      For a bound constraint of the form::\n",
      "     |\n",
      "     |          lb <= x <= ub\n",
      "     |\n",
      "     |      the lower and upper residuals between `x` and the bounds are values\n",
      "     |      ``sl`` and ``sb`` such that::\n",
      "     |\n",
      "     |          lb + sl == x == ub - sb\n",
      "     |\n",
      "     |      When all elements of ``sl`` and ``sb`` are positive, all elements of\n",
      "     |      ``x`` lie within the bounds; a negative element in ``sl`` or ``sb``\n",
      "     |      indicates that the corresponding element of ``x`` is out of bounds.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x: array_like\n",
      "     |          Vector of independent variables\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      sl, sb : array-like\n",
      "     |          The lower and upper residuals\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class BroydenFirst(GenericBroyden)\n",
      "     |  BroydenFirst(alpha=None, reduction_method='restart', max_rank=None)\n",
      "     |\n",
      "     |  Find a root of a function, using Broyden's first Jacobian approximation.\n",
      "     |\n",
      "     |  This method is also known as \"Broyden's good method\".\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  %(params_basic)s\n",
      "     |  %(broyden_params)s\n",
      "     |  %(params_extra)s\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  root : Interface to root finding algorithms for multivariate\n",
      "     |         functions. See ``method='broyden1'`` in particular.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "     |\n",
      "     |  .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
      "     |\n",
      "     |  which corresponds to Broyden's first Jacobian update\n",
      "     |\n",
      "     |  .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
      "     |\n",
      "     |\n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] B.A. van der Rotten, PhD thesis,\n",
      "     |     \"A limited memory Broyden method to solve high-dimensional\n",
      "     |     systems of nonlinear equations\". Mathematisch Instituut,\n",
      "     |     Universiteit Leiden, The Netherlands (2003).\n",
      "     |     https://math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  The following functions define a system of nonlinear equations\n",
      "     |\n",
      "     |  >>> def fun(x):\n",
      "     |  ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "     |  ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "     |\n",
      "     |  A solution can be obtained as follows.\n",
      "     |\n",
      "     |  >>> from scipy import optimize\n",
      "     |  >>> sol = optimize.broyden1(fun, [0, 0])\n",
      "     |  >>> sol\n",
      "     |  array([0.84116396, 0.15883641])\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      BroydenFirst\n",
      "     |      GenericBroyden\n",
      "     |      Jacobian\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, alpha=None, reduction_method='restart', max_rank=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  matvec(self, f)\n",
      "     |\n",
      "     |  rmatvec(self, f)\n",
      "     |\n",
      "     |  rsolve(self, f, tol=0)\n",
      "     |\n",
      "     |  setup(self, x, F, func)\n",
      "     |\n",
      "     |  solve(self, f, tol=0)\n",
      "     |\n",
      "     |  todense(self)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GenericBroyden:\n",
      "     |\n",
      "     |  update(self, x, f)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Jacobian:\n",
      "     |\n",
      "     |  aspreconditioner(self)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Jacobian:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class HessianUpdateStrategy(builtins.object)\n",
      "     |  Interface for implementing Hessian update strategies.\n",
      "     |\n",
      "     |  Many optimization methods make use of Hessian (or inverse Hessian)\n",
      "     |  approximations, such as the quasi-Newton methods BFGS, SR1, L-BFGS.\n",
      "     |  Some of these  approximations, however, do not actually need to store\n",
      "     |  the entire matrix or can compute the internal matrix product with a\n",
      "     |  given vector in a very efficiently manner. This class serves as an\n",
      "     |  abstract interface between the optimization algorithm and the\n",
      "     |  quasi-Newton update strategies, giving freedom of implementation\n",
      "     |  to store and update the internal matrix as efficiently as possible.\n",
      "     |  Different choices of initialization and update procedure will result\n",
      "     |  in different quasi-Newton strategies.\n",
      "     |\n",
      "     |  Four methods should be implemented in derived classes: ``initialize``,\n",
      "     |  ``update``, ``dot`` and ``get_matrix``. The matrix multiplication\n",
      "     |  operator ``@`` is also defined to call the ``dot`` method.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Any instance of a class that implements this interface,\n",
      "     |  can be accepted by the method ``minimize`` and used by\n",
      "     |  the compatible solvers to approximate the Hessian (or\n",
      "     |  inverse Hessian) used by the optimization algorithms.\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __matmul__(self, p)\n",
      "     |\n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |\n",
      "     |  get_matrix(self)\n",
      "     |      Return current internal matrix.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      H : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian\n",
      "     |          or its inverse (depending on how 'approx_type'\n",
      "     |          is defined).\n",
      "     |\n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |\n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |\n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |\n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class InverseJacobian(builtins.object)\n",
      "     |  InverseJacobian(jacobian)\n",
      "     |\n",
      "     |  A simple wrapper that inverts the Jacobian using the `solve` method.\n",
      "     |\n",
      "     |  .. legacy:: class\n",
      "     |\n",
      "     |      See the newer, more consistent interfaces in :mod:`scipy.optimize`.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  jacobian : Jacobian\n",
      "     |      The Jacobian to invert.\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  shape\n",
      "     |      Matrix dimensions (M, N)\n",
      "     |  dtype\n",
      "     |      Data type of the matrix.\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, jacobian)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |\n",
      "     |  dtype\n",
      "     |\n",
      "     |  shape\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class KrylovJacobian(Jacobian)\n",
      "     |  KrylovJacobian(rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, **kw)\n",
      "     |\n",
      "     |  Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
      "     |\n",
      "     |  This method is suitable for solving large-scale problems.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  %(params_basic)s\n",
      "     |  rdiff : float, optional\n",
      "     |      Relative step size to use in numerical differentiation.\n",
      "     |  method : str or callable, optional\n",
      "     |      Krylov method to use to approximate the Jacobian.  Can be a string,\n",
      "     |      or a function implementing the same interface as the iterative\n",
      "     |      solvers in `scipy.sparse.linalg`. If a string, needs to be one of:\n",
      "     |      ``'lgmres'``, ``'gmres'``, ``'bicgstab'``, ``'cgs'``, ``'minres'``,\n",
      "     |      ``'tfqmr'``.\n",
      "     |\n",
      "     |      The default is `scipy.sparse.linalg.lgmres`.\n",
      "     |  inner_maxiter : int, optional\n",
      "     |      Parameter to pass to the \"inner\" Krylov solver: maximum number of\n",
      "     |      iterations. Iteration will stop after maxiter steps even if the\n",
      "     |      specified tolerance has not been achieved.\n",
      "     |  inner_M : LinearOperator or InverseJacobian\n",
      "     |      Preconditioner for the inner Krylov iteration.\n",
      "     |      Note that you can use also inverse Jacobians as (adaptive)\n",
      "     |      preconditioners. For example,\n",
      "     |\n",
      "     |      >>> from scipy.optimize import BroydenFirst, KrylovJacobian\n",
      "     |      >>> from scipy.optimize import InverseJacobian\n",
      "     |      >>> jac = BroydenFirst()\n",
      "     |      >>> kjac = KrylovJacobian(inner_M=InverseJacobian(jac))\n",
      "     |\n",
      "     |      If the preconditioner has a method named 'update', it will be called\n",
      "     |      as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
      "     |      the current point, and ``f`` the current function value.\n",
      "     |  outer_k : int, optional\n",
      "     |      Size of the subspace kept across LGMRES nonlinear iterations.\n",
      "     |      See `scipy.sparse.linalg.lgmres` for details.\n",
      "     |  inner_kwargs : kwargs\n",
      "     |      Keyword parameters for the \"inner\" Krylov solver\n",
      "     |      (defined with `method`). Parameter names must start with\n",
      "     |      the `inner_` prefix which will be stripped before passing on\n",
      "     |      the inner method. See, e.g., `scipy.sparse.linalg.gmres` for details.\n",
      "     |  %(params_extra)s\n",
      "     |\n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  root : Interface to root finding algorithms for multivariate\n",
      "     |         functions. See ``method='krylov'`` in particular.\n",
      "     |  scipy.sparse.linalg.gmres\n",
      "     |  scipy.sparse.linalg.lgmres\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This function implements a Newton-Krylov solver. The basic idea is\n",
      "     |  to compute the inverse of the Jacobian with an iterative Krylov\n",
      "     |  method. These methods require only evaluating the Jacobian-vector\n",
      "     |  products, which are conveniently approximated by a finite difference:\n",
      "     |\n",
      "     |  .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
      "     |\n",
      "     |  Due to the use of iterative matrix inverses, these methods can\n",
      "     |  deal with large nonlinear problems.\n",
      "     |\n",
      "     |  SciPy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
      "     |  solvers to choose from. The default here is `lgmres`, which is a\n",
      "     |  variant of restarted GMRES iteration that reuses some of the\n",
      "     |  information obtained in the previous Newton steps to invert\n",
      "     |  Jacobians in subsequent steps.\n",
      "     |\n",
      "     |  For a review on Newton-Krylov methods, see for example [1]_,\n",
      "     |  and for the LGMRES sparse inverse method, see [2]_.\n",
      "     |\n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] C. T. Kelley, Solving Nonlinear Equations with Newton's Method,\n",
      "     |         SIAM, pp.57-83, 2003.\n",
      "     |         :doi:`10.1137/1.9780898718898.ch3`\n",
      "     |  .. [2] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2004).\n",
      "     |         :doi:`10.1016/j.jcp.2003.08.010`\n",
      "     |  .. [3] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
      "     |         SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
      "     |         :doi:`10.1137/S0895479803422014`\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  The following functions define a system of nonlinear equations\n",
      "     |\n",
      "     |  >>> def fun(x):\n",
      "     |  ...     return [x[0] + 0.5 * x[1] - 1.0,\n",
      "     |  ...             0.5 * (x[1] - x[0]) ** 2]\n",
      "     |\n",
      "     |  A solution can be obtained as follows.\n",
      "     |\n",
      "     |  >>> from scipy import optimize\n",
      "     |  >>> sol = optimize.newton_krylov(fun, [0, 0])\n",
      "     |  >>> sol\n",
      "     |  array([0.66731771, 0.66536458])\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      KrylovJacobian\n",
      "     |      Jacobian\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, **kw)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  matvec(self, v)\n",
      "     |\n",
      "     |  setup(self, x, f, func)\n",
      "     |\n",
      "     |  solve(self, rhs, tol=0)\n",
      "     |\n",
      "     |  update(self, x, f)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Jacobian:\n",
      "     |\n",
      "     |  aspreconditioner(self)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Jacobian:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class LbfgsInvHessProduct(scipy.sparse.linalg._interface.LinearOperator)\n",
      "     |  LbfgsInvHessProduct(sk, yk)\n",
      "     |\n",
      "     |  Linear operator for the L-BFGS approximate inverse Hessian.\n",
      "     |\n",
      "     |  This operator computes the product of a vector with the approximate inverse\n",
      "     |  of the Hessian of the objective function, using the L-BFGS limited\n",
      "     |  memory approximation to the inverse Hessian, accumulated during the\n",
      "     |  optimization.\n",
      "     |\n",
      "     |  Objects of this class implement the ``scipy.sparse.linalg.LinearOperator``\n",
      "     |  interface.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the solution vector.\n",
      "     |      (See [1]).\n",
      "     |  yk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the gradient. (See [1]).\n",
      "     |\n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge. \"Updating quasi-Newton matrices with limited\n",
      "     |     storage.\" Mathematics of computation 35.151 (1980): 773-782.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      LbfgsInvHessProduct\n",
      "     |      scipy.sparse.linalg._interface.LinearOperator\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, sk, yk)\n",
      "     |      Construct the operator.\n",
      "     |\n",
      "     |  todense(self)\n",
      "     |      Return a dense array representation of this operator.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      arr : ndarray, shape=(n, n)\n",
      "     |          An array with the same shape and containing\n",
      "     |          the same data represented by this `LinearOperator`.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy.sparse.linalg._interface.LinearOperator:\n",
      "     |\n",
      "     |  __add__(self, x)\n",
      "     |\n",
      "     |  __call__(self, x)\n",
      "     |      Call self as a function.\n",
      "     |\n",
      "     |  __matmul__(self, other)\n",
      "     |\n",
      "     |  __mul__(self, x)\n",
      "     |\n",
      "     |  __neg__(self)\n",
      "     |\n",
      "     |  __pow__(self, p)\n",
      "     |\n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __rmatmul__(self, other)\n",
      "     |\n",
      "     |  __rmul__(self, x)\n",
      "     |\n",
      "     |  __sub__(self, x)\n",
      "     |\n",
      "     |  __truediv__(self, other)\n",
      "     |\n",
      "     |  adjoint(self)\n",
      "     |      Hermitian adjoint.\n",
      "     |\n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |\n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |\n",
      "     |  dot(self, x)\n",
      "     |      Matrix-matrix or matrix-vector multiplication.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          1-d or 2-d array, representing a vector or matrix.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Ax : array\n",
      "     |          1-d or 2-d array (depending on the shape of x) that represents\n",
      "     |          the result of applying this linear operator on x.\n",
      "     |\n",
      "     |  matmat(self, X)\n",
      "     |      Matrix-matrix multiplication.\n",
      "     |\n",
      "     |      Performs the operation y=A@X where A is an MxN linear\n",
      "     |      operator and X dense N*K matrix or ndarray.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          An array with shape (N,K).\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,K) depending on\n",
      "     |          the type of the X argument.\n",
      "     |\n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matmat wraps any user-specified matmat routine or overridden\n",
      "     |      _matmat method to ensure that y has the correct type.\n",
      "     |\n",
      "     |  matvec(self, x)\n",
      "     |      Matrix-vector multiplication.\n",
      "     |\n",
      "     |      Performs the operation y=A@x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (N,) or (N,1).\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,) or (M,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |\n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matvec wraps the user-specified matvec routine or overridden\n",
      "     |      _matvec method to ensure that y has the correct shape and type.\n",
      "     |\n",
      "     |  rmatmat(self, X)\n",
      "     |      Adjoint matrix-matrix multiplication.\n",
      "     |\n",
      "     |      Performs the operation y = A^H @ x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array, or 2-d array.\n",
      "     |      The default implementation defers to the adjoint.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          A matrix or 2D array.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or 2D array depending on the type of the input.\n",
      "     |\n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatmat wraps the user-specified rmatmat routine.\n",
      "     |\n",
      "     |  rmatvec(self, x)\n",
      "     |      Adjoint matrix-vector multiplication.\n",
      "     |\n",
      "     |      Performs the operation y = A^H @ x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (M,) or (M,1).\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (N,) or (N,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |\n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatvec wraps the user-specified rmatvec routine or overridden\n",
      "     |      _rmatvec method to ensure that y has the correct shape and type.\n",
      "     |\n",
      "     |  transpose(self)\n",
      "     |      Transpose this linear operator.\n",
      "     |\n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from scipy.sparse.linalg._interface.LinearOperator:\n",
      "     |\n",
      "     |  __new__(cls, *args, **kwargs)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from scipy.sparse.linalg._interface.LinearOperator:\n",
      "     |\n",
      "     |  H\n",
      "     |      Hermitian adjoint.\n",
      "     |\n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |\n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |\n",
      "     |  T\n",
      "     |      Transpose this linear operator.\n",
      "     |\n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from scipy.sparse.linalg._interface.LinearOperator:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from scipy.sparse.linalg._interface.LinearOperator:\n",
      "     |\n",
      "     |  __array_ufunc__ = None\n",
      "     |\n",
      "     |  ndim = 2\n",
      "\n",
      "    class LinearConstraint(builtins.object)\n",
      "     |  LinearConstraint(A, lb=-inf, ub=inf, keep_feasible=False)\n",
      "     |\n",
      "     |  Linear constraint on the variables.\n",
      "     |\n",
      "     |  The constraint has the general inequality form::\n",
      "     |\n",
      "     |      lb <= A.dot(x) <= ub\n",
      "     |\n",
      "     |  Here the vector of independent variables x is passed as ndarray of shape\n",
      "     |  (n,) and the matrix A has shape (m, n).\n",
      "     |\n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  A : {array_like, sparse array}, shape (m, n)\n",
      "     |      Matrix defining the constraint.\n",
      "     |  lb, ub : dense array_like, optional\n",
      "     |      Lower and upper limits on the constraint. Each array must have the\n",
      "     |      shape (m,) or be a scalar, in the latter case a bound will be the same\n",
      "     |      for all components of the constraint. Use ``np.inf`` with an\n",
      "     |      appropriate sign to specify a one-sided constraint.\n",
      "     |      Set components of `lb` and `ub` equal to represent an equality\n",
      "     |      constraint. Note that you can mix constraints of different types:\n",
      "     |      interval, one-sided or equality, by setting different components of\n",
      "     |      `lb` and `ub` as  necessary. Defaults to ``lb = -np.inf``\n",
      "     |      and ``ub = np.inf`` (no limits).\n",
      "     |  keep_feasible : dense array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, A, lb=-inf, ub=inf, keep_feasible=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  residual(self, x)\n",
      "     |      Calculate the residual between the constraint function and the limits\n",
      "     |\n",
      "     |      For a linear constraint of the form::\n",
      "     |\n",
      "     |          lb <= A@x <= ub\n",
      "     |\n",
      "     |      the lower and upper residuals between ``A@x`` and the limits are values\n",
      "     |      ``sl`` and ``sb`` such that::\n",
      "     |\n",
      "     |          lb + sl == A@x == ub - sb\n",
      "     |\n",
      "     |      When all elements of ``sl`` and ``sb`` are positive, all elements of\n",
      "     |      the constraint are satisfied; a negative element in ``sl`` or ``sb``\n",
      "     |      indicates that the corresponding element of the constraint is not\n",
      "     |      satisfied.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x: array_like\n",
      "     |          Vector of independent variables\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      sl, sb : array-like\n",
      "     |          The lower and upper residuals\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class NoConvergence(builtins.Exception)\n",
      "     |  Exception raised when nonlinear solver fails to converge within the specified\n",
      "     |  `maxiter`.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      NoConvergence\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |\n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |\n",
      "     |  __new__(*args, **kwargs) class method of builtins.Exception\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |\n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |\n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(...)\n",
      "     |\n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  add_note(...)\n",
      "     |      Exception.add_note(note) --\n",
      "     |      add a note to the exception\n",
      "     |\n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |\n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |\n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |\n",
      "     |  __dict__\n",
      "     |\n",
      "     |  __suppress_context__\n",
      "     |\n",
      "     |  __traceback__\n",
      "     |\n",
      "     |  args\n",
      "\n",
      "    class NonlinearConstraint(builtins.object)\n",
      "     |  NonlinearConstraint(fun, lb, ub, jac='2-point', hess=None, keep_feasible=False, finite_diff_rel_step=None, finite_diff_jac_sparsity=None)\n",
      "     |\n",
      "     |  Nonlinear constraint on the variables.\n",
      "     |\n",
      "     |  The constraint has the general inequality form::\n",
      "     |\n",
      "     |      lb <= fun(x) <= ub\n",
      "     |\n",
      "     |  Here the vector of independent variables x is passed as ndarray of shape\n",
      "     |  (n,) and ``fun`` returns a vector with m components.\n",
      "     |\n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fun : callable\n",
      "     |      The function defining the constraint.\n",
      "     |      The signature is ``fun(x) -> array_like, shape (m,)``.\n",
      "     |  lb, ub : array_like\n",
      "     |      Lower and upper bounds on the constraint. Each array must have the\n",
      "     |      shape (m,) or be a scalar, in the latter case a bound will be the same\n",
      "     |      for all components of the constraint. Use ``np.inf`` with an\n",
      "     |      appropriate sign to specify a one-sided constraint.\n",
      "     |      Set components of `lb` and `ub` equal to represent an equality\n",
      "     |      constraint. Note that you can mix constraints of different types:\n",
      "     |      interval, one-sided or equality, by setting different components of\n",
      "     |      `lb` and `ub` as  necessary.\n",
      "     |  jac : {callable,  '2-point', '3-point', 'cs'}, optional\n",
      "     |      Method of computing the Jacobian matrix (an m-by-n matrix,\n",
      "     |      where element (i, j) is the partial derivative of f[i] with\n",
      "     |      respect to x[j]).  The keywords {'2-point', '3-point',\n",
      "     |      'cs'} select a finite difference scheme for the numerical estimation.\n",
      "     |      A callable must have the following signature::\n",
      "     |\n",
      "     |          jac(x) -> {ndarray, sparse array}, shape (m, n)\n",
      "     |\n",
      "     |      Default is '2-point'.\n",
      "     |  hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy, None}, optional\n",
      "     |      Method for computing the Hessian matrix. The keywords\n",
      "     |      {'2-point', '3-point', 'cs'} select a finite difference scheme for\n",
      "     |      numerical  estimation.  Alternatively, objects implementing\n",
      "     |      `HessianUpdateStrategy` interface can be used to approximate the\n",
      "     |      Hessian. Currently available implementations are:\n",
      "     |\n",
      "     |      - `BFGS` (default option)\n",
      "     |      - `SR1`\n",
      "     |\n",
      "     |      A callable must return the Hessian matrix of ``dot(fun, v)`` and\n",
      "     |      must have the following signature:\n",
      "     |      ``hess(x, v) -> {LinearOperator, sparse array, array_like}, shape (n, n)``.\n",
      "     |      Here ``v`` is ndarray with shape (m,) containing Lagrange multipliers.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  finite_diff_rel_step: None or array_like, optional\n",
      "     |      Relative step size for the finite difference approximation. Default is\n",
      "     |      None, which will select a reasonable value automatically depending\n",
      "     |      on a finite difference scheme.\n",
      "     |  finite_diff_jac_sparsity: {None, array_like, sparse array}, optional\n",
      "     |      Defines the sparsity structure of the Jacobian matrix for finite\n",
      "     |      difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "     |      only few non-zero elements in *each* row, providing the sparsity\n",
      "     |      structure will greatly speed up the computations. A zero entry means\n",
      "     |      that a corresponding element in the Jacobian is identically zero.\n",
      "     |      If provided, forces the use of 'lsmr' trust-region solver.\n",
      "     |      If None (default) then dense differencing will be used.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Finite difference schemes {'2-point', '3-point', 'cs'} may be used for\n",
      "     |  approximating either the Jacobian or the Hessian. We, however, do not allow\n",
      "     |  its use for approximating both simultaneously. Hence whenever the Jacobian\n",
      "     |  is estimated via finite-differences, we require the Hessian to be estimated\n",
      "     |  using one of the quasi-Newton strategies.\n",
      "     |\n",
      "     |  The scheme 'cs' is potentially the most accurate, but requires the function\n",
      "     |  to correctly handles complex inputs and be analytically continuable to the\n",
      "     |  complex plane. The scheme '3-point' is more accurate than '2-point' but\n",
      "     |  requires twice as many operations.\n",
      "     |\n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Constrain ``x[0] < sin(x[1]) + 1.9``\n",
      "     |\n",
      "     |  >>> from scipy.optimize import NonlinearConstraint\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> con = lambda x: x[0] - np.sin(x[1])\n",
      "     |  >>> nlc = NonlinearConstraint(con, -np.inf, 1.9)\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, fun, lb, ub, jac='2-point', hess=None, keep_feasible=False, finite_diff_rel_step=None, finite_diff_jac_sparsity=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class OptimizeResult(scipy._lib._util._RichResult)\n",
      "     |  Represents the optimization result.\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  x : ndarray\n",
      "     |      The solution of the optimization.\n",
      "     |  success : bool\n",
      "     |      Whether or not the optimizer exited successfully.\n",
      "     |  status : int\n",
      "     |      Termination status of the optimizer. Its value depends on the\n",
      "     |      underlying solver. Refer to `message` for details.\n",
      "     |  message : str\n",
      "     |      Description of the cause of the termination.\n",
      "     |  fun : float\n",
      "     |      Value of objective function at `x`.\n",
      "     |  jac, hess : ndarray\n",
      "     |      Values of objective function's Jacobian and its Hessian at `x` (if\n",
      "     |      available). The Hessian may be an approximation, see the documentation\n",
      "     |      of the function in question.\n",
      "     |  hess_inv : object\n",
      "     |      Inverse of the objective function's Hessian; may be an approximation.\n",
      "     |      Not available for all solvers. The type of this attribute may be\n",
      "     |      either np.ndarray or scipy.sparse.linalg.LinearOperator.\n",
      "     |  nfev, njev, nhev : int\n",
      "     |      Number of evaluations of the objective functions and of its\n",
      "     |      Jacobian and Hessian.\n",
      "     |  nit : int\n",
      "     |      Number of iterations performed by the optimizer.\n",
      "     |  maxcv : float\n",
      "     |      The maximum constraint violation.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Depending on the specific solver being used, `OptimizeResult` may\n",
      "     |  not have all attributes listed here, and they may have additional\n",
      "     |  attributes not listed here. Since this class is essentially a\n",
      "     |  subclass of dict with attribute accessors, one can see which\n",
      "     |  attributes are available using the `OptimizeResult.keys` method.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      OptimizeResult\n",
      "     |      scipy._lib._util._RichResult\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods inherited from scipy._lib._util._RichResult:\n",
      "     |\n",
      "     |  __delattr__ = __delitem__(self, key, /)\n",
      "     |\n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |\n",
      "     |  __getattr__(self, name)\n",
      "     |\n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setattr__ = __setitem__(self, key, value, /)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from scipy._lib._util._RichResult:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |\n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |\n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |\n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |\n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |\n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |\n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |\n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |\n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |\n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |\n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |\n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |\n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |\n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |\n",
      "     |  __reversed__(self, /)\n",
      "     |      Return a reverse iterator over the dict keys.\n",
      "     |\n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |\n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |\n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |\n",
      "     |  clear(...)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |\n",
      "     |  copy(...)\n",
      "     |      D.copy() -> a shallow copy of D\n",
      "     |\n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |\n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |\n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |\n",
      "     |  pop(...)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |\n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |\n",
      "     |  popitem(self, /)\n",
      "     |      Remove and return a (key, value) pair as a 2-tuple.\n",
      "     |\n",
      "     |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      "     |      Raises KeyError if the dict is empty.\n",
      "     |\n",
      "     |  setdefault(self, key, default=None, /)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |\n",
      "     |  update(...)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E.keys(): D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |\n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |\n",
      "     |  __class_getitem__(...)\n",
      "     |      See PEP 585\n",
      "     |\n",
      "     |  fromkeys(iterable, value=None, /)\n",
      "     |      Create a new dictionary with keys from iterable and values set to value.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |\n",
      "     |  __new__(*args, **kwargs) class method of builtins.dict\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.dict:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "\n",
      "    class OptimizeWarning(builtins.UserWarning)\n",
      "     |  General warning for :mod:`scipy.optimize`.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      OptimizeWarning\n",
      "     |      builtins.UserWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.UserWarning:\n",
      "     |\n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.UserWarning:\n",
      "     |\n",
      "     |  __new__(*args, **kwargs) class method of builtins.UserWarning\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |\n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |\n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setstate__(...)\n",
      "     |\n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  add_note(...)\n",
      "     |      Exception.add_note(note) --\n",
      "     |      add a note to the exception\n",
      "     |\n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |\n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |\n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |\n",
      "     |  __dict__\n",
      "     |\n",
      "     |  __suppress_context__\n",
      "     |\n",
      "     |  __traceback__\n",
      "     |\n",
      "     |  args\n",
      "\n",
      "    class RootResults(scipy.optimize._optimize.OptimizeResult)\n",
      "     |  RootResults(root, iterations, function_calls, flag, method)\n",
      "     |\n",
      "     |  Represents the root finding result.\n",
      "     |\n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  root : float\n",
      "     |      Estimated root location.\n",
      "     |  iterations : int\n",
      "     |      Number of iterations needed to find the root.\n",
      "     |  function_calls : int\n",
      "     |      Number of times the function was called.\n",
      "     |  converged : bool\n",
      "     |      True if the routine converged.\n",
      "     |  flag : str\n",
      "     |      Description of the cause of termination.\n",
      "     |  method : str\n",
      "     |      Root finding method used.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      RootResults\n",
      "     |      scipy.optimize._optimize.OptimizeResult\n",
      "     |      scipy._lib._util._RichResult\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, root, iterations, function_calls, flag, method)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy._lib._util._RichResult:\n",
      "     |\n",
      "     |  __delattr__ = __delitem__(self, key, /)\n",
      "     |\n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |\n",
      "     |  __getattr__(self, name)\n",
      "     |\n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setattr__ = __setitem__(self, key, value, /)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from scipy._lib._util._RichResult:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |\n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |\n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |\n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |\n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |\n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |\n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |\n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |\n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |\n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |\n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |\n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |\n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |\n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |\n",
      "     |  __reversed__(self, /)\n",
      "     |      Return a reverse iterator over the dict keys.\n",
      "     |\n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |\n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |\n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |\n",
      "     |  clear(...)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |\n",
      "     |  copy(...)\n",
      "     |      D.copy() -> a shallow copy of D\n",
      "     |\n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |\n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |\n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |\n",
      "     |  pop(...)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |\n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |\n",
      "     |  popitem(self, /)\n",
      "     |      Remove and return a (key, value) pair as a 2-tuple.\n",
      "     |\n",
      "     |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      "     |      Raises KeyError if the dict is empty.\n",
      "     |\n",
      "     |  setdefault(self, key, default=None, /)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |\n",
      "     |  update(...)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E.keys(): D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |\n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |\n",
      "     |  __class_getitem__(...)\n",
      "     |      See PEP 585\n",
      "     |\n",
      "     |  fromkeys(iterable, value=None, /)\n",
      "     |      Create a new dictionary with keys from iterable and values set to value.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |\n",
      "     |  __new__(*args, **kwargs) class method of builtins.dict\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.dict:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "\n",
      "    class SR1(FullHessianUpdateStrategy)\n",
      "     |  SR1(min_denominator=1e-08, init_scale='auto')\n",
      "     |\n",
      "     |  Symmetric-rank-1 Hessian update strategy.\n",
      "     |\n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  min_denominator : float\n",
      "     |      This number, scaled by a normalization factor,\n",
      "     |      defines the minimum denominator magnitude allowed\n",
      "     |      in the update. When the condition is violated we skip\n",
      "     |      the update. By default uses ``1e-8``.\n",
      "     |  init_scale : {float, np.array, 'auto'}, optional\n",
      "     |      This parameter can be used to initialize the Hessian or its\n",
      "     |      inverse. When a float is given, the relevant array is initialized\n",
      "     |      to ``np.eye(n) * init_scale``, where ``n`` is the problem dimension.\n",
      "     |      Alternatively, if a precisely ``(n, n)`` shaped, symmetric array is given,\n",
      "     |      this array will be used. Otherwise an error is generated.\n",
      "     |      Set it to 'auto' in order to use an automatic heuristic for choosing\n",
      "     |      the initial scale. The heuristic is described in [1]_, p.143.\n",
      "     |      The default is 'auto'.\n",
      "     |\n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The update is based on the description in [1]_, p.144-146.\n",
      "     |\n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n",
      "     |         Second Edition (2006).\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      SR1\n",
      "     |      FullHessianUpdateStrategy\n",
      "     |      HessianUpdateStrategy\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, min_denominator=1e-08, init_scale='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FullHessianUpdateStrategy:\n",
      "     |\n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |\n",
      "     |  get_matrix(self)\n",
      "     |      Return the current internal matrix.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      M : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian or its inverse\n",
      "     |          (depending on how `approx_type` was defined).\n",
      "     |\n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |\n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |\n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |\n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from HessianUpdateStrategy:\n",
      "     |\n",
      "     |  __matmul__(self, p)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from HessianUpdateStrategy:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "FUNCTIONS\n",
      "    anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using (extended) Anderson mixing.\n",
      "\n",
      "        The Jacobian is formed by for a 'best' solution in the space\n",
      "        spanned by last `M` vectors. As a result, only a MxM matrix\n",
      "        inversions and MxN multiplications are required. [Ey]_\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        M : float, optional\n",
      "            Number of previous vectors to retain. Defaults to 5.\n",
      "        w0 : float, optional\n",
      "            Regularization parameter for numerical stability.\n",
      "            Compared to unity, good values of the order of 0.01.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method='anderson'`` in particular.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [Ey] V. Eyert, J. Comp. Phys., 124, 271 (1996).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "\n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "\n",
      "        A solution can be obtained as follows.\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.anderson(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116588, 0.15883789])\n",
      "\n",
      "    approx_fprime(xk, f, epsilon=np.float64(1.4901161193847656e-08), *args)\n",
      "        Finite difference approximation of the derivatives of a\n",
      "        scalar or vector-valued function.\n",
      "\n",
      "        If a function maps from :math:`R^n` to :math:`R^m`, its derivatives form\n",
      "        an m-by-n matrix\n",
      "        called the Jacobian, where an element :math:`(i, j)` is a partial\n",
      "        derivative of f[i] with respect to ``xk[j]``.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        xk : array_like\n",
      "            The coordinate vector at which to determine the gradient of `f`.\n",
      "        f : callable\n",
      "            Function of which to estimate the derivatives of. Has the signature\n",
      "            ``f(xk, *args)`` where `xk` is the argument in the form of a 1-D array\n",
      "            and `args` is a tuple of any additional fixed parameters needed to\n",
      "            completely specify the function. The argument `xk` passed to this\n",
      "            function is an ndarray of shape (n,) (never a scalar even if n=1).\n",
      "            It must return a 1-D array_like of shape (m,) or a scalar.\n",
      "\n",
      "            Suppose the callable has signature ``f0(x, *my_args, **my_kwargs)``, where\n",
      "            ``my_args`` and ``my_kwargs`` are required positional and keyword arguments.\n",
      "            Rather than passing ``f0`` as the callable, wrap it to accept\n",
      "            only ``x``; e.g., pass ``fun=lambda x: f0(x, *my_args, **my_kwargs)`` as the\n",
      "            callable, where ``my_args`` (tuple) and ``my_kwargs`` (dict) have been\n",
      "            gathered before invoking this function.\n",
      "\n",
      "            .. versionchanged:: 1.9.0\n",
      "                `f` is now able to return a 1-D array-like, with the :math:`(m, n)`\n",
      "                Jacobian being estimated.\n",
      "\n",
      "        epsilon : {float, array_like}, optional\n",
      "            Increment to `xk` to use for determining the function gradient.\n",
      "            If a scalar, uses the same finite difference delta for all partial\n",
      "            derivatives. If an array, should contain one value per element of\n",
      "            `xk`. Defaults to ``sqrt(np.finfo(float).eps)``, which is approximately\n",
      "            1.49e-08.\n",
      "        \\*args : args, optional\n",
      "            Any other arguments that are to be passed to `f`.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        jac : ndarray\n",
      "            The partial derivatives of `f` to `xk`.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        check_grad : Check correctness of gradient function against approx_fprime.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The function gradient is determined by the forward finite difference\n",
      "        formula::\n",
      "\n",
      "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
      "            f'[i] = ---------------------------------\n",
      "                                epsilon[i]\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c0, c1):\n",
      "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
      "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
      "\n",
      "        >>> x = np.ones(2)\n",
      "        >>> c0, c1 = (1, 200)\n",
      "        >>> eps = np.sqrt(np.finfo(float).eps)\n",
      "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
      "        array([   2.        ,  400.00004208])\n",
      "\n",
      "    basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None, rng=None, *, target_accept_rate=0.5, stepwise_factor=0.9, seed=None)\n",
      "        Find the global minimum of a function using the basin-hopping algorithm.\n",
      "\n",
      "        Basin-hopping is a two-phase method that combines a global stepping\n",
      "        algorithm with local minimization at each step. Designed to mimic\n",
      "        the natural process of energy minimization of clusters of atoms, it works\n",
      "        well for similar problems with \"funnel-like, but rugged\" energy landscapes\n",
      "        [5]_.\n",
      "\n",
      "        As the step-taking, step acceptance, and minimization methods are all\n",
      "        customizable, this function can also be used to implement other two-phase\n",
      "        methods.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            Function to be optimized.  ``args`` can be passed as an optional item\n",
      "            in the dict `minimizer_kwargs`\n",
      "        x0 : array_like\n",
      "            Initial guess.\n",
      "        niter : integer, optional\n",
      "            The number of basin-hopping iterations. There will be a total of\n",
      "            ``niter + 1`` runs of the local minimizer.\n",
      "        T : float, optional\n",
      "            The \"temperature\" parameter for the acceptance or rejection criterion.\n",
      "            Higher \"temperatures\" mean that larger jumps in function value will be\n",
      "            accepted.  For best results `T` should be comparable to the\n",
      "            separation (in function value) between local minima.\n",
      "        stepsize : float, optional\n",
      "            Maximum step size for use in the random displacement.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the local minimizer\n",
      "            `scipy.optimize.minimize` Some important options could be:\n",
      "\n",
      "            method : str\n",
      "                The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
      "            args : tuple\n",
      "                Extra arguments passed to the objective function (`func`) and\n",
      "                its derivatives (Jacobian, Hessian).\n",
      "        take_step : callable ``take_step(x)``, optional\n",
      "            Replace the default step-taking routine with this routine. The default\n",
      "            step-taking routine is a random displacement of the coordinates, but\n",
      "            other step-taking algorithms may be better for some systems.\n",
      "            `take_step` can optionally have the attribute ``take_step.stepsize``.\n",
      "            If this attribute exists, then `basinhopping` will adjust\n",
      "            ``take_step.stepsize`` in order to try to optimize the global minimum\n",
      "            search.\n",
      "        accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
      "            Define a test which will be used to judge whether to accept the\n",
      "            step. This will be used in addition to the Metropolis test based on\n",
      "            \"temperature\" `T`. The acceptable return values are True,\n",
      "            False, or ``\"force accept\"``. If any of the tests return False\n",
      "            then the step is rejected. If the latter, then this will override any\n",
      "            other tests in order to accept the step. This can be used, for example,\n",
      "            to forcefully escape from a local minimum that `basinhopping` is\n",
      "            trapped in.\n",
      "        callback : callable, ``callback(x, f, accept)``, optional\n",
      "            A callback function which will be called for all minima found. ``x``\n",
      "            and ``f`` are the coordinates and function value of the trial minimum,\n",
      "            and ``accept`` is whether that minimum was accepted. This can\n",
      "            be used, for example, to save the lowest N minima found. Also,\n",
      "            `callback` can be used to specify a user defined stop criterion by\n",
      "            optionally returning True to stop the `basinhopping` routine.\n",
      "        interval : integer, optional\n",
      "            interval for how often to update the `stepsize`\n",
      "        disp : bool, optional\n",
      "            Set to True to print status messages\n",
      "        niter_success : integer, optional\n",
      "            Stop the run if the global minimum candidate remains the same for this\n",
      "            number of iterations.\n",
      "        rng : {None, int, `numpy.random.Generator`}, optional\n",
      "            If `rng` is passed by keyword, types other than `numpy.random.Generator` are\n",
      "            passed to `numpy.random.default_rng` to instantiate a ``Generator``.\n",
      "            If `rng` is already a ``Generator`` instance, then the provided instance is\n",
      "            used. Specify `rng` for repeatable function behavior.\n",
      "\n",
      "            If this argument is passed by position or `seed` is passed by keyword,\n",
      "            legacy behavior for the argument `seed` applies:\n",
      "\n",
      "            - If `seed` is None (or `numpy.random`), the `numpy.random.RandomState`\n",
      "              singleton is used.\n",
      "            - If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "              seeded with `seed`.\n",
      "            - If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "              that instance is used.\n",
      "\n",
      "            .. versionchanged:: 1.15.0\n",
      "                As part of the `SPEC-007 <https://scientific-python.org/specs/spec-0007/>`_\n",
      "                transition from use of `numpy.random.RandomState` to\n",
      "                `numpy.random.Generator`, this keyword was changed from `seed` to `rng`.\n",
      "                For an interim period, both keywords will continue to work, although only one\n",
      "                may be specified at a time. After the interim period, function calls using the\n",
      "                `seed` keyword will emit warnings. The behavior of both `seed` and\n",
      "                `rng` are outlined above, but only the `rng` keyword should be used in new code.\n",
      "\n",
      "            The random numbers generated only affect the default Metropolis\n",
      "            `accept_test` and the default `take_step`. If you supply your own\n",
      "            `take_step` and `accept_test`, and these functions use random\n",
      "            number generation, then those functions are responsible for the state\n",
      "            of their random number generator.\n",
      "        target_accept_rate : float, optional\n",
      "            The target acceptance rate that is used to adjust the `stepsize`.\n",
      "            If the current acceptance rate is greater than the target,\n",
      "            then the `stepsize` is increased. Otherwise, it is decreased.\n",
      "            Range is (0, 1). Default is 0.5.\n",
      "\n",
      "            .. versionadded:: 1.8.0\n",
      "        stepwise_factor : float, optional\n",
      "            The `stepsize` is multiplied or divided by this stepwise factor upon\n",
      "            each update. Range is (0, 1). Default is 0.9.\n",
      "\n",
      "            .. versionadded:: 1.8.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``fun`` the value\n",
      "            of the function at the solution, and ``message`` which describes the\n",
      "            cause of the termination. The ``OptimizeResult`` object returned by the\n",
      "            selected minimizer at the lowest minimum is also contained within this\n",
      "            object and can be accessed through the ``lowest_optimization_result``\n",
      "            attribute. ``lowest_optimization_result`` will only be updated if a\n",
      "            local minimization was successful.\n",
      "            See `OptimizeResult` for a description of other attributes.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "\n",
      "        :func:`minimize`\n",
      "            The local minimization function called once for each basinhopping step. `minimizer_kwargs` is passed to this routine.\n",
      "\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Basin-hopping is a stochastic algorithm which attempts to find the global\n",
      "        minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
      "        [4]_. The algorithm in its current form was described by David Wales and\n",
      "        Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
      "\n",
      "        The algorithm is iterative with each cycle composed of the following\n",
      "        features\n",
      "\n",
      "        1) random perturbation of the coordinates\n",
      "\n",
      "        2) local minimization\n",
      "\n",
      "        3) accept or reject the new coordinates based on the minimized function\n",
      "           value\n",
      "\n",
      "        The acceptance test used here is the Metropolis criterion of standard Monte\n",
      "        Carlo algorithms, although there are many other possibilities [3]_.\n",
      "\n",
      "        This global minimization method has been shown to be extremely efficient\n",
      "        for a wide variety of problems in physics and chemistry. It is\n",
      "        particularly useful when the function has many minima separated by large\n",
      "        barriers. See the `Cambridge Cluster Database\n",
      "        <https://www-wales.ch.cam.ac.uk/CCD.html>`_ for databases of molecular\n",
      "        systems that have been optimized primarily using basin-hopping. This\n",
      "        database includes minimization problems exceeding 300 degrees of freedom.\n",
      "\n",
      "        See the free software program `GMIN <https://www-wales.ch.cam.ac.uk/GMIN>`_\n",
      "        for a Fortran implementation of basin-hopping. This implementation has many\n",
      "        variations of the procedure described above, including more\n",
      "        advanced step taking algorithms and alternate acceptance criterion.\n",
      "\n",
      "        For stochastic global optimization there is no way to determine if the true\n",
      "        global minimum has actually been found. Instead, as a consistency check,\n",
      "        the algorithm can be run from a number of different random starting points\n",
      "        to ensure the lowest minimum found in each example has converged to the\n",
      "        global minimum. For this reason, `basinhopping` will by default simply\n",
      "        run for the number of iterations `niter` and return the lowest minimum\n",
      "        found. It is left to the user to ensure that this is in fact the global\n",
      "        minimum.\n",
      "\n",
      "        Choosing `stepsize`:  This is a crucial parameter in `basinhopping` and\n",
      "        depends on the problem being solved. The step is chosen uniformly in the\n",
      "        region from x0-stepsize to x0+stepsize, in each dimension. Ideally, it\n",
      "        should be comparable to the typical separation (in argument values) between\n",
      "        local minima of the function being optimized. `basinhopping` will, by\n",
      "        default, adjust `stepsize` to find an optimal value, but this may take\n",
      "        many iterations. You will get quicker results if you set a sensible\n",
      "        initial value for ``stepsize``.\n",
      "\n",
      "        Choosing `T`: The parameter `T` is the \"temperature\" used in the\n",
      "        Metropolis criterion. Basinhopping steps are always accepted if\n",
      "        ``func(xnew) < func(xold)``. Otherwise, they are accepted with\n",
      "        probability::\n",
      "\n",
      "            exp( -(func(xnew) - func(xold)) / T )\n",
      "\n",
      "        So, for best results, `T` should to be comparable to the typical\n",
      "        difference (in function values) between local minima. (The height of\n",
      "        \"walls\" between local minima is irrelevant.)\n",
      "\n",
      "        If `T` is 0, the algorithm becomes Monotonic Basin-Hopping, in which all\n",
      "        steps that increase energy are rejected.\n",
      "\n",
      "        .. versionadded:: 0.12.0\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
      "            Cambridge, UK.\n",
      "        .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
      "            the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
      "            110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
      "        .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
      "            multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
      "            1987, 84, 6611.\n",
      "        .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
      "            crystals, and biomolecules, Science, 1999, 285, 1368.\n",
      "        .. [5] Olson, B., Hashmi, I., Molloy, K., and Shehu1, A., Basin Hopping as\n",
      "            a General and Versatile Optimization Framework for the Characterization\n",
      "            of Biological Macromolecules, Advances in Artificial Intelligence,\n",
      "            Volume 2012 (2012), Article ID 674832, :doi:`10.1155/2012/674832`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a 1-D minimization problem, with many\n",
      "        local minima superimposed on a parabola.\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import basinhopping\n",
      "        >>> func = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
      "        >>> x0 = [1.]\n",
      "\n",
      "        Basinhopping, internally, uses a local minimization algorithm. We will use\n",
      "        the parameter `minimizer_kwargs` to tell basinhopping which algorithm to\n",
      "        use and how to set up that minimizer. This parameter will be passed to\n",
      "        `scipy.optimize.minimize`.\n",
      "\n",
      "        >>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
      "        >>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> # the global minimum is:\n",
      "        >>> ret.x, ret.fun\n",
      "        -0.1951, -1.0009\n",
      "\n",
      "        Next consider a 2-D minimization problem. Also, this time, we\n",
      "        will use gradient information to significantly speed up the search.\n",
      "\n",
      "        >>> def func2d(x):\n",
      "        ...     f = np.cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
      "        ...                                                            0.2) * x[0]\n",
      "        ...     df = np.zeros(2)\n",
      "        ...     df[0] = -14.5 * np.sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
      "        ...     df[1] = 2. * x[1] + 0.2\n",
      "        ...     return f, df\n",
      "\n",
      "        We'll also use a different local minimization algorithm. Also, we must tell\n",
      "        the minimizer that our function returns both energy and gradient (Jacobian).\n",
      "\n",
      "        >>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
      "        >>> x0 = [1.0, 1.0]\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x) = -1.0109\n",
      "\n",
      "        Here is an example using a custom step-taking routine. Imagine you want\n",
      "        the first coordinate to take larger steps than the rest of the coordinates.\n",
      "        This can be implemented like so:\n",
      "\n",
      "        >>> class MyTakeStep:\n",
      "        ...    def __init__(self, stepsize=0.5):\n",
      "        ...        self.stepsize = stepsize\n",
      "        ...        self.rng = np.random.default_rng()\n",
      "        ...    def __call__(self, x):\n",
      "        ...        s = self.stepsize\n",
      "        ...        x[0] += self.rng.uniform(-2.*s, 2.*s)\n",
      "        ...        x[1:] += self.rng.uniform(-s, s, x[1:].shape)\n",
      "        ...        return x\n",
      "\n",
      "        Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
      "        of `stepsize` to optimize the search. We'll use the same 2-D function as\n",
      "        before\n",
      "\n",
      "        >>> mytakestep = MyTakeStep()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200, take_step=mytakestep)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x) = -1.0109\n",
      "\n",
      "        Now, let's do an example using a custom callback function which prints the\n",
      "        value of every minimum found\n",
      "\n",
      "        >>> def print_fun(x, f, accepted):\n",
      "        ...         print(\"at minimum %.4f accepted %d\" % (f, int(accepted)))\n",
      "\n",
      "        We'll run it for only 10 basinhopping steps this time.\n",
      "\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, callback=print_fun, rng=rng)\n",
      "        at minimum 0.4159 accepted 1\n",
      "        at minimum -0.4317 accepted 1\n",
      "        at minimum -1.0109 accepted 1\n",
      "        at minimum -0.9073 accepted 1\n",
      "        at minimum -0.4317 accepted 0\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum -0.7425 accepted 1\n",
      "        at minimum -0.9073 accepted 1\n",
      "        at minimum -0.4317 accepted 0\n",
      "        at minimum -0.7425 accepted 1\n",
      "        at minimum -0.9073 accepted 1\n",
      "\n",
      "        The minimum at -1.0109 is actually the global minimum, found already on the\n",
      "        8th iteration.\n",
      "\n",
      "    bisect(f, a, b, args=(), xtol=2e-12, rtol=np.float64(8.881784197001252e-16), maxiter=100, full_output=False, disp=True)\n",
      "        Find root of a function within an interval using bisection.\n",
      "\n",
      "        Basic bisection routine to find a root of the function `f` between the\n",
      "        arguments `a` and `b`. `f(a)` and `f(b)` cannot have the same signs.\n",
      "        Slow but sure.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  `f` must be continuous, and\n",
      "            f(a) and f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.isclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be positive.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.isclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where x is the root, and r is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in a `RootResults`\n",
      "            return object.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        root : float\n",
      "            Root of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        As mentioned in the parameter documentation, the computed root ``x0`` will\n",
      "        satisfy ``np.isclose(x, x0, atol=xtol, rtol=rtol)``, where ``x`` is the\n",
      "        exact root. In equation form, this terminating condition is ``abs(x - x0)\n",
      "        <= xtol + rtol * abs(x0)``.\n",
      "\n",
      "        The default value ``xtol=2e-12`` may lead to surprising behavior if one\n",
      "        expects `bisect` to always compute roots with relative error near machine\n",
      "        precision. Care should be taken to select `xtol` for the use case at hand.\n",
      "        Setting ``xtol=5e-324``, the smallest subnormal number, will ensure the\n",
      "        highest level of accuracy. Larger values of `xtol` may be useful for saving\n",
      "        function evaluations when a root is at or near zero in applications where\n",
      "        the tiny absolute differences available between floating point numbers near\n",
      "        zero are not meaningful.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "\n",
      "        >>> root = optimize.bisect(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "\n",
      "        >>> root = optimize.bisect(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        fsolve : n-dimensional root-finding\n",
      "        elementwise.find_root : efficient elementwise 1-D root-finder\n",
      "\n",
      "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
      "        Bracket the minimum of a function.\n",
      "\n",
      "        Given a function and distinct initial points, search in the\n",
      "        downhill direction (as defined by the initial points) and return\n",
      "        three points that bracket the minimum of the function.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to minimize.\n",
      "        xa, xb : float, optional\n",
      "            Initial points. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
      "            A local minimum need not be contained within this interval.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to `func`.\n",
      "        grow_limit : float, optional\n",
      "            Maximum grow limit.  Defaults to 110.0\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Defaults to 1000.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        xa, xb, xc : float\n",
      "            Final points of the bracket.\n",
      "        fa, fb, fc : float\n",
      "            Objective function values at the bracket points.\n",
      "        funcalls : int\n",
      "            Number of function evaluations made.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        BracketError\n",
      "            If no valid bracket is found before the algorithm terminates.\n",
      "            See notes for conditions of a valid bracket.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm attempts to find three strictly ordered points (i.e.\n",
      "        :math:`x_a < x_b < x_c` or :math:`x_c < x_b < x_a`) satisfying\n",
      "        :math:`f(x_b) ≤ f(x_a)` and :math:`f(x_b) ≤ f(x_c)`, where one of the\n",
      "        inequalities must be satisfied strictly and all :math:`x_i` must be\n",
      "        finite.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        This function can find a downward convex region of a function:\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.optimize import bracket\n",
      "        >>> def f(x):\n",
      "        ...     return 10*x**2 + 3*x + 5\n",
      "        >>> x = np.linspace(-2, 2)\n",
      "        >>> y = f(x)\n",
      "        >>> init_xa, init_xb = 0.1, 1\n",
      "        >>> xa, xb, xc, fa, fb, fc, funcalls = bracket(f, xa=init_xa, xb=init_xb)\n",
      "        >>> plt.axvline(x=init_xa, color=\"k\", linestyle=\"--\")\n",
      "        >>> plt.axvline(x=init_xb, color=\"k\", linestyle=\"--\")\n",
      "        >>> plt.plot(x, y, \"-k\")\n",
      "        >>> plt.plot(xa, fa, \"bx\")\n",
      "        >>> plt.plot(xb, fb, \"rx\")\n",
      "        >>> plt.plot(xc, fc, \"bx\")\n",
      "        >>> plt.show()\n",
      "\n",
      "        Note that both initial points were to the right of the minimum, and the\n",
      "        third point was found in the \"downhill\" direction: the direction\n",
      "        in which the function appeared to be decreasing (to the left).\n",
      "        The final points are strictly ordered, and the function value\n",
      "        at the middle point is less than the function values at the endpoints;\n",
      "        it follows that a minimum must lie within the bracket.\n",
      "\n",
      "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
      "        Given a function of one variable and a possible bracket, return\n",
      "        a local minimizer of the function isolated to a fractional precision\n",
      "        of tol.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present).\n",
      "        brack : tuple, optional\n",
      "            Either a triple ``(xa, xb, xc)`` satisfying ``xa < xb < xc`` and\n",
      "            ``func(xb) < func(xa) and  func(xb) < func(xc)``, or a pair\n",
      "            ``(xa, xb)`` to be used as initial points for a downhill bracket search\n",
      "            (see `scipy.optimize.bracket`).\n",
      "            The minimizer ``x`` will not necessarily satisfy ``xa <= x <= xb``.\n",
      "        tol : float, optional\n",
      "            Relative error in solution `xopt` acceptable for convergence.\n",
      "        full_output : bool, optional\n",
      "            If True, return all output args (xmin, fval, iter,\n",
      "            funcalls).\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations in solution.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        xmin : ndarray\n",
      "            Optimum point.\n",
      "        fval : float\n",
      "            (Optional output) Optimum function value.\n",
      "        iter : int\n",
      "            (Optional output) Number of iterations.\n",
      "        funcalls : int\n",
      "            (Optional output) Number of objective function evaluations made.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Brent' `method` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Uses inverse parabolic interpolation when possible to speed up\n",
      "        convergence of golden section method.\n",
      "\n",
      "        Does not ensure that the minimum lies in the range specified by\n",
      "        `brack`. See `scipy.optimize.fminbound`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3 respectively. In the case where `brack` is of the\n",
      "        form ``(xa, xb)``, we can see for the given values, the output does\n",
      "        not necessarily lie in the range ``(xa, xb)``.\n",
      "\n",
      "        >>> def f(x):\n",
      "        ...     return (x-1)**2\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "\n",
      "        >>> minimizer = optimize.brent(f, brack=(1, 2))\n",
      "        >>> minimizer\n",
      "        1\n",
      "        >>> res = optimize.brent(f, brack=(-1, 0.5, 2), full_output=True)\n",
      "        >>> xmin, fval, iter, funcalls = res\n",
      "        >>> f(xmin), fval\n",
      "        (0.0, 0.0)\n",
      "\n",
      "    brenth(f, a, b, args=(), xtol=2e-12, rtol=np.float64(8.881784197001252e-16), maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's\n",
      "        method with hyperbolic extrapolation.\n",
      "\n",
      "        A variation on the classic Brent routine to find a root of the function f\n",
      "        between the arguments a and b that uses hyperbolic extrapolation instead of\n",
      "        inverse quadratic extrapolation. Bus & Dekker (1975) guarantee convergence\n",
      "        for this method, claiming that the upper bound of function evaluations here\n",
      "        is 4 or 5 times that of bisection.\n",
      "        f(a) and f(b) cannot have the same signs. Generally, on a par with the\n",
      "        brent routine, but not as heavily tested. It is a safe version of the\n",
      "        secant method that uses hyperbolic extrapolation.\n",
      "        The version here is by Chuck Harris, and implements Algorithm M of\n",
      "        [BusAndDekker1975]_, where further details (convergence properties,\n",
      "        additional remarks and such) can be found\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.isclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be positive. As with `brentq`, for nice\n",
      "            functions the method will often satisfy the above condition\n",
      "            with ``xtol/2`` and ``rtol/2``.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.isclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. As with `brentq`, for nice functions\n",
      "            the method will often satisfy the above condition with\n",
      "            ``xtol/2`` and ``rtol/2``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        root : float\n",
      "            Root of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        fmin, fmin_powell, fmin_cg, fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
      "        leastsq : nonlinear least squares minimizer\n",
      "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
      "        basinhopping, differential_evolution, brute : global optimizers\n",
      "        fminbound, brent, golden, bracket : local scalar minimizers\n",
      "        fsolve : N-D root-finding\n",
      "        brentq, ridder, bisect, newton : 1-D root-finding\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        elementwise.find_root : efficient elementwise 1-D root-finder\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        As mentioned in the parameter documentation, the computed root ``x0`` will\n",
      "        satisfy ``np.isclose(x, x0, atol=xtol, rtol=rtol)``, where ``x`` is the\n",
      "        exact root. In equation form, this terminating condition is ``abs(x - x0)\n",
      "        <= xtol + rtol * abs(x0)``.\n",
      "\n",
      "        The default value ``xtol=2e-12`` may lead to surprising behavior if one\n",
      "        expects `brenth` to always compute roots with relative error near machine\n",
      "        precision. Care should be taken to select `xtol` for the use case at hand.\n",
      "        Setting ``xtol=5e-324``, the smallest subnormal number, will ensure the\n",
      "        highest level of accuracy. Larger values of `xtol` may be useful for saving\n",
      "        function evaluations when a root is at or near zero in applications where\n",
      "        the tiny absolute differences available between floating point numbers near\n",
      "        zero are not meaningful.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [BusAndDekker1975]\n",
      "           Bus, J. C. P., Dekker, T. J.,\n",
      "           \"Two Efficient Algorithms with Guaranteed Convergence for Finding a Zero\n",
      "           of a Function\", ACM Transactions on Mathematical Software, Vol. 1, Issue\n",
      "           4, Dec. 1975, pp. 330-345. Section 3: \"Algorithm M\".\n",
      "           :doi:`10.1145/355656.355659`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "\n",
      "        >>> root = optimize.brenth(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "\n",
      "        >>> root = optimize.brenth(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "\n",
      "    brentq(f, a, b, args=(), xtol=2e-12, rtol=np.float64(8.881784197001252e-16), maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's method.\n",
      "\n",
      "        Uses the classic Brent's method to find a root of the function `f` on\n",
      "        the sign changing interval [a , b]. Generally considered the best of the\n",
      "        rootfinding routines here. It is a safe version of the secant method that\n",
      "        uses inverse quadratic extrapolation. Brent's method combines root\n",
      "        bracketing, interval bisection, and inverse quadratic interpolation. It is\n",
      "        sometimes known as the van Wijngaarden-Dekker-Brent method. Brent (1973)\n",
      "        claims convergence is guaranteed for functions computable within [a,b].\n",
      "\n",
      "        [Brent1973]_ provides the classic description of the algorithm. Another\n",
      "        description can be found in a recent edition of Numerical Recipes, including\n",
      "        [PressEtal1992]_. A third description is at\n",
      "        http://mathworld.wolfram.com/BrentsMethod.html. It should be easy to\n",
      "        understand the algorithm just by reading our code. Our code diverges a bit\n",
      "        from standard presentations: we choose a different formula for the\n",
      "        extrapolation step.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)` must\n",
      "            have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval :math:`[a, b]`.\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval :math:`[a, b]`.\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.isclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be positive. For nice functions, Brent's\n",
      "            method will often satisfy the above condition with ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.isclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. For nice functions, Brent's\n",
      "            method will often satisfy the above condition with ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        root : float\n",
      "            Root of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        fmin, fmin_powell, fmin_cg, fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
      "        leastsq : nonlinear least squares minimizer\n",
      "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
      "        basinhopping, differential_evolution, brute : global optimizers\n",
      "        fminbound, brent, golden, bracket : local scalar minimizers\n",
      "        fsolve : N-D root-finding\n",
      "        brenth, ridder, bisect, newton : 1-D root-finding\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        elementwise.find_root : efficient elementwise 1-D root-finder\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
      "\n",
      "        As mentioned in the parameter documentation, the computed root ``x0`` will\n",
      "        satisfy ``np.isclose(x, x0, atol=xtol, rtol=rtol)``, where ``x`` is the\n",
      "        exact root. In equation form, this terminating condition is ``abs(x - x0)\n",
      "        <= xtol + rtol * abs(x0)``.\n",
      "\n",
      "        The default value ``xtol=2e-12`` may lead to surprising behavior if one\n",
      "        expects `brentq` to always compute roots with relative error near machine\n",
      "        precision. Care should be taken to select `xtol` for the use case at hand.\n",
      "        Setting ``xtol=5e-324``, the smallest subnormal number, will ensure the\n",
      "        highest level of accuracy. Larger values of `xtol` may be useful for saving\n",
      "        function evaluations when a root is at or near zero in applications where\n",
      "        the tiny absolute differences available between floating point numbers near\n",
      "        zero are not meaningful.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [Brent1973]\n",
      "           Brent, R. P.,\n",
      "           *Algorithms for Minimization Without Derivatives*.\n",
      "           Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
      "\n",
      "        .. [PressEtal1992]\n",
      "           Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
      "           *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
      "           Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
      "           Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "\n",
      "        >>> root = optimize.brentq(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "\n",
      "        >>> root = optimize.brentq(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "\n",
      "    broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's first Jacobian approximation.\n",
      "\n",
      "        This method is also known as \"Broyden's good method\".\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "\n",
      "            Methods available:\n",
      "\n",
      "            - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "            - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "            - ``svd``: keep only the most significant SVD components.\n",
      "              Takes an extra parameter, ``to_retain``, which determines the\n",
      "              number of SVD components to retain when rank reduction is done.\n",
      "              Default is ``max_rank - 2``.\n",
      "\n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (i.e., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method='broyden1'`` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "\n",
      "        .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
      "\n",
      "        which corresponds to Broyden's first Jacobian update\n",
      "\n",
      "        .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
      "\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "           https://math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "\n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "\n",
      "        A solution can be obtained as follows.\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.broyden1(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116396, 0.15883641])\n",
      "\n",
      "    broyden2(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's second Jacobian approximation.\n",
      "\n",
      "        This method is also known as \"Broyden's bad method\".\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "\n",
      "            Methods available:\n",
      "\n",
      "            - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "            - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "            - ``svd``: keep only the most significant SVD components.\n",
      "              Takes an extra parameter, ``to_retain``, which determines the\n",
      "              number of SVD components to retain when rank reduction is done.\n",
      "              Default is ``max_rank - 2``.\n",
      "\n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (i.e., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method='broyden2'`` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "\n",
      "        .. math:: H_+ = H + (dx - H df) df^\\dagger / ( df^\\dagger df)\n",
      "\n",
      "        corresponding to Broyden's second method.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "\n",
      "           https://web.archive.org/web/20161022015821/http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "\n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "\n",
      "        A solution can be obtained as follows.\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.broyden2(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116365, 0.15883529])\n",
      "\n",
      "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin at 0x302b434c0>, disp=False, workers=1)\n",
      "        Minimize a function over a given range by brute force.\n",
      "\n",
      "        Uses the \"brute force\" method, i.e., computes the function's value\n",
      "        at each point of a multidimensional grid of points, to find the global\n",
      "        minimum of the function.\n",
      "\n",
      "        The function is evaluated everywhere in the range with the datatype of the\n",
      "        first call to the function, as enforced by the ``vectorize`` NumPy\n",
      "        function. The value and type of the function evaluation returned when\n",
      "        ``full_output=True`` are affected in addition by the ``finish`` argument\n",
      "        (see Notes).\n",
      "\n",
      "        The brute force approach is inefficient because the number of grid points\n",
      "        increases exponentially - the number of grid points to evaluate is\n",
      "        ``Ns ** len(x)``. Consequently, even with coarse grid spacing, even\n",
      "        moderately sized problems can take a long time to run, and/or run into\n",
      "        memory limitations.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the\n",
      "            form ``f(x, *args)``, where ``x`` is the argument in\n",
      "            the form of a 1-D array and ``args`` is a tuple of any\n",
      "            additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        ranges : tuple\n",
      "            Each component of the `ranges` tuple must be either a\n",
      "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
      "            The program uses these to create the grid of points on which\n",
      "            the objective function will be computed. See `Note 2` for\n",
      "            more detail.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        Ns : int, optional\n",
      "            Number of grid points along the axes, if not otherwise\n",
      "            specified. See `Note2`.\n",
      "        full_output : bool, optional\n",
      "            If True, return the evaluation grid and the objective function's\n",
      "            values on it.\n",
      "        finish : callable, optional\n",
      "            An optimization function that is called with the result of brute force\n",
      "            minimization as initial guess. `finish` should take `func` and\n",
      "            the initial guess as positional arguments, and take `args` as\n",
      "            keyword arguments. It may additionally take `full_output`\n",
      "            and/or `disp` as keyword arguments. Use None if no \"polishing\"\n",
      "            function is to be used. See Notes for more details.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages from the `finish` callable.\n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the grid is subdivided into `workers`\n",
      "            sections and evaluated in parallel (uses\n",
      "            `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply `-1` to use all cores available to the Process.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the grid in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            Requires that `func` be pickleable.\n",
      "\n",
      "            .. versionadded:: 1.3.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        x0 : ndarray\n",
      "            A 1-D array containing the coordinates of a point at which the\n",
      "            objective function had its minimum value. (See `Note 1` for\n",
      "            which point is returned.)\n",
      "        fval : float\n",
      "            Function value at the point `x0`. (Returned when `full_output` is\n",
      "            True.)\n",
      "        grid : tuple\n",
      "            Representation of the evaluation grid. It has the same\n",
      "            length as `x0`. (Returned when `full_output` is True.)\n",
      "        Jout : ndarray\n",
      "            Function values at each point of the evaluation\n",
      "            grid, i.e., ``Jout = func(*grid)``. (Returned\n",
      "            when `full_output` is True.)\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        basinhopping, differential_evolution\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
      "        of the objective function occurs. If `finish` is None, that is the\n",
      "        point returned. When the global minimum occurs within (or not very far\n",
      "        outside) the grid's boundaries, and the grid is fine enough, that\n",
      "        point will be in the neighborhood of the global minimum.\n",
      "\n",
      "        However, users often employ some other optimization program to\n",
      "        \"polish\" the gridpoint values, i.e., to seek a more precise\n",
      "        (local) minimum near `brute's` best gridpoint.\n",
      "        The `brute` function's `finish` option provides a convenient way to do\n",
      "        that. Any polishing program used must take `brute's` output as its\n",
      "        initial guess as a positional argument, and take `brute's` input values\n",
      "        for `args` as keyword arguments, otherwise an error will be raised.\n",
      "        It may additionally take `full_output` and/or `disp` as keyword arguments.\n",
      "\n",
      "        `brute` assumes that the `finish` function returns either an\n",
      "        `OptimizeResult` object or a tuple in the form:\n",
      "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing\n",
      "        value of the argument, ``Jmin`` is the minimum value of the objective\n",
      "        function, \"...\" may be some other returned values (which are not used\n",
      "        by `brute`), and ``statuscode`` is the status code of the `finish` program.\n",
      "\n",
      "        Note that when `finish` is not None, the values returned are those\n",
      "        of the `finish` program, *not* the gridpoint ones. Consequently,\n",
      "        while `brute` confines its search to the input grid points,\n",
      "        the `finish` program's results usually will not coincide with any\n",
      "        gridpoint, and may fall outside the grid's boundary. Thus, if a\n",
      "        minimum only needs to be found over the provided grid points, make\n",
      "        sure to pass in ``finish=None``.\n",
      "\n",
      "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
      "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
      "        Each component of the `ranges` tuple can be either a slice object or a\n",
      "        two-tuple giving a range of values, such as (0, 5). If the component is a\n",
      "        slice object, `brute` uses it directly. If the component is a two-tuple\n",
      "        range, `brute` internally converts it to a slice object that interpolates\n",
      "        `Ns` points from its low-value to its high-value, inclusive.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the use of `brute` to seek the global minimum of a function\n",
      "        of two variables that is given as the sum of a positive-definite\n",
      "        quadratic and two deep \"Gaussian-shaped\" craters. Specifically, define\n",
      "        the objective function `f` as the sum of three other functions,\n",
      "        ``f = f1 + f2 + f3``. We suppose each of these has a signature\n",
      "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
      "        are as defined below.\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
      "        >>> def f1(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
      "\n",
      "        >>> def f2(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
      "\n",
      "        >>> def f3(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
      "\n",
      "        >>> def f(z, *params):\n",
      "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
      "\n",
      "        Thus, the objective function may have local minima near the minimum\n",
      "        of each of the three functions of which it is composed. To\n",
      "        use `fmin` to polish its gridpoint result, we may then continue as\n",
      "        follows:\n",
      "\n",
      "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
      "        >>> from scipy import optimize\n",
      "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
      "        ...                           finish=optimize.fmin)\n",
      "        >>> resbrute[0]  # global minimum\n",
      "        array([-1.05665192,  1.80834843])\n",
      "        >>> resbrute[1]  # function value at global minimum\n",
      "        -3.4085818767\n",
      "\n",
      "        Note that if `finish` had been set to None, we would have gotten the\n",
      "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
      "\n",
      "    check_grad(func, grad, x0, *args, epsilon=np.float64(1.4901161193847656e-08), direction='all', rng=None, seed=None)\n",
      "        Check the correctness of a gradient function by comparing it against a\n",
      "        (forward) finite-difference approximation of the gradient.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x0, *args)``\n",
      "            Function whose derivative is to be checked.\n",
      "        grad : callable ``grad(x0, *args)``\n",
      "            Jacobian of `func`.\n",
      "        x0 : ndarray\n",
      "            Points to check `grad` against forward difference approximation of grad\n",
      "            using `func`.\n",
      "        args : \\\\*args, optional\n",
      "            Extra arguments passed to `func` and `grad`.\n",
      "        epsilon : float, optional\n",
      "            Step size used for the finite difference approximation. It defaults to\n",
      "            ``sqrt(np.finfo(float).eps)``, which is approximately 1.49e-08.\n",
      "        direction : str, optional\n",
      "            If set to ``'random'``, then gradients along a random vector\n",
      "            are used to check `grad` against forward difference approximation\n",
      "            using `func`. By default it is ``'all'``, in which case, all\n",
      "            the one hot direction vectors are considered to check `grad`.\n",
      "            If `func` is a vector valued function then only ``'all'`` can be used.\n",
      "        rng : {None, int, `numpy.random.Generator`}, optional\n",
      "            If `rng` is passed by keyword, types other than `numpy.random.Generator` are\n",
      "            passed to `numpy.random.default_rng` to instantiate a ``Generator``.\n",
      "            If `rng` is already a ``Generator`` instance, then the provided instance is\n",
      "            used. Specify `rng` for repeatable function behavior.\n",
      "\n",
      "            If this argument is passed by position or `seed` is passed by keyword,\n",
      "            legacy behavior for the argument `seed` applies:\n",
      "\n",
      "            - If `seed` is None (or `numpy.random`), the `numpy.random.RandomState`\n",
      "              singleton is used.\n",
      "            - If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "              seeded with `seed`.\n",
      "            - If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "              that instance is used.\n",
      "\n",
      "            .. versionchanged:: 1.15.0\n",
      "                As part of the `SPEC-007 <https://scientific-python.org/specs/spec-0007/>`_\n",
      "                transition from use of `numpy.random.RandomState` to\n",
      "                `numpy.random.Generator`, this keyword was changed from `seed` to `rng`.\n",
      "                For an interim period, both keywords will continue to work, although only one\n",
      "                may be specified at a time. After the interim period, function calls using the\n",
      "                `seed` keyword will emit warnings. The behavior of both `seed` and\n",
      "                `rng` are outlined above, but only the `rng` keyword should be used in new code.\n",
      "\n",
      "            The random numbers generated affect the random vector along which gradients\n",
      "            are computed to check ``grad``. Note that `rng` is only used when `direction`\n",
      "            argument is set to `'random'`.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        err : float\n",
      "            The square root of the sum of squares (i.e., the 2-norm) of the\n",
      "            difference between ``grad(x0, *args)`` and the finite difference\n",
      "            approximation of `grad` using func at the points `x0`.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "\n",
      "        :func:`approx_fprime`\n",
      "            ..\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> def func(x):\n",
      "        ...     return x[0]**2 - 0.5 * x[1]**3\n",
      "        >>> def grad(x):\n",
      "        ...     return [2 * x[0], -1.5 * x[1]**2]\n",
      "        >>> from scipy.optimize import check_grad\n",
      "        >>> check_grad(func, grad, [1.5, -1.5])\n",
      "        2.9802322387695312e-08  # may vary\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> check_grad(func, grad, [1.5, -1.5],\n",
      "        ...             direction='random', seed=rng)\n",
      "        2.9802322387695312e-08\n",
      "\n",
      "    curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=None, bounds=(-inf, inf), method=None, jac=None, *, full_output=False, nan_policy=None, **kwargs)\n",
      "        Use non-linear least squares to fit a function, f, to data.\n",
      "\n",
      "        Assumes ``ydata = f(xdata, *params) + eps``.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            The model function, f(x, ...). It must take the independent\n",
      "            variable as the first argument and the parameters to fit as\n",
      "            separate remaining arguments.\n",
      "        xdata : array_like\n",
      "            The independent variable where the data is measured.\n",
      "            Should usually be an M-length sequence or an (k,M)-shaped array for\n",
      "            functions with k predictors, and each element should be float\n",
      "            convertible if it is an array like object.\n",
      "        ydata : array_like\n",
      "            The dependent data, a length M array - nominally ``f(xdata, ...)``.\n",
      "        p0 : array_like, optional\n",
      "            Initial guess for the parameters (length N). If None, then the\n",
      "            initial values will all be 1 (if the number of parameters for the\n",
      "            function can be determined using introspection, otherwise a\n",
      "            ValueError is raised).\n",
      "        sigma : None or scalar or M-length sequence or MxM array, optional\n",
      "            Determines the uncertainty in `ydata`. If we define residuals as\n",
      "            ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n",
      "            depends on its number of dimensions:\n",
      "\n",
      "            - A scalar or 1-D `sigma` should contain values of standard deviations of\n",
      "              errors in `ydata`. In this case, the optimized function is\n",
      "              ``chisq = sum((r / sigma) ** 2)``.\n",
      "\n",
      "            - A 2-D `sigma` should contain the covariance matrix of\n",
      "              errors in `ydata`. In this case, the optimized function is\n",
      "              ``chisq = r.T @ inv(sigma) @ r``.\n",
      "\n",
      "              .. versionadded:: 0.19\n",
      "\n",
      "            None (default) is equivalent of 1-D `sigma` filled with ones.\n",
      "        absolute_sigma : bool, optional\n",
      "            If True, `sigma` is used in an absolute sense and the estimated parameter\n",
      "            covariance `pcov` reflects these absolute values.\n",
      "\n",
      "            If False (default), only the relative magnitudes of the `sigma` values matter.\n",
      "            The returned parameter covariance matrix `pcov` is based on scaling\n",
      "            `sigma` by a constant factor. This constant is set by demanding that the\n",
      "            reduced `chisq` for the optimal parameters `popt` when using the\n",
      "            *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n",
      "            match the sample variance of the residuals after the fit. Default is False.\n",
      "            Mathematically,\n",
      "            ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\n",
      "        check_finite : bool, optional\n",
      "            If True, check that the input arrays do not contain nans of infs,\n",
      "            and raise a ValueError if they do. Setting this parameter to\n",
      "            False may silently produce nonsensical results if the input arrays\n",
      "            do contain nans. Default is True if `nan_policy` is not specified\n",
      "            explicitly and False otherwise.\n",
      "        bounds : 2-tuple of array_like or `Bounds`, optional\n",
      "            Lower and upper bounds on parameters. Defaults to no bounds.\n",
      "            There are two ways to specify the bounds:\n",
      "\n",
      "            - Instance of `Bounds` class.\n",
      "\n",
      "            - 2-tuple of array_like: Each element of the tuple must be either\n",
      "              an array with the length equal to the number of parameters, or a\n",
      "              scalar (in which case the bound is taken to be the same for all\n",
      "              parameters). Use ``np.inf`` with an appropriate sign to disable\n",
      "              bounds on all or some parameters.\n",
      "\n",
      "        method : {'lm', 'trf', 'dogbox'}, optional\n",
      "            Method to use for optimization. See `least_squares` for more details.\n",
      "            Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n",
      "            provided. The method 'lm' won't work when the number of observations\n",
      "            is less than the number of variables, use 'trf' or 'dogbox' in this\n",
      "            case.\n",
      "\n",
      "            .. versionadded:: 0.17\n",
      "        jac : callable, string or None, optional\n",
      "            Function with signature ``jac(x, ...)`` which computes the Jacobian\n",
      "            matrix of the model function with respect to parameters as a dense\n",
      "            array_like structure. It will be scaled according to provided `sigma`.\n",
      "            If None (default), the Jacobian will be estimated numerically.\n",
      "            String keywords for 'trf' and 'dogbox' methods can be used to select\n",
      "            a finite difference scheme, see `least_squares`.\n",
      "\n",
      "            .. versionadded:: 0.18\n",
      "        full_output : boolean, optional\n",
      "            If True, this function returns additional information: `infodict`,\n",
      "            `mesg`, and `ier`.\n",
      "\n",
      "            .. versionadded:: 1.9\n",
      "        nan_policy : {'raise', 'omit', None}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is None):\n",
      "\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "            * None: no special handling of NaNs is performed\n",
      "              (except what is done by check_finite); the behavior when NaNs\n",
      "              are present is implementation-dependent and may change.\n",
      "\n",
      "            Note that if this value is specified explicitly (not None),\n",
      "            `check_finite` will be set as False.\n",
      "\n",
      "            .. versionadded:: 1.11\n",
      "        **kwargs\n",
      "            Keyword arguments passed to `leastsq` for ``method='lm'`` or\n",
      "            `least_squares` otherwise.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        popt : array\n",
      "            Optimal values for the parameters so that the sum of the squared\n",
      "            residuals of ``f(xdata, *popt) - ydata`` is minimized.\n",
      "        pcov : 2-D array\n",
      "            The estimated approximate covariance of popt. The diagonals provide\n",
      "            the variance of the parameter estimate. To compute one standard\n",
      "            deviation errors on the parameters, use\n",
      "            ``perr = np.sqrt(np.diag(pcov))``. Note that the relationship between\n",
      "            `cov` and parameter error estimates is derived based on a linear\n",
      "            approximation to the model function around the optimum [1]_.\n",
      "            When this approximation becomes inaccurate, `cov` may not provide an\n",
      "            accurate measure of uncertainty.\n",
      "\n",
      "            How the `sigma` parameter affects the estimated covariance\n",
      "            depends on `absolute_sigma` argument, as described above.\n",
      "\n",
      "            If the Jacobian matrix at the solution doesn't have a full rank, then\n",
      "            'lm' method returns a matrix filled with ``np.inf``, on the other hand\n",
      "            'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n",
      "            the covariance matrix. Covariance matrices with large condition numbers\n",
      "            (e.g. computed with `numpy.linalg.cond`) may indicate that results are\n",
      "            unreliable.\n",
      "        infodict : dict (returned only if `full_output` is True)\n",
      "            a dictionary of optional outputs with the keys:\n",
      "\n",
      "            ``nfev``\n",
      "                The number of function calls. Methods 'trf' and 'dogbox' do not\n",
      "                count function calls for numerical Jacobian approximation,\n",
      "                as opposed to 'lm' method.\n",
      "            ``fvec``\n",
      "                The residual values evaluated at the solution, for a 1-D `sigma`\n",
      "                this is ``(f(x, *popt) - ydata)/sigma``.\n",
      "            ``fjac``\n",
      "                A permutation of the R matrix of a QR\n",
      "                factorization of the final approximate\n",
      "                Jacobian matrix, stored column wise.\n",
      "                Together with ipvt, the covariance of the\n",
      "                estimate can be approximated.\n",
      "                Method 'lm' only provides this information.\n",
      "            ``ipvt``\n",
      "                An integer array of length N which defines\n",
      "                a permutation matrix, p, such that\n",
      "                fjac*p = q*r, where r is upper triangular\n",
      "                with diagonal elements of nonincreasing\n",
      "                magnitude. Column j of p is column ipvt(j)\n",
      "                of the identity matrix.\n",
      "                Method 'lm' only provides this information.\n",
      "            ``qtf``\n",
      "                The vector (transpose(q) * fvec).\n",
      "                Method 'lm' only provides this information.\n",
      "\n",
      "            .. versionadded:: 1.9\n",
      "        mesg : str (returned only if `full_output` is True)\n",
      "            A string message giving information about the solution.\n",
      "\n",
      "            .. versionadded:: 1.9\n",
      "        ier : int (returned only if `full_output` is True)\n",
      "            An integer flag. If it is equal to 1, 2, 3 or 4, the solution was\n",
      "            found. Otherwise, the solution was not found. In either case, the\n",
      "            optional output variable `mesg` gives more information.\n",
      "\n",
      "            .. versionadded:: 1.9\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            if either `ydata` or `xdata` contain NaNs, or if incompatible options\n",
      "            are used.\n",
      "\n",
      "        RuntimeError\n",
      "            if the least-squares minimization fails.\n",
      "\n",
      "        OptimizeWarning\n",
      "            if covariance of the parameters can not be estimated.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Minimize the sum of squares of nonlinear functions.\n",
      "        scipy.stats.linregress : Calculate a linear least squares regression for\n",
      "                                 two sets of measurements.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Users should ensure that inputs `xdata`, `ydata`, and the output of `f`\n",
      "        are ``float64``, or else the optimization may return incorrect results.\n",
      "\n",
      "        With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\n",
      "        through `leastsq`. Note that this algorithm can only deal with\n",
      "        unconstrained problems.\n",
      "\n",
      "        Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\n",
      "        the docstring of `least_squares` for more information.\n",
      "\n",
      "        Parameters to be fitted must have similar scale. Differences of multiple\n",
      "        orders of magnitude can lead to incorrect results. For the 'trf' and\n",
      "        'dogbox' methods, the `x_scale` keyword argument can be used to scale\n",
      "        the parameters.\n",
      "\n",
      "        `curve_fit` is for local optimization of parameters to minimize the sum of squares\n",
      "        of residuals. For global optimization, other choices of objective function, and\n",
      "        other advanced features, consider using SciPy's :ref:`tutorial_optimize_global`\n",
      "        tools or the `LMFIT <https://lmfit.github.io/lmfit-py/index.html>`_ package.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] K. Vugrin et al. Confidence region estimation techniques for nonlinear\n",
      "               regression in groundwater flow: Three case studies. Water Resources\n",
      "               Research, Vol. 43, W03423, :doi:`10.1029/2005WR004804`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.optimize import curve_fit\n",
      "\n",
      "        >>> def func(x, a, b, c):\n",
      "        ...     return a * np.exp(-b * x) + c\n",
      "\n",
      "        Define the data to be fit with some noise:\n",
      "\n",
      "        >>> xdata = np.linspace(0, 4, 50)\n",
      "        >>> y = func(xdata, 2.5, 1.3, 0.5)\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> y_noise = 0.2 * rng.normal(size=xdata.size)\n",
      "        >>> ydata = y + y_noise\n",
      "        >>> plt.plot(xdata, ydata, 'b-', label='data')\n",
      "\n",
      "        Fit for the parameters a, b, c of the function `func`:\n",
      "\n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata)\n",
      "        >>> popt\n",
      "        array([2.56274217, 1.37268521, 0.47427475])\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'r-',\n",
      "        ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "\n",
      "        Constrain the optimization to the region of ``0 <= a <= 3``,\n",
      "        ``0 <= b <= 1`` and ``0 <= c <= 0.5``:\n",
      "\n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\n",
      "        >>> popt\n",
      "        array([2.43736712, 1.        , 0.34463856])\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'g--',\n",
      "        ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "\n",
      "        >>> plt.xlabel('x')\n",
      "        >>> plt.ylabel('y')\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "\n",
      "        For reliable results, the model `func` should not be overparametrized;\n",
      "        redundant parameters can cause unreliable covariance matrices and, in some\n",
      "        cases, poorer quality fits. As a quick check of whether the model may be\n",
      "        overparameterized, calculate the condition number of the covariance matrix:\n",
      "\n",
      "        >>> np.linalg.cond(pcov)\n",
      "        34.571092161547405  # may vary\n",
      "\n",
      "        The value is small, so it does not raise much concern. If, however, we were\n",
      "        to add a fourth parameter ``d`` to `func` with the same effect as ``a``:\n",
      "\n",
      "        >>> def func2(x, a, b, c, d):\n",
      "        ...     return a * d * np.exp(-b * x) + c  # a and d are redundant\n",
      "        >>> popt, pcov = curve_fit(func2, xdata, ydata)\n",
      "        >>> np.linalg.cond(pcov)\n",
      "        1.13250718925596e+32  # may vary\n",
      "\n",
      "        Such a large value is cause for concern. The diagonal elements of the\n",
      "        covariance matrix, which is related to uncertainty of the fit, gives more\n",
      "        information:\n",
      "\n",
      "        >>> np.diag(pcov)\n",
      "        array([1.48814742e+29, 3.78596560e-02, 5.39253738e-03, 2.76417220e+28])  # may vary\n",
      "\n",
      "        Note that the first and last terms are much larger than the other elements,\n",
      "        suggesting that the optimal values of these parameters are ambiguous and\n",
      "        that only one of these parameters is needed in the model.\n",
      "\n",
      "        If the optimal parameters of `f` differ by multiple orders of magnitude, the\n",
      "        resulting fit can be inaccurate. Sometimes, `curve_fit` can fail to find any\n",
      "        results:\n",
      "\n",
      "        >>> ydata = func(xdata, 500000, 0.01, 15)\n",
      "        >>> try:\n",
      "        ...     popt, pcov = curve_fit(func, xdata, ydata, method = 'trf')\n",
      "        ... except RuntimeError as e:\n",
      "        ...     print(e)\n",
      "        Optimal parameters not found: The maximum number of function evaluations is\n",
      "        exceeded.\n",
      "\n",
      "        If parameter scale is roughly known beforehand, it can be defined in\n",
      "        `x_scale` argument:\n",
      "\n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata, method = 'trf',\n",
      "        ...                        x_scale = [1000, 1, 1])\n",
      "        >>> popt\n",
      "        array([5.00000000e+05, 1.00000000e-02, 1.49999999e+01])\n",
      "\n",
      "    diagbroyden(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using diagonal Broyden Jacobian approximation.\n",
      "\n",
      "        The Jacobian approximation is derived from previous iterations, by\n",
      "        retaining only the diagonal of Broyden matrices.\n",
      "\n",
      "        .. warning::\n",
      "\n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method='diagbroyden'`` in particular.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "\n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "\n",
      "        A solution can be obtained as follows.\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.diagbroyden(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116403, 0.15883384])\n",
      "\n",
      "    differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=1000, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, rng=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0, updating='immediate', workers=1, constraints=(), x0=None, *, integrality=None, vectorized=False, seed=None)\n",
      "        Finds the global minimum of a multivariate function.\n",
      "\n",
      "        The differential evolution method [1]_ is stochastic in nature. It does\n",
      "        not use gradient methods to find the minimum, and can search large areas\n",
      "        of candidate space, but often requires larger numbers of function\n",
      "        evaluations than conventional gradient-based techniques.\n",
      "\n",
      "        The algorithm is due to Storn and Price [2]_.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a tuple of any additional fixed parameters needed to\n",
      "            completely specify the function. The number of parameters, N, is equal\n",
      "            to ``len(x)``.\n",
      "        bounds : sequence or `Bounds`\n",
      "            Bounds for variables. There are two ways to specify the bounds:\n",
      "\n",
      "            1. Instance of `Bounds` class.\n",
      "            2. ``(min, max)`` pairs for each element in ``x``, defining the\n",
      "               finite lower and upper bounds for the optimizing argument of\n",
      "               `func`.\n",
      "\n",
      "            The total number of bounds is used to determine the number of\n",
      "            parameters, N. If there are parameters whose bounds are equal the total\n",
      "            number of free parameters is ``N - N_equal``.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to\n",
      "            completely specify the objective function.\n",
      "        strategy : {str, callable}, optional\n",
      "            The differential evolution strategy to use. Should be one of:\n",
      "\n",
      "            - 'best1bin'\n",
      "            - 'best1exp'\n",
      "            - 'rand1bin'\n",
      "            - 'rand1exp'\n",
      "            - 'rand2bin'\n",
      "            - 'rand2exp'\n",
      "            - 'randtobest1bin'\n",
      "            - 'randtobest1exp'\n",
      "            - 'currenttobest1bin'\n",
      "            - 'currenttobest1exp'\n",
      "            - 'best2exp'\n",
      "            - 'best2bin'\n",
      "\n",
      "            The default is 'best1bin'. Strategies that may be implemented are\n",
      "            outlined in 'Notes'.\n",
      "            Alternatively the differential evolution strategy can be customized by\n",
      "            providing a callable that constructs a trial vector. The callable must\n",
      "            have the form ``strategy(candidate: int, population: np.ndarray, rng=None)``,\n",
      "            where ``candidate`` is an integer specifying which entry of the\n",
      "            population is being evolved, ``population`` is an array of shape\n",
      "            ``(S, N)`` containing all the population members (where S is the\n",
      "            total population size), and ``rng`` is the random number generator\n",
      "            being used within the solver.\n",
      "            ``candidate`` will be in the range ``[0, S)``.\n",
      "            ``strategy`` must return a trial vector with shape ``(N,)``. The\n",
      "            fitness of this trial vector is compared against the fitness of\n",
      "            ``population[candidate]``.\n",
      "\n",
      "            .. versionchanged:: 1.12.0\n",
      "                Customization of evolution strategy via a callable.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of generations over which the entire population is\n",
      "            evolved. The maximum number of function evaluations (with no polishing)\n",
      "            is: ``(maxiter + 1) * popsize * (N - N_equal)``\n",
      "        popsize : int, optional\n",
      "            A multiplier for setting the total population size. The population has\n",
      "            ``popsize * (N - N_equal)`` individuals. This keyword is overridden if\n",
      "            an initial population is supplied via the `init` keyword. When using\n",
      "            ``init='sobol'`` the population size is calculated as the next power\n",
      "            of 2 after ``popsize * (N - N_equal)``.\n",
      "        tol : float, optional\n",
      "            Relative tolerance for convergence, the solving stops when\n",
      "            ``np.std(population_energies) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        mutation : float or tuple(float, float), optional\n",
      "            The mutation constant. In the literature this is also known as\n",
      "            differential weight, being denoted by :math:`F`.\n",
      "            If specified as a float it should be in the range [0, 2).\n",
      "            If specified as a tuple ``(min, max)`` dithering is employed. Dithering\n",
      "            randomly changes the mutation constant on a generation by generation\n",
      "            basis. The mutation constant for that generation is taken from\n",
      "            ``U[min, max)``. Dithering can help speed convergence significantly.\n",
      "            Increasing the mutation constant increases the search radius, but will\n",
      "            slow down convergence.\n",
      "        recombination : float, optional\n",
      "            The recombination constant, should be in the range [0, 1]. In the\n",
      "            literature this is also known as the crossover probability, being\n",
      "            denoted by CR. Increasing this value allows a larger number of mutants\n",
      "            to progress into the next generation, but at the risk of population\n",
      "            stability.\n",
      "        rng : {None, int, `numpy.random.Generator`}, optional\n",
      "            If `rng` is passed by keyword, types other than `numpy.random.Generator` are\n",
      "            passed to `numpy.random.default_rng` to instantiate a ``Generator``.\n",
      "            If `rng` is already a ``Generator`` instance, then the provided instance is\n",
      "            used. Specify `rng` for repeatable function behavior.\n",
      "\n",
      "            If this argument is passed by position or `seed` is passed by keyword,\n",
      "            legacy behavior for the argument `seed` applies:\n",
      "\n",
      "            - If `seed` is None (or `numpy.random`), the `numpy.random.RandomState`\n",
      "              singleton is used.\n",
      "            - If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "              seeded with `seed`.\n",
      "            - If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "              that instance is used.\n",
      "\n",
      "            .. versionchanged:: 1.15.0\n",
      "                As part of the `SPEC-007 <https://scientific-python.org/specs/spec-0007/>`_\n",
      "                transition from use of `numpy.random.RandomState` to\n",
      "                `numpy.random.Generator`, this keyword was changed from `seed` to `rng`.\n",
      "                For an interim period, both keywords will continue to work, although only one\n",
      "                may be specified at a time. After the interim period, function calls using the\n",
      "                `seed` keyword will emit warnings. The behavior of both `seed` and\n",
      "                `rng` are outlined above, but only the `rng` keyword should be used in new code.\n",
      "\n",
      "        disp : bool, optional\n",
      "            Prints the evaluated `func` at every iteration.\n",
      "        callback : callable, optional\n",
      "            A callable called after each iteration. Has the signature::\n",
      "\n",
      "                callback(intermediate_result: OptimizeResult)\n",
      "\n",
      "            where ``intermediate_result`` is a keyword parameter containing an\n",
      "            `OptimizeResult` with attributes ``x`` and ``fun``, the best solution\n",
      "            found so far and the objective function. Note that the name\n",
      "            of the parameter must be ``intermediate_result`` for the callback\n",
      "            to be passed an `OptimizeResult`.\n",
      "\n",
      "            The callback also supports a signature like::\n",
      "\n",
      "                callback(x, convergence: float=val)\n",
      "\n",
      "            ``val`` represents the fractional value of the population convergence.\n",
      "            When ``val`` is greater than ``1.0``, the function halts.\n",
      "\n",
      "            Introspection is used to determine which of the signatures is invoked.\n",
      "\n",
      "            Global minimization will halt if the callback raises ``StopIteration``\n",
      "            or returns ``True``; any polishing is still carried out.\n",
      "\n",
      "            .. versionchanged:: 1.12.0\n",
      "                callback accepts the ``intermediate_result`` keyword.\n",
      "        polish : bool, optional\n",
      "            If True (default), then `scipy.optimize.minimize` with the `L-BFGS-B`\n",
      "            method is used to polish the best population member at the end, which\n",
      "            can improve the minimization slightly. If a constrained problem is\n",
      "            being studied then the `trust-constr` method is used instead. For large\n",
      "            problems with many constraints, polishing can take a long time due to\n",
      "            the Jacobian computations.\n",
      "\n",
      "            .. versionchanged:: 1.15.0\n",
      "                If `workers` is specified then the map-like callable that wraps\n",
      "                `func` is supplied to `minimize` instead of it using `func`\n",
      "                directly. This allows the caller to control how and where the\n",
      "                invocations actually run.\n",
      "        init : str or array-like, optional\n",
      "            Specify which type of population initialization is performed. Should be\n",
      "            one of:\n",
      "\n",
      "            - 'latinhypercube'\n",
      "            - 'sobol'\n",
      "            - 'halton'\n",
      "            - 'random'\n",
      "            - array specifying the initial population. The array should have\n",
      "              shape ``(S, N)``, where S is the total population size and N is\n",
      "              the number of parameters.\n",
      "\n",
      "            `init` is clipped to `bounds` before use.\n",
      "\n",
      "            The default is 'latinhypercube'. Latin Hypercube sampling tries to\n",
      "            maximize coverage of the available parameter space.\n",
      "\n",
      "            'sobol' and 'halton' are superior alternatives and maximize even more\n",
      "            the parameter space. 'sobol' will enforce an initial population\n",
      "            size which is calculated as the next power of 2 after\n",
      "            ``popsize * (N - N_equal)``. 'halton' has no requirements but is a bit\n",
      "            less efficient. See `scipy.stats.qmc` for more details.\n",
      "\n",
      "            'random' initializes the population randomly - this has the drawback\n",
      "            that clustering can occur, preventing the whole of parameter space\n",
      "            being covered. Use of an array to specify a population could be used,\n",
      "            for example, to create a tight bunch of initial guesses in an location\n",
      "            where the solution is known to exist, thereby reducing time for\n",
      "            convergence.\n",
      "        atol : float, optional\n",
      "            Absolute tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        updating : {'immediate', 'deferred'}, optional\n",
      "            If ``'immediate'``, the best solution vector is continuously updated\n",
      "            within a single generation [4]_. This can lead to faster convergence as\n",
      "            trial vectors can take advantage of continuous improvements in the best\n",
      "            solution.\n",
      "            With ``'deferred'``, the best solution vector is updated once per\n",
      "            generation. Only ``'deferred'`` is compatible with parallelization or\n",
      "            vectorization, and the `workers` and `vectorized` keywords can\n",
      "            over-ride this option.\n",
      "\n",
      "            .. versionadded:: 1.2.0\n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the population is subdivided into `workers`\n",
      "            sections and evaluated in parallel\n",
      "            (uses `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply -1 to use all available CPU cores.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the population in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            This option will override the `updating` keyword to\n",
      "            ``updating='deferred'`` if ``workers != 1``.\n",
      "            This option overrides the `vectorized` keyword if ``workers != 1``.\n",
      "            Requires that `func` be pickleable.\n",
      "\n",
      "            .. versionadded:: 1.2.0\n",
      "        constraints : {NonLinearConstraint, LinearConstraint, Bounds}\n",
      "            Constraints on the solver, over and above those applied by the `bounds`\n",
      "            kwd. Uses the approach by Lampinen [5]_.\n",
      "\n",
      "            .. versionadded:: 1.4.0\n",
      "        x0 : None or array-like, optional\n",
      "            Provides an initial guess to the minimization. Once the population has\n",
      "            been initialized this vector replaces the first (best) member. This\n",
      "            replacement is done even if `init` is given an initial population.\n",
      "            ``x0.shape == (N,)``.\n",
      "\n",
      "            .. versionadded:: 1.7.0\n",
      "        integrality : 1-D array, optional\n",
      "            For each decision variable, a boolean value indicating whether the\n",
      "            decision variable is constrained to integer values. The array is\n",
      "            broadcast to ``(N,)``.\n",
      "            If any decision variables are constrained to be integral, they will not\n",
      "            be changed during polishing.\n",
      "            Only integer values lying between the lower and upper bounds are used.\n",
      "            If there are no integer values lying between the bounds then a\n",
      "            `ValueError` is raised.\n",
      "\n",
      "            .. versionadded:: 1.9.0\n",
      "        vectorized : bool, optional\n",
      "            If ``vectorized is True``, `func` is sent an `x` array with\n",
      "            ``x.shape == (N, S)``, and is expected to return an array of shape\n",
      "            ``(S,)``, where `S` is the number of solution vectors to be calculated.\n",
      "            If constraints are applied, each of the functions used to construct\n",
      "            a `Constraint` object should accept an `x` array with\n",
      "            ``x.shape == (N, S)``, and return an array of shape ``(M, S)``, where\n",
      "            `M` is the number of constraint components.\n",
      "            This option is an alternative to the parallelization offered by\n",
      "            `workers`, and may help in optimization speed by reducing interpreter\n",
      "            overhead from multiple function calls. This keyword is ignored if\n",
      "            ``workers != 1``.\n",
      "            This option will override the `updating` keyword to\n",
      "            ``updating='deferred'``.\n",
      "            See the notes section for further discussion on when to use\n",
      "            ``'vectorized'``, and when to use ``'workers'``.\n",
      "\n",
      "            .. versionadded:: 1.9.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully,\n",
      "            ``message`` which describes the cause of the termination,\n",
      "            ``population`` the solution vectors present in the population, and\n",
      "            ``population_energies`` the value of the objective function for each\n",
      "            entry in ``population``.\n",
      "            See `OptimizeResult` for a description of other attributes. If `polish`\n",
      "            was employed, and a lower minimum was obtained by the polishing, then\n",
      "            OptimizeResult also contains the ``jac`` attribute.\n",
      "            If the eventual solution does not satisfy the applied constraints\n",
      "            ``success`` will be `False`.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Differential evolution is a stochastic population based method that is\n",
      "        useful for global optimization problems. At each pass through the\n",
      "        population the algorithm mutates each candidate solution by mixing with\n",
      "        other candidate solutions to create a trial candidate. There are several\n",
      "        strategies [3]_ for creating trial candidates, which suit some problems\n",
      "        more than others. The 'best1bin' strategy is a good starting point for\n",
      "        many systems. In this strategy two members of the population are randomly\n",
      "        chosen. Their difference is used to mutate the best member (the 'best' in\n",
      "        'best1bin'), :math:`x_0`, so far:\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            b' = x_0 + F \\cdot (x_{r_0} - x_{r_1})\n",
      "\n",
      "        where :math:`F` is the `mutation` parameter.\n",
      "        A trial vector is then constructed. Starting with a randomly chosen ith\n",
      "        parameter the trial is sequentially filled (in modulo) with parameters\n",
      "        from ``b'`` or the original candidate. The choice of whether to use ``b'``\n",
      "        or the original candidate is made with a binomial distribution (the 'bin'\n",
      "        in 'best1bin') - a random number in [0, 1) is generated. If this number is\n",
      "        less than the `recombination` constant then the parameter is loaded from\n",
      "        ``b'``, otherwise it is loaded from the original candidate. The final\n",
      "        parameter is always loaded from ``b'``. Once the trial candidate is built\n",
      "        its fitness is assessed. If the trial is better than the original candidate\n",
      "        then it takes its place. If it is also better than the best overall\n",
      "        candidate it also replaces that.\n",
      "\n",
      "        The other strategies available are outlined in Qiang and\n",
      "        Mitchell (2014) [3]_.\n",
      "\n",
      "        - ``rand1`` : :math:`b' = x_{r_0} + F \\cdot (x_{r_1} - x_{r_2})`\n",
      "        - ``rand2`` : :math:`b' = x_{r_0} + F \\cdot (x_{r_1} + x_{r_2} - x_{r_3} - x_{r_4})`\n",
      "        - ``best1`` : :math:`b' = x_0 + F \\cdot (x_{r_0} - x_{r_1})`\n",
      "        - ``best2`` : :math:`b' = x_0 + F \\cdot (x_{r_0} + x_{r_1} - x_{r_2} - x_{r_3})`\n",
      "        - ``currenttobest1`` : :math:`b' = x_i + F \\cdot (x_0 - x_i + x_{r_0} - x_{r_1})`\n",
      "        - ``randtobest1`` : :math:`b' = x_{r_0} + F \\cdot (x_0 - x_{r_0} + x_{r_1} - x_{r_2})`\n",
      "\n",
      "        where the integers :math:`r_0, r_1, r_2, r_3, r_4` are chosen randomly\n",
      "        from the interval [0, NP) with `NP` being the total population size and\n",
      "        the original candidate having index `i`. The user can fully customize the\n",
      "        generation of the trial candidates by supplying a callable to ``strategy``.\n",
      "\n",
      "        To improve your chances of finding a global minimum use higher `popsize`\n",
      "        values, with higher `mutation` and (dithering), but lower `recombination`\n",
      "        values. This has the effect of widening the search radius, but slowing\n",
      "        convergence.\n",
      "\n",
      "        By default the best solution vector is updated continuously within a single\n",
      "        iteration (``updating='immediate'``). This is a modification [4]_ of the\n",
      "        original differential evolution algorithm which can lead to faster\n",
      "        convergence as trial vectors can immediately benefit from improved\n",
      "        solutions. To use the original Storn and Price behaviour, updating the best\n",
      "        solution once per iteration, set ``updating='deferred'``.\n",
      "        The ``'deferred'`` approach is compatible with both parallelization and\n",
      "        vectorization (``'workers'`` and ``'vectorized'`` keywords). These may\n",
      "        improve minimization speed by using computer resources more efficiently.\n",
      "        The ``'workers'`` distribute calculations over multiple processors. By\n",
      "        default the Python `multiprocessing` module is used, but other approaches\n",
      "        are also possible, such as the Message Passing Interface (MPI) used on\n",
      "        clusters [6]_ [7]_. The overhead from these approaches (creating new\n",
      "        Processes, etc) may be significant, meaning that computational speed\n",
      "        doesn't necessarily scale with the number of processors used.\n",
      "        Parallelization is best suited to computationally expensive objective\n",
      "        functions. If the objective function is less expensive, then\n",
      "        ``'vectorized'`` may aid by only calling the objective function once per\n",
      "        iteration, rather than multiple times for all the population members; the\n",
      "        interpreter overhead is reduced.\n",
      "\n",
      "        .. versionadded:: 0.15.0\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Differential evolution, Wikipedia,\n",
      "               http://en.wikipedia.org/wiki/Differential_evolution\n",
      "        .. [2] Storn, R and Price, K, Differential Evolution - a Simple and\n",
      "               Efficient Heuristic for Global Optimization over Continuous Spaces,\n",
      "               Journal of Global Optimization, 1997, 11, 341 - 359.\n",
      "        .. [3] Qiang, J., Mitchell, C., A Unified Differential Evolution Algorithm\n",
      "                for Global Optimization, 2014, https://www.osti.gov/servlets/purl/1163659\n",
      "        .. [4] Wormington, M., Panaccione, C., Matney, K. M., Bowen, D. K., -\n",
      "               Characterization of structures from X-ray scattering data using\n",
      "               genetic algorithms, Phil. Trans. R. Soc. Lond. A, 1999, 357,\n",
      "               2827-2848\n",
      "        .. [5] Lampinen, J., A constraint handling approach for the differential\n",
      "               evolution algorithm. Proceedings of the 2002 Congress on\n",
      "               Evolutionary Computation. CEC'02 (Cat. No. 02TH8600). Vol. 2. IEEE,\n",
      "               2002.\n",
      "        .. [6] https://mpi4py.readthedocs.io/en/stable/\n",
      "        .. [7] https://schwimmbad.readthedocs.io/en/latest/\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function is implemented in `rosen` in `scipy.optimize`.\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import rosen, differential_evolution\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = differential_evolution(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "\n",
      "        Now repeat, but with parallelization.\n",
      "\n",
      "        >>> result = differential_evolution(rosen, bounds, updating='deferred',\n",
      "        ...                                 workers=2)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "\n",
      "        Let's do a constrained minimization.\n",
      "\n",
      "        >>> from scipy.optimize import LinearConstraint, Bounds\n",
      "\n",
      "        We add the constraint that the sum of ``x[0]`` and ``x[1]`` must be less\n",
      "        than or equal to 1.9.  This is a linear constraint, which may be written\n",
      "        ``A @ x <= 1.9``, where ``A = array([[1, 1]])``.  This can be encoded as\n",
      "        a `LinearConstraint` instance:\n",
      "\n",
      "        >>> lc = LinearConstraint([[1, 1]], -np.inf, 1.9)\n",
      "\n",
      "        Specify limits using a `Bounds` object.\n",
      "\n",
      "        >>> bounds = Bounds([0., 0.], [2., 2.])\n",
      "        >>> result = differential_evolution(rosen, bounds, constraints=lc,\n",
      "        ...                                 rng=1)\n",
      "        >>> result.x, result.fun\n",
      "        (array([0.96632622, 0.93367155]), 0.0011352416852625719)\n",
      "\n",
      "        Next find the minimum of the Ackley function\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization).\n",
      "\n",
      "        >>> def ackley(x):\n",
      "        ...     arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))\n",
      "        ...     arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))\n",
      "        ...     return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
      "        >>> bounds = [(-5, 5), (-5, 5)]\n",
      "        >>> result = differential_evolution(ackley, bounds, rng=1)\n",
      "        >>> result.x, result.fun\n",
      "        (array([0., 0.]), 4.440892098500626e-16)\n",
      "\n",
      "        The Ackley function is written in a vectorized manner, so the\n",
      "        ``'vectorized'`` keyword can be employed. Note the reduced number of\n",
      "        function evaluations.\n",
      "\n",
      "        >>> result = differential_evolution(\n",
      "        ...     ackley, bounds, vectorized=True, updating='deferred', rng=1\n",
      "        ... )\n",
      "        >>> result.x, result.fun\n",
      "        (array([0., 0.]), 4.440892098500626e-16)\n",
      "\n",
      "        The following custom strategy function mimics 'best1bin':\n",
      "\n",
      "        >>> def custom_strategy_fn(candidate, population, rng=None):\n",
      "        ...     parameter_count = population.shape(-1)\n",
      "        ...     mutation, recombination = 0.7, 0.9\n",
      "        ...     trial = np.copy(population[candidate])\n",
      "        ...     fill_point = rng.choice(parameter_count)\n",
      "        ...\n",
      "        ...     pool = np.arange(len(population))\n",
      "        ...     rng.shuffle(pool)\n",
      "        ...\n",
      "        ...     # two unique random numbers that aren't the same, and\n",
      "        ...     # aren't equal to candidate.\n",
      "        ...     idxs = []\n",
      "        ...     while len(idxs) < 2 and len(pool) > 0:\n",
      "        ...         idx = pool[0]\n",
      "        ...         pool = pool[1:]\n",
      "        ...         if idx != candidate:\n",
      "        ...             idxs.append(idx)\n",
      "        ...\n",
      "        ...     r0, r1 = idxs[:2]\n",
      "        ...\n",
      "        ...     bprime = (population[0] + mutation *\n",
      "        ...               (population[r0] - population[r1]))\n",
      "        ...\n",
      "        ...     crossovers = rng.uniform(size=parameter_count)\n",
      "        ...     crossovers = crossovers < recombination\n",
      "        ...     crossovers[fill_point] = True\n",
      "        ...     trial = np.where(crossovers, bprime, trial)\n",
      "        ...     return trial\n",
      "\n",
      "    direct(func: Callable[[numpy.ndarray[tuple[int], numpy.dtype[numpy.float64]]], float | numpy.floating[Any] | numpy.integer[Any] | numpy.bool], bounds: Union[Iterable, scipy.optimize._constraints.Bounds], *, args: tuple = (), eps: float = 0.0001, maxfun: int | None = None, maxiter: int = 1000, locally_biased: bool = True, f_min: float = -inf, f_min_rtol: float = 0.0001, vol_tol: float = 1e-16, len_tol: float = 1e-06, callback: Optional[Callable[[numpy.ndarray[tuple[int], numpy.dtype[numpy.float64]]], object]] = None) -> scipy.optimize._optimize.OptimizeResult\n",
      "        Finds the global minimum of a function using the\n",
      "        DIRECT algorithm.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized.\n",
      "            ``func(x, *args) -> float``\n",
      "            where ``x`` is an 1-D array with shape (n,) and ``args`` is a tuple of\n",
      "            the fixed parameters needed to completely specify the function.\n",
      "        bounds : sequence or `Bounds`\n",
      "            Bounds for variables. There are two ways to specify the bounds:\n",
      "\n",
      "            1. Instance of `Bounds` class.\n",
      "            2. ``(min, max)`` pairs for each element in ``x``.\n",
      "\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to\n",
      "            completely specify the objective function.\n",
      "        eps : float, optional\n",
      "            Minimal required difference of the objective function values\n",
      "            between the current best hyperrectangle and the next potentially\n",
      "            optimal hyperrectangle to be divided. In consequence, `eps` serves as a\n",
      "            tradeoff between local and global search: the smaller, the more local\n",
      "            the search becomes. Default is 1e-4.\n",
      "        maxfun : int or None, optional\n",
      "            Approximate upper bound on objective function evaluations.\n",
      "            If `None`, will be automatically set to ``1000 * N`` where ``N``\n",
      "            represents the number of dimensions. Will be capped if necessary to\n",
      "            limit DIRECT's RAM usage to app. 1GiB. This will only occur for very\n",
      "            high dimensional problems and excessive `max_fun`. Default is `None`.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations. Default is 1000.\n",
      "        locally_biased : bool, optional\n",
      "            If `True` (default), use the locally biased variant of the\n",
      "            algorithm known as DIRECT_L. If `False`, use the original unbiased\n",
      "            DIRECT algorithm. For hard problems with many local minima,\n",
      "            `False` is recommended.\n",
      "        f_min : float, optional\n",
      "            Function value of the global optimum. Set this value only if the\n",
      "            global optimum is known. Default is ``-np.inf``, so that this\n",
      "            termination criterion is deactivated.\n",
      "        f_min_rtol : float, optional\n",
      "            Terminate the optimization once the relative error between the\n",
      "            current best minimum `f` and the supplied global minimum `f_min`\n",
      "            is smaller than `f_min_rtol`. This parameter is only used if\n",
      "            `f_min` is also set. Must lie between 0 and 1. Default is 1e-4.\n",
      "        vol_tol : float, optional\n",
      "            Terminate the optimization once the volume of the hyperrectangle\n",
      "            containing the lowest function value is smaller than `vol_tol`\n",
      "            of the complete search space. Must lie between 0 and 1.\n",
      "            Default is 1e-16.\n",
      "        len_tol : float, optional\n",
      "            If ``locally_biased=True``, terminate the optimization once half of\n",
      "            the normalized maximal side length of the hyperrectangle containing\n",
      "            the lowest function value is smaller than `len_tol`.\n",
      "            If ``locally_biased=False``, terminate the optimization once half of\n",
      "            the normalized diagonal of the hyperrectangle containing the lowest\n",
      "            function value is smaller than `len_tol`. Must lie between 0 and 1.\n",
      "            Default is 1e-6.\n",
      "        callback : callable, optional\n",
      "            A callback function with signature ``callback(xk)`` where ``xk``\n",
      "            represents the best function value found so far.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        DIviding RECTangles (DIRECT) is a deterministic global\n",
      "        optimization algorithm capable of minimizing a black box function with\n",
      "        its variables subject to lower and upper bound constraints by sampling\n",
      "        potential solutions in the search space [1]_. The algorithm starts by\n",
      "        normalising the search space to an n-dimensional unit hypercube.\n",
      "        It samples the function at the center of this hypercube and at 2n\n",
      "        (n is the number of variables) more points, 2 in each coordinate\n",
      "        direction. Using these function values, DIRECT then divides the\n",
      "        domain into hyperrectangles, each having exactly one of the sampling\n",
      "        points as its center. In each iteration, DIRECT chooses, using the `eps`\n",
      "        parameter which defaults to 1e-4, some of the existing hyperrectangles\n",
      "        to be further divided. This division process continues until either the\n",
      "        maximum number of iterations or maximum function evaluations allowed\n",
      "        are exceeded, or the hyperrectangle containing the minimal value found\n",
      "        so far becomes small enough. If `f_min` is specified, the optimization\n",
      "        will stop once this function value is reached within a relative tolerance.\n",
      "        The locally biased variant of DIRECT (originally called DIRECT_L) [2]_ is\n",
      "        used by default. It makes the search more locally biased and more\n",
      "        efficient for cases with only a few local minima.\n",
      "\n",
      "        A note about termination criteria: `vol_tol` refers to the volume of the\n",
      "        hyperrectangle containing the lowest function value found so far. This\n",
      "        volume decreases exponentially with increasing dimensionality of the\n",
      "        problem. Therefore `vol_tol` should be decreased to avoid premature\n",
      "        termination of the algorithm for higher dimensions. This does not hold\n",
      "        for `len_tol`: it refers either to half of the maximal side length\n",
      "        (for ``locally_biased=True``) or half of the diagonal of the\n",
      "        hyperrectangle (for ``locally_biased=False``).\n",
      "\n",
      "        This code is based on the DIRECT 2.0.4 Fortran code by Gablonsky et al. at\n",
      "        https://ctk.math.ncsu.edu/SOFTWARE/DIRECTv204.tar.gz .\n",
      "        This original version was initially converted via f2c and then cleaned up\n",
      "        and reorganized by Steven G. Johnson, August 2007, for the NLopt project.\n",
      "        The `direct` function wraps the C implementation.\n",
      "\n",
      "        .. versionadded:: 1.9.0\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Jones, D.R., Perttunen, C.D. & Stuckman, B.E. Lipschitzian\n",
      "            optimization without the Lipschitz constant. J Optim Theory Appl\n",
      "            79, 157-181 (1993).\n",
      "        .. [2] Gablonsky, J., Kelley, C. A Locally-Biased form of the DIRECT\n",
      "            Algorithm. Journal of Global Optimization 21, 27-37 (2001).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a 2-D problem with four local minima: minimizing\n",
      "        the Styblinski-Tang function\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization).\n",
      "\n",
      "        >>> from scipy.optimize import direct, Bounds\n",
      "        >>> def styblinski_tang(pos):\n",
      "        ...     x, y = pos\n",
      "        ...     return 0.5 * (x**4 - 16*x**2 + 5*x + y**4 - 16*y**2 + 5*y)\n",
      "        >>> bounds = Bounds([-4., -4.], [4., 4.])\n",
      "        >>> result = direct(styblinski_tang, bounds)\n",
      "        >>> result.x, result.fun, result.nfev\n",
      "        array([-2.90321597, -2.90321597]), -78.3323279095383, 2011\n",
      "\n",
      "        The correct global minimum was found but with a huge number of function\n",
      "        evaluations (2011). Loosening the termination tolerances `vol_tol` and\n",
      "        `len_tol` can be used to stop DIRECT earlier.\n",
      "\n",
      "        >>> result = direct(styblinski_tang, bounds, len_tol=1e-3)\n",
      "        >>> result.x, result.fun, result.nfev\n",
      "        array([-2.9044353, -2.9044353]), -78.33230330754142, 207\n",
      "\n",
      "    dual_annealing(func, bounds, args=(), maxiter=1000, minimizer_kwargs=None, initial_temp=5230.0, restart_temp_ratio=2e-05, visit=2.62, accept=-5.0, maxfun=10000000.0, rng=None, no_local_search=False, callback=None, x0=None, *, seed=None)\n",
      "        Find the global minimum of a function using Dual Annealing.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a  tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence or `Bounds`\n",
      "            Bounds for variables. There are two ways to specify the bounds:\n",
      "\n",
      "            1. Instance of `Bounds` class.\n",
      "            2. Sequence of ``(min, max)`` pairs for each element in `x`.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify the\n",
      "            objective function.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of global search iterations. Default value is 1000.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Keyword arguments to be passed to the local minimizer\n",
      "            (`minimize`). An important option could be ``method`` for the minimizer\n",
      "            method to use.\n",
      "            If no keyword arguments are provided, the local minimizer defaults to\n",
      "            'L-BFGS-B' and uses the already supplied bounds. If `minimizer_kwargs`\n",
      "            is specified, then the dict must contain all parameters required to\n",
      "            control the local minimization. `args` is ignored in this dict, as it is\n",
      "            passed automatically. `bounds` is not automatically passed on to the\n",
      "            local minimizer as the method may not support them.\n",
      "        initial_temp : float, optional\n",
      "            The initial temperature, use higher values to facilitates a wider\n",
      "            search of the energy landscape, allowing dual_annealing to escape\n",
      "            local minima that it is trapped in. Default value is 5230. Range is\n",
      "            (0.01, 5.e4].\n",
      "        restart_temp_ratio : float, optional\n",
      "            During the annealing process, temperature is decreasing, when it\n",
      "            reaches ``initial_temp * restart_temp_ratio``, the reannealing process\n",
      "            is triggered. Default value of the ratio is 2e-5. Range is (0, 1).\n",
      "        visit : float, optional\n",
      "            Parameter for visiting distribution. Default value is 2.62. Higher\n",
      "            values give the visiting distribution a heavier tail, this makes\n",
      "            the algorithm jump to a more distant region. The value range is (1, 3].\n",
      "        accept : float, optional\n",
      "            Parameter for acceptance distribution. It is used to control the\n",
      "            probability of acceptance. The lower the acceptance parameter, the\n",
      "            smaller the probability of acceptance. Default value is -5.0 with\n",
      "            a range (-1e4, -5].\n",
      "        maxfun : int, optional\n",
      "            Soft limit for the number of objective function calls. If the\n",
      "            algorithm is in the middle of a local search, this number will be\n",
      "            exceeded, the algorithm will stop just after the local search is\n",
      "            done. Default value is 1e7.\n",
      "        rng : {None, int, `numpy.random.Generator`}, optional\n",
      "            If `rng` is passed by keyword, types other than `numpy.random.Generator` are\n",
      "            passed to `numpy.random.default_rng` to instantiate a ``Generator``.\n",
      "            If `rng` is already a ``Generator`` instance, then the provided instance is\n",
      "            used. Specify `rng` for repeatable function behavior.\n",
      "\n",
      "            If this argument is passed by position or `seed` is passed by keyword,\n",
      "            legacy behavior for the argument `seed` applies:\n",
      "\n",
      "            - If `seed` is None (or `numpy.random`), the `numpy.random.RandomState`\n",
      "              singleton is used.\n",
      "            - If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "              seeded with `seed`.\n",
      "            - If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "              that instance is used.\n",
      "\n",
      "            .. versionchanged:: 1.15.0\n",
      "                As part of the `SPEC-007 <https://scientific-python.org/specs/spec-0007/>`_\n",
      "                transition from use of `numpy.random.RandomState` to\n",
      "                `numpy.random.Generator`, this keyword was changed from `seed` to `rng`.\n",
      "                For an interim period, both keywords will continue to work, although only one\n",
      "                may be specified at a time. After the interim period, function calls using the\n",
      "                `seed` keyword will emit warnings. The behavior of both `seed` and\n",
      "                `rng` are outlined above, but only the `rng` keyword should be used in new code.\n",
      "\n",
      "            Specify `rng` for repeatable minimizations. The random numbers\n",
      "            generated only affect the visiting distribution function\n",
      "            and new coordinates generation.\n",
      "        no_local_search : bool, optional\n",
      "            If `no_local_search` is set to True, a traditional Generalized\n",
      "            Simulated Annealing will be performed with no local search\n",
      "            strategy applied.\n",
      "        callback : callable, optional\n",
      "            A callback function with signature ``callback(x, f, context)``,\n",
      "            which will be called for all minima found.\n",
      "            ``x`` and ``f`` are the coordinates and function value of the\n",
      "            latest minimum found, and ``context`` has one of the following\n",
      "            values:\n",
      "\n",
      "            - ``0``: minimum detected in the annealing process.\n",
      "            - ``1``: detection occurred in the local search process.\n",
      "            - ``2``: detection done in the dual annealing process.\n",
      "\n",
      "            If the callback implementation returns True, the algorithm will stop.\n",
      "        x0 : ndarray, shape(n,), optional\n",
      "            Coordinates of a single N-D starting point.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``fun`` the value\n",
      "            of the function at the solution, and ``message`` which describes the\n",
      "            cause of the termination.\n",
      "            See `OptimizeResult` for a description of other attributes.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This function implements the Dual Annealing optimization. This stochastic\n",
      "        approach derived from [3]_ combines the generalization of CSA (Classical\n",
      "        Simulated Annealing) and FSA (Fast Simulated Annealing) [1]_ [2]_ coupled\n",
      "        to a strategy for applying a local search on accepted locations [4]_.\n",
      "        An alternative implementation of this same algorithm is described in [5]_\n",
      "        and benchmarks are presented in [6]_. This approach introduces an advanced\n",
      "        method to refine the solution found by the generalized annealing\n",
      "        process. This algorithm uses a distorted Cauchy-Lorentz visiting\n",
      "        distribution, with its shape controlled by the parameter :math:`q_{v}`\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            g_{q_{v}}(\\Delta x(t)) \\propto \\frac{ \\\n",
      "            \\left[T_{q_{v}}(t) \\right]^{-\\frac{D}{3-q_{v}}}}{ \\\n",
      "            \\left[{1+(q_{v}-1)\\frac{(\\Delta x(t))^{2}} { \\\n",
      "            \\left[T_{q_{v}}(t)\\right]^{\\frac{2}{3-q_{v}}}}}\\right]^{ \\\n",
      "            \\frac{1}{q_{v}-1}+\\frac{D-1}{2}}}\n",
      "\n",
      "        Where :math:`t` is the artificial time. This visiting distribution is used\n",
      "        to generate a trial jump distance :math:`\\Delta x(t)` of variable\n",
      "        :math:`x(t)` under artificial temperature :math:`T_{q_{v}}(t)`.\n",
      "\n",
      "        From the starting point, after calling the visiting distribution\n",
      "        function, the acceptance probability is computed as follows:\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            p_{q_{a}} = \\min{\\{1,\\left[1-(1-q_{a}) \\beta \\Delta E \\right]^{ \\\n",
      "            \\frac{1}{1-q_{a}}}\\}}\n",
      "\n",
      "        Where :math:`q_{a}` is a acceptance parameter. For :math:`q_{a}<1`, zero\n",
      "        acceptance probability is assigned to the cases where\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            [1-(1-q_{a}) \\beta \\Delta E] < 0\n",
      "\n",
      "        The artificial temperature :math:`T_{q_{v}}(t)` is decreased according to\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            T_{q_{v}}(t) = T_{q_{v}}(1) \\frac{2^{q_{v}-1}-1}{\\left( \\\n",
      "            1 + t\\right)^{q_{v}-1}-1}\n",
      "\n",
      "        Where :math:`q_{v}` is the visiting parameter.\n",
      "\n",
      "        .. versionadded:: 1.2.0\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsallis C. Possible generalization of Boltzmann-Gibbs\n",
      "            statistics. Journal of Statistical Physics, 52, 479-487 (1988).\n",
      "        .. [2] Tsallis C, Stariolo DA. Generalized Simulated Annealing.\n",
      "            Physica A, 233, 395-406 (1996).\n",
      "        .. [3] Xiang Y, Sun DY, Fan W, Gong XG. Generalized Simulated\n",
      "            Annealing Algorithm and Its Application to the Thomson Model.\n",
      "            Physics Letters A, 233, 216-220 (1997).\n",
      "        .. [4] Xiang Y, Gong XG. Efficiency of Generalized Simulated\n",
      "            Annealing. Physical Review E, 62, 4473 (2000).\n",
      "        .. [5] Xiang Y, Gubian S, Suomela B, Hoeng J. Generalized\n",
      "            Simulated Annealing for Efficient Global Optimization: the GenSA\n",
      "            Package for R. The R Journal, Volume 5/1 (2013).\n",
      "        .. [6] Mullen, K. Continuous Global Optimization in R. Journal of\n",
      "            Statistical Software, 60(6), 1 - 45, (2014).\n",
      "            :doi:`10.18637/jss.v060.i06`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a 10-D problem, with many local minima.\n",
      "        The function involved is called Rastrigin\n",
      "        (https://en.wikipedia.org/wiki/Rastrigin_function)\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import dual_annealing\n",
      "        >>> func = lambda x: np.sum(x*x - 10*np.cos(2*np.pi*x)) + 10*np.size(x)\n",
      "        >>> lw = [-5.12] * 10\n",
      "        >>> up = [5.12] * 10\n",
      "        >>> ret = dual_annealing(func, bounds=list(zip(lw, up)))\n",
      "        >>> ret.x\n",
      "        array([-4.26437714e-09, -3.91699361e-09, -1.86149218e-09, -3.97165720e-09,\n",
      "               -6.29151648e-09, -6.53145322e-09, -3.93616815e-09, -6.55623025e-09,\n",
      "               -6.05775280e-09, -5.00668935e-09]) # random\n",
      "        >>> ret.fun\n",
      "        0.000000\n",
      "\n",
      "    excitingmixing(F, xin, iter=None, alpha=None, alphamax=1.0, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a tuned diagonal Jacobian approximation.\n",
      "\n",
      "        The Jacobian matrix is diagonal and is tuned on each iteration.\n",
      "\n",
      "        .. warning::\n",
      "\n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method='excitingmixing'`` in particular.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial Jacobian approximation is (-1/alpha).\n",
      "        alphamax : float, optional\n",
      "            The entries of the diagonal Jacobian are kept in the range\n",
      "            ``[alpha, alphamax]``.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "\n",
      "    fixed_point(func, x0, args=(), xtol=1e-08, maxiter=500, method='del2')\n",
      "        Find a fixed point of the function.\n",
      "\n",
      "        Given a function of one or more variables and a starting point, find a\n",
      "        fixed point of the function: i.e., where ``func(x0) == x0``.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            Function to evaluate.\n",
      "        x0 : array_like\n",
      "            Fixed point of function.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to `func`.\n",
      "        xtol : float, optional\n",
      "            Convergence tolerance, defaults to 1e-08.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations, defaults to 500.\n",
      "        method : {\"del2\", \"iteration\"}, optional\n",
      "            Method of finding the fixed-point, defaults to \"del2\",\n",
      "            which uses Steffensen's Method with Aitken's ``Del^2``\n",
      "            convergence acceleration [1]_. The \"iteration\" method simply iterates\n",
      "            the function until convergence is detected, without attempting to\n",
      "            accelerate the convergence.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Burden, Faires, \"Numerical Analysis\", 5th edition, pg. 80\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c1, c2):\n",
      "        ...    return np.sqrt(c1/(x+c2))\n",
      "        >>> c1 = np.array([10,12.])\n",
      "        >>> c2 = np.array([3, 5.])\n",
      "        >>> optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))\n",
      "        array([ 1.4920333 ,  1.37228132])\n",
      "\n",
      "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, initial_simplex=None)\n",
      "        Minimize a function using the downhill simplex algorithm.\n",
      "\n",
      "        This algorithm only uses function values, not derivatives or second\n",
      "        derivatives.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            The objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func, i.e., ``f(x,*args)``.\n",
      "        xtol : float, optional\n",
      "            Absolute error in xopt between iterations that is acceptable for\n",
      "            convergence.\n",
      "        ftol : number, optional\n",
      "            Absolute error in func(xopt) between iterations that is acceptable for\n",
      "            convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : number, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            Set to True if fopt and warnflag outputs are desired.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages.\n",
      "        retall : bool, optional\n",
      "            Set to True to return list of solutions at each iteration.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        initial_simplex : array_like of shape (N + 1, N), optional\n",
      "            Initial simplex. If given, overrides `x0`.\n",
      "            ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "            the jth vertex of the ``N+1`` vertices in the simplex, where\n",
      "            ``N`` is the dimension.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter that minimizes function.\n",
      "        fopt : float\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        iter : int\n",
      "            Number of iterations performed.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            1 : Maximum number of function evaluations made.\n",
      "            2 : Maximum number of iterations reached.\n",
      "        allvecs : list\n",
      "            Solution at each iteration.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Nelder-Mead' `method` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
      "        one or more variables.\n",
      "\n",
      "        This algorithm has a long history of successful use in applications.\n",
      "        But it will usually be slower than an algorithm that uses first or\n",
      "        second derivative information. In practice, it can have poor\n",
      "        performance in high-dimensional problems and is not robust to\n",
      "        minimizing complicated functions. Additionally, there currently is no\n",
      "        complete theory describing when the algorithm will successfully\n",
      "        converge to the minimum, or how fast it will if it does. Both the ftol and\n",
      "        xtol criteria must be met for convergence.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "\n",
      "        >>> minimum = optimize.fmin(f, 1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 17\n",
      "                 Function evaluations: 34\n",
      "        >>> minimum[0]\n",
      "        -8.8817841970012523e-16\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
      "               minimization\", The Computer Journal, 7, pp. 308-313\n",
      "\n",
      "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
      "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
      "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
      "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
      "               Harlow, UK, pp. 191-208.\n",
      "\n",
      "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=np.float64(1.4901161193847656e-08), maxiter=None, full_output=0, disp=1, retall=0, callback=None, xrtol=0, c1=0.0001, c2=0.9, hess_inv0=None)\n",
      "        Minimize a function using the BFGS algorithm.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable ``f(x,*args)``\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess, shape (n,)\n",
      "        fprime : callable ``f'(x,*args)``, optional\n",
      "            Gradient of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f and fprime.\n",
      "        gtol : float, optional\n",
      "            Terminate successfully if gradient norm is less than `gtol`\n",
      "        norm : float, optional\n",
      "            Order of norm (Inf is max, -Inf is min)\n",
      "        epsilon : int or ndarray, optional\n",
      "            If `fprime` is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function to call after each\n",
      "            iteration. Called as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True, return ``fopt``, ``func_calls``, ``grad_calls``, and\n",
      "            ``warnflag`` in addition to ``xopt``.\n",
      "        disp : bool, optional\n",
      "            Print convergence message if True.\n",
      "        retall : bool, optional\n",
      "            Return a list of results at each iteration if True.\n",
      "        xrtol : float, default: 0\n",
      "            Relative tolerance for `x`. Terminate successfully if step\n",
      "            size is less than ``xk * xrtol`` where ``xk`` is the current\n",
      "            parameter vector.\n",
      "        c1 : float, default: 1e-4\n",
      "            Parameter for Armijo condition rule.\n",
      "        c2 : float, default: 0.9\n",
      "            Parameter for curvature condition rule.\n",
      "        hess_inv0 : None or ndarray, optional``\n",
      "            Initial inverse hessian estimate, shape (n, n). If None (default) then\n",
      "            the identity matrix is used.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., ``f(xopt) == fopt``.\n",
      "        fopt : float\n",
      "            Minimum value.\n",
      "        gopt : ndarray\n",
      "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
      "        Bopt : ndarray\n",
      "            Value of 1/f''(xopt), i.e., the inverse Hessian matrix.\n",
      "        func_calls : int\n",
      "            Number of function_calls made.\n",
      "        grad_calls : int\n",
      "            Number of gradient calls made.\n",
      "        warnflag : integer\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Gradient and/or function calls not changing.\n",
      "            3 : NaN result encountered.\n",
      "        allvecs : list\n",
      "            The value of `xopt` at each iteration. Only returned if `retall` is\n",
      "            True.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Optimize the function, `f`, whose gradient is given by `fprime`\n",
      "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
      "        and Shanno (BFGS).\n",
      "\n",
      "        Parameters `c1` and `c2` must satisfy ``0 < c1 < c2 < 1``.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See ``method='BFGS'`` in particular.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        Wright, and Nocedal 'Numerical Optimization', 1999, p. 198.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import fmin_bfgs\n",
      "        >>> def quadratic_cost(x, Q):\n",
      "        ...     return x @ Q @ x\n",
      "        ...\n",
      "        >>> x0 = np.array([-3, -4])\n",
      "        >>> cost_weight =  np.diag([1., 10.])\n",
      "        >>> # Note that a trailing comma is necessary for a tuple with single element\n",
      "        >>> fmin_bfgs(quadratic_cost, x0, args=(cost_weight,))\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 0.000000\n",
      "                Iterations: 7                   # may vary\n",
      "                Function evaluations: 24        # may vary\n",
      "                Gradient evaluations: 8         # may vary\n",
      "        array([ 2.85169950e-06, -4.61820139e-07])\n",
      "\n",
      "        >>> def quadratic_cost_grad(x, Q):\n",
      "        ...     return 2 * Q @ x\n",
      "        ...\n",
      "        >>> fmin_bfgs(quadratic_cost, x0, quadratic_cost_grad, args=(cost_weight,))\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 0.000000\n",
      "                Iterations: 7\n",
      "                Function evaluations: 8\n",
      "                Gradient evaluations: 8\n",
      "        array([ 2.85916637e-06, -4.54371951e-07])\n",
      "\n",
      "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=np.float64(1.4901161193847656e-08), maxiter=None, full_output=0, disp=1, retall=0, callback=None, c1=0.0001, c2=0.4)\n",
      "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable, ``f(x, *args)``\n",
      "            Objective function to be minimized. Here `x` must be a 1-D array of\n",
      "            the variables that are to be changed in the search for a minimum, and\n",
      "            `args` are the other (fixed) parameters of `f`.\n",
      "        x0 : ndarray\n",
      "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
      "            It must be a 1-D array of values.\n",
      "        fprime : callable, ``fprime(x, *args)``, optional\n",
      "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
      "            are as described above for `f`. The returned value must be a 1-D array.\n",
      "            Defaults to None, in which case the gradient is approximated\n",
      "            numerically (see `epsilon`, below).\n",
      "        args : tuple, optional\n",
      "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
      "            additional fixed parameters are needed to completely specify the\n",
      "            functions `f` and `fprime`.\n",
      "        gtol : float, optional\n",
      "            Stop when the norm of the gradient is less than `gtol`.\n",
      "        norm : float, optional\n",
      "            Order to use for the norm of the gradient\n",
      "            (``-np.inf`` is min, ``np.inf`` is max).\n",
      "        epsilon : float or ndarray, optional\n",
      "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
      "            scalar or a 1-D array. Defaults to ``sqrt(eps)``, with eps the\n",
      "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
      "            1.5e-8.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
      "        full_output : bool, optional\n",
      "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
      "            addition to `xopt`.  See the Returns section below for additional\n",
      "            information on optional return values.\n",
      "        disp : bool, optional\n",
      "            If True, return a convergence message, followed by `xopt`.\n",
      "        retall : bool, optional\n",
      "            If True, add to the returned values the results of each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each iteration.\n",
      "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
      "        c1 : float, default: 1e-4\n",
      "            Parameter for Armijo condition rule.\n",
      "        c2 : float, default: 0.4\n",
      "            Parameter for curvature condition rule.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., ``f(xopt) == fopt``.\n",
      "        fopt : float, optional\n",
      "            Minimum value found, f(xopt). Only returned if `full_output` is True.\n",
      "        func_calls : int, optional\n",
      "            The number of function_calls made. Only returned if `full_output`\n",
      "            is True.\n",
      "        grad_calls : int, optional\n",
      "            The number of gradient calls made. Only returned if `full_output` is\n",
      "            True.\n",
      "        warnflag : int, optional\n",
      "            Integer value with warning status, only returned if `full_output` is\n",
      "            True.\n",
      "\n",
      "            0 : Success.\n",
      "\n",
      "            1 : The maximum number of iterations was exceeded.\n",
      "\n",
      "            2 : Gradient and/or function calls were not changing. May indicate\n",
      "                that precision was lost, i.e., the routine did not converge.\n",
      "\n",
      "            3 : NaN result encountered.\n",
      "\n",
      "        allvecs : list of ndarray, optional\n",
      "            List of arrays, containing the results at each iteration.\n",
      "            Only returned if `retall` is True.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        minimize : common interface to all `scipy.optimize` algorithms for\n",
      "                   unconstrained and constrained minimization of multivariate\n",
      "                   functions. It provides an alternative way to call\n",
      "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
      "        [1]_.\n",
      "\n",
      "        Conjugate gradient methods tend to work better when:\n",
      "\n",
      "        1. `f` has a unique global minimizing point, and no local minima or\n",
      "           other stationary points,\n",
      "        2. `f` is, at least locally, reasonably well approximated by a\n",
      "           quadratic function of the variables,\n",
      "        3. `f` is continuous and has a continuous gradient,\n",
      "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
      "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
      "           minimizing point, `xopt`.\n",
      "\n",
      "        Parameters `c1` and `c2` must satisfy ``0 < c1 < c2 < 1``.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Example 1: seek the minimum value of the expression\n",
      "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
      "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
      "        >>> def f(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
      "        >>> def gradf(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
      "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
      "        ...     return np.asarray((gu, gv))\n",
      "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
      "        >>> from scipy import optimize\n",
      "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 1.617021\n",
      "                 Iterations: 4\n",
      "                 Function evaluations: 8\n",
      "                 Gradient evaluations: 8\n",
      "        >>> res1\n",
      "        array([-1.80851064, -0.25531915])\n",
      "\n",
      "        Example 2: solve the same problem using the `minimize` function.\n",
      "        (This `myopts` dictionary shows all of the available options,\n",
      "        although in practice only non-default values would be needed.\n",
      "        The returned value will be a dictionary.)\n",
      "\n",
      "        >>> opts = {'maxiter' : None,    # default value.\n",
      "        ...         'disp' : True,    # non-default value.\n",
      "        ...         'gtol' : 1e-5,    # default value.\n",
      "        ...         'norm' : np.inf,  # default value.\n",
      "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
      "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
      "        ...                          method='CG', options=opts)\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 1.617021\n",
      "                Iterations: 4\n",
      "                Function evaluations: 8\n",
      "                Gradient evaluations: 8\n",
      "        >>> res2.x  # minimum found\n",
      "        array([-1.80851064, -0.25531915])\n",
      "\n",
      "    fmin_cobyla(func, x0, cons, args=(), consargs=None, rhobeg=1.0, rhoend=0.0001, maxfun=1000, disp=None, catol=0.0002, *, callback=None)\n",
      "        Minimize a function using the Constrained Optimization By Linear\n",
      "        Approximation (COBYLA) method. This method uses the pure-python implementation\n",
      "        of the algorithm from PRIMA.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            Function to minimize. In the form func(x, \\*args).\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        cons : sequence\n",
      "            Constraint functions; must all be ``>=0`` (a single function\n",
      "            if only 1 constraint). Each function takes the parameters `x`\n",
      "            as its first argument, and it can return either a single number or\n",
      "            an array or list of numbers.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to function.\n",
      "        consargs : tuple, optional\n",
      "            Extra arguments to pass to constraint functions (default of None means\n",
      "            use same extra arguments as those passed to func).\n",
      "            Use ``()`` for no extra arguments.\n",
      "        rhobeg : float, optional\n",
      "            Reasonable initial changes to the variables.\n",
      "        rhoend : float, optional\n",
      "            Final accuracy in the optimization (not precisely guaranteed). This\n",
      "            is a lower bound on the size of the trust region.\n",
      "        disp : {0, 1, 2, 3}, optional\n",
      "            Controls the frequency of output; 0 implies no output.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        catol : float, optional\n",
      "            Absolute tolerance for constraint violations.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(x)``, where ``x`` is the\n",
      "            current parameter vector.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The argument that minimises `f`.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'COBYLA' `method` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm is based on linear approximations to the objective\n",
      "        function and each constraint. We briefly describe the algorithm.\n",
      "\n",
      "        Suppose the function is being minimized over k variables. At the\n",
      "        jth iteration the algorithm has k+1 points v_1, ..., v_(k+1),\n",
      "        an approximate solution x_j, and a radius RHO_j.\n",
      "        (i.e., linear plus a constant) approximations to the objective\n",
      "        function and constraint functions such that their function values\n",
      "        agree with the linear approximation on the k+1 points v_1,.., v_(k+1).\n",
      "        This gives a linear program to solve (where the linear approximations\n",
      "        of the constraint functions are constrained to be non-negative).\n",
      "\n",
      "        However, the linear approximations are likely only good\n",
      "        approximations near the current simplex, so the linear program is\n",
      "        given the further requirement that the solution, which\n",
      "        will become x_(j+1), must be within RHO_j from x_j. RHO_j only\n",
      "        decreases, never increases. The initial RHO_j is rhobeg and the\n",
      "        final RHO_j is rhoend. In this way COBYLA's iterations behave\n",
      "        like a trust region algorithm.\n",
      "\n",
      "        Additionally, the linear program may be inconsistent, or the\n",
      "        approximation may give poor improvement. For details about\n",
      "        how these issues are resolved, as well as how the points v_i are\n",
      "        updated, refer to the source code or the references below.\n",
      "\n",
      "            .. versionchanged:: 1.16.0\n",
      "                The original Powell implementation was replaced by a pure\n",
      "                Python version from the PRIMA package, with bug fixes and\n",
      "                improvements being made.\n",
      "\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1994), \"A direct search optimization method that models\n",
      "        the objective and constraint functions by linear interpolation.\", in\n",
      "        Advances in Optimization and Numerical Analysis, eds. S. Gomez and\n",
      "        J-P Hennart, Kluwer Academic (Dordrecht), pp. 51-67\n",
      "\n",
      "        Powell M.J.D. (1998), \"Direct search algorithms for optimization\n",
      "        calculations\", Acta Numerica 7, 287-336\n",
      "\n",
      "        Powell M.J.D. (2007), \"A view of algorithms for optimization without\n",
      "        derivatives\", Cambridge University Technical Report DAMTP 2007/NA03\n",
      "\n",
      "        Zhang Z. (2023), \"PRIMA: Reference Implementation for Powell's Methods with\n",
      "        Modernization and Amelioration\", https://www.libprima.net,\n",
      "        :doi:`10.5281/zenodo.8052654`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Minimize the objective function f(x,y) = x*y subject\n",
      "        to the constraints x**2 + y**2 < 1 and y > 0::\n",
      "\n",
      "            >>> def objective(x):\n",
      "            ...     return x[0]*x[1]\n",
      "            ...\n",
      "            >>> def constr1(x):\n",
      "            ...     return 1 - (x[0]**2 + x[1]**2)\n",
      "            ...\n",
      "            >>> def constr2(x):\n",
      "            ...     return x[1]\n",
      "            ...\n",
      "            >>> from scipy.optimize import fmin_cobyla\n",
      "            >>> fmin_cobyla(objective, [0.0, 0.1], [constr1, constr2], rhoend=1e-7)\n",
      "            array([-0.70710685,  0.70710671])\n",
      "\n",
      "        The exact solution is (-sqrt(2)/2, sqrt(2)/2).\n",
      "\n",
      "    fmin_l_bfgs_b(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, m=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08, iprint=<object object at 0x147f08e20>, maxfun=15000, maxiter=15000, disp=<object object at 0x147f08e20>, callback=None, maxls=20)\n",
      "        Minimize a function func using the L-BFGS-B algorithm.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Function to minimize.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable fprime(x,*args), optional\n",
      "            The gradient of `func`. If None, then `func` returns the function\n",
      "            value and the gradient (``f, g = func(x, *args)``), unless\n",
      "            `approx_grad` is True in which case `func` returns only ``f``.\n",
      "        args : sequence, optional\n",
      "            Arguments to pass to `func` and `fprime`.\n",
      "        approx_grad : bool, optional\n",
      "            Whether to approximate the gradient numerically (in which case\n",
      "            `func` returns only the function value).\n",
      "        bounds : list, optional\n",
      "            ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the bounds on that parameter. Use None or +-inf for one of ``min`` or\n",
      "            ``max`` when there is no bound in that direction.\n",
      "        m : int, optional\n",
      "            The maximum number of variable metric corrections\n",
      "            used to define the limited memory matrix. (The limited memory BFGS\n",
      "            method does not store the full hessian but uses this many terms in an\n",
      "            approximation to it.)\n",
      "        factr : float, optional\n",
      "            The iteration stops when\n",
      "            ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
      "            where ``eps`` is the machine precision, which is automatically\n",
      "            generated by the code. Typical values for `factr` are: 1e12 for\n",
      "            low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
      "            high accuracy. See Notes for relationship to `ftol`, which is exposed\n",
      "            (instead of `factr`) by the `scipy.optimize.minimize` interface to\n",
      "            L-BFGS-B.\n",
      "        pgtol : float, optional\n",
      "            The iteration will stop when\n",
      "            ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
      "            where ``proj g_i`` is the i-th component of the projected gradient.\n",
      "        epsilon : float, optional\n",
      "            Step size used when `approx_grad` is True, for numerically\n",
      "            calculating the gradient\n",
      "        iprint : int, optional\n",
      "            Deprecated option that previously controlled the text printed on the\n",
      "            screen during the problem solution. Now the code does not emit any\n",
      "            output and this keyword has no function.\n",
      "\n",
      "            .. deprecated:: 1.15.0\n",
      "                This keyword is deprecated and will be removed from SciPy 1.18.0.\n",
      "\n",
      "        disp : int, optional\n",
      "            Deprecated option that previously controlled the text printed on the\n",
      "            screen during the problem solution. Now the code does not emit any\n",
      "            output and this keyword has no function.\n",
      "\n",
      "            .. deprecated:: 1.15.0\n",
      "                This keyword is deprecated and will be removed from SciPy 1.18.0.\n",
      "\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations. Note that this function\n",
      "            may violate the limit because of evaluating gradients by numerical\n",
      "            differentiation.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        maxls : int, optional\n",
      "            Maximum number of line search steps (per iteration). Default is 20.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        x : array_like\n",
      "            Estimated position of the minimum.\n",
      "        f : float\n",
      "            Value of `func` at the minimum.\n",
      "        d : dict\n",
      "            Information dictionary.\n",
      "\n",
      "            * d['warnflag'] is\n",
      "\n",
      "              - 0 if converged,\n",
      "              - 1 if too many function evaluations or too many iterations,\n",
      "              - 2 if stopped for another reason, given in d['task']\n",
      "\n",
      "            * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
      "            * d['funcalls'] is the number of function calls made.\n",
      "            * d['nit'] is the number of iterations.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'L-BFGS-B' `method` in particular. Note that the\n",
      "            `ftol` option is made available via that interface, while `factr` is\n",
      "            provided via this interface, where `factr` is the factor multiplying\n",
      "            the default machine floating-point precision to arrive at `ftol`:\n",
      "            ``ftol = factr * numpy.finfo(float).eps``.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        SciPy uses a C-translated and modified version of the Fortran code,\n",
      "        L-BFGS-B v3.0 (released April 25, 2011, BSD-3 licensed). Original Fortran\n",
      "        version was written by Ciyou Zhu, Richard Byrd, Jorge Nocedal and,\n",
      "        Jose Luis Morales.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
      "          Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
      "          Statistical Computing, 16, 5, pp. 1190-1208.\n",
      "        * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (1997),\n",
      "          ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
      "        * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (2011),\n",
      "          ACM Transactions on Mathematical Software, 38, 1.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Solve a linear regression problem via `fmin_l_bfgs_b`. To do this, first we\n",
      "        define an objective function ``f(m, b) = (y - y_model)**2``, where `y`\n",
      "        describes the observations and `y_model` the prediction of the linear model\n",
      "        as ``y_model = m*x + b``. The bounds for the parameters, ``m`` and ``b``,\n",
      "        are arbitrarily chosen as ``(0,5)`` and ``(5,10)`` for this example.\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import fmin_l_bfgs_b\n",
      "        >>> X = np.arange(0, 10, 1)\n",
      "        >>> M = 2\n",
      "        >>> B = 3\n",
      "        >>> Y = M * X + B\n",
      "        >>> def func(parameters, *args):\n",
      "        ...     x = args[0]\n",
      "        ...     y = args[1]\n",
      "        ...     m, b = parameters\n",
      "        ...     y_model = m*x + b\n",
      "        ...     error = sum(np.power((y - y_model), 2))\n",
      "        ...     return error\n",
      "\n",
      "        >>> initial_values = np.array([0.0, 1.0])\n",
      "\n",
      "        >>> x_opt, f_opt, info = fmin_l_bfgs_b(func, x0=initial_values, args=(X, Y),\n",
      "        ...                                    approx_grad=True)\n",
      "        >>> x_opt, f_opt\n",
      "        array([1.99999999, 3.00000006]), 1.7746231151323805e-14  # may vary\n",
      "\n",
      "        The optimized parameters in ``x_opt`` agree with the ground truth parameters\n",
      "        ``m`` and ``b``. Next, let us perform a bound constrained optimization using\n",
      "        the `bounds` parameter.\n",
      "\n",
      "        >>> bounds = [(0, 5), (5, 10)]\n",
      "        >>> x_opt, f_op, info = fmin_l_bfgs_b(func, x0=initial_values, args=(X, Y),\n",
      "        ...                                   approx_grad=True, bounds=bounds)\n",
      "        >>> x_opt, f_opt\n",
      "        array([1.65990508, 5.31649385]), 15.721334516453945  # may vary\n",
      "\n",
      "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=np.float64(1.4901161193847656e-08), maxiter=None, full_output=0, disp=1, retall=0, callback=None, c1=0.0001, c2=0.9)\n",
      "        Unconstrained minimization of a function using the Newton-CG method.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable ``f(x, *args)``\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable ``f'(x, *args)``\n",
      "            Gradient of f.\n",
      "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
      "            Function which computes the Hessian of f times an\n",
      "            arbitrary vector, p.\n",
      "        fhess : callable ``fhess(x, *args)``, optional\n",
      "            Function to compute the Hessian matrix of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
      "            (the same set of extra arguments is supplied to all of\n",
      "            these functions).\n",
      "        epsilon : float or ndarray, optional\n",
      "            If fhess is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function which is called after\n",
      "            each iteration. Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        avextol : float, optional\n",
      "            Convergence is assumed when the average relative error in\n",
      "            the minimizer falls below this amount.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True, return the optional outputs.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence message.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of results at each iteration.\n",
      "        c1 : float, default: 1e-4\n",
      "            Parameter for Armijo condition rule.\n",
      "        c2 : float, default: 0.9\n",
      "            Parameter for curvature condition rule\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., ``f(xopt) == fopt``.\n",
      "        fopt : float\n",
      "            Value of the function at xopt, i.e., ``fopt = f(xopt)``.\n",
      "        fcalls : int\n",
      "            Number of function calls made.\n",
      "        gcalls : int\n",
      "            Number of gradient calls made.\n",
      "        hcalls : int\n",
      "            Number of Hessian calls made.\n",
      "        warnflag : int\n",
      "            Warnings generated by the algorithm.\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Line search failure (precision loss).\n",
      "            3 : NaN result encountered.\n",
      "        allvecs : list\n",
      "            The result at each iteration, if retall is True (see below).\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Newton-CG' `method` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
      "        is provided, then `fhess_p` will be ignored. If neither `fhess`\n",
      "        nor `fhess_p` is provided, then the hessian product will be\n",
      "        approximated using finite differences on `fprime`. `fhess_p`\n",
      "        must compute the hessian times an arbitrary vector. If it is not\n",
      "        given, finite-differences on `fprime` are used to compute\n",
      "        it.\n",
      "\n",
      "        Newton-CG methods are also called truncated Newton methods. This\n",
      "        function differs from scipy.optimize.fmin_tnc because\n",
      "\n",
      "        1. scipy.optimize.fmin_ncg is written purely in Python using NumPy\n",
      "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
      "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
      "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
      "            or box constrained minimization. (Box constraints give\n",
      "            lower and upper bounds for each variable separately.)\n",
      "\n",
      "        Parameters `c1` and `c2` must satisfy ``0 < c1 < c2 < 1``.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        Wright & Nocedal, 'Numerical Optimization', 1999, p. 140.\n",
      "\n",
      "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
      "        Minimize a function using modified Powell's method.\n",
      "\n",
      "        This method only uses function values, not derivatives.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func.\n",
      "        xtol : float, optional\n",
      "            Line-search error tolerance.\n",
      "        ftol : float, optional\n",
      "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            If True, ``fopt``, ``xi``, ``direc``, ``iter``, ``funcalls``, and\n",
      "            ``warnflag`` are returned.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence messages.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of the solution at each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each\n",
      "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        direc : ndarray, optional\n",
      "            Initial fitting step and parameter order set as an (N, N) array, where N\n",
      "            is the number of fitting parameters in `x0`. Defaults to step size 1.0\n",
      "            fitting all parameters simultaneously (``np.eye((N, N))``). To\n",
      "            prevent initial consideration of values in a step or to change initial\n",
      "            step size, set to 0 or desired step size in the Jth position in the Mth\n",
      "            block, where J is the position in `x0` and M is the desired evaluation\n",
      "            step, with steps being evaluated in index order. Step size and ordering\n",
      "            will change freely as minimization proceeds.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter which minimizes `func`.\n",
      "        fopt : number\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        direc : ndarray\n",
      "            Current direction set.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            Integer warning flag:\n",
      "                1 : Maximum number of function evaluations.\n",
      "                2 : Maximum number of iterations.\n",
      "                3 : NaN result encountered.\n",
      "                4 : The result is out of the provided bounds.\n",
      "        allvecs : list\n",
      "            List of solutions at each iteration.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to unconstrained minimization algorithms for\n",
      "            multivariate functions. See the 'Powell' method in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Uses a modification of Powell's method to find the minimum of\n",
      "        a function of N variables. Powell's method is a conjugate\n",
      "        direction method.\n",
      "\n",
      "        The algorithm has two loops. The outer loop merely iterates over the inner\n",
      "        loop. The inner loop minimizes over each current direction in the direction\n",
      "        set. At the end of the inner loop, if certain conditions are met, the\n",
      "        direction that gave the largest decrease is dropped and replaced with the\n",
      "        difference between the current estimated x and the estimated x from the\n",
      "        beginning of the inner-loop.\n",
      "\n",
      "        The technical conditions for replacing the direction of greatest\n",
      "        increase amount to checking that\n",
      "\n",
      "        1. No further gain can be made along the direction of greatest increase\n",
      "           from that iteration.\n",
      "        2. The direction of greatest increase accounted for a large sufficient\n",
      "           fraction of the decrease in the function value from that iteration of\n",
      "           the inner loop.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
      "        function of several variables without calculating derivatives,\n",
      "        Computer Journal, 7 (2):155-162.\n",
      "\n",
      "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
      "        Numerical Recipes (any edition), Cambridge University Press\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "\n",
      "        >>> minimum = optimize.fmin_powell(f, -1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 2\n",
      "                 Function evaluations: 16\n",
      "        >>> minimum\n",
      "        array(0.0)\n",
      "\n",
      "    fmin_slsqp(func, x0, eqcons=(), f_eqcons=None, ieqcons=(), f_ieqcons=None, bounds=(), fprime=None, fprime_eqcons=None, fprime_ieqcons=None, args=(), iter=100, acc=1e-06, iprint=1, disp=None, full_output=0, epsilon=np.float64(1.4901161193847656e-08), callback=None)\n",
      "        Minimize a function using Sequential Least Squares Programming\n",
      "\n",
      "        Python interface function for the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.  Must return a scalar.\n",
      "        x0 : 1-D ndarray of float\n",
      "            Initial guess for the independent variable(s).\n",
      "        eqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            eqcons[j](x,*args) == 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_eqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D array in which each element must equal 0.0 in a\n",
      "            successfully optimized problem. If f_eqcons is specified,\n",
      "            eqcons is ignored.\n",
      "        ieqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            ieqcons[j](x,*args) >= 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_ieqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D ndarray in which each element must be greater or\n",
      "            equal to 0.0 in a successfully optimized problem. If\n",
      "            f_ieqcons is specified, ieqcons is ignored.\n",
      "        bounds : list, optional\n",
      "            A list of tuples specifying the lower and upper bound\n",
      "            for each independent variable [(xl0, xu0),(xl1, xu1),...]\n",
      "            Infinite values will be interpreted as large floating values.\n",
      "        fprime : callable ``f(x,*args)``, optional\n",
      "            A function that evaluates the partial derivatives of func.\n",
      "        fprime_eqcons : callable ``f(x,*args)``, optional\n",
      "            A function of the form ``f(x, *args)`` that returns the m by n\n",
      "            array of equality constraint normals. If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_eqcons should be sized as ( len(eqcons), len(x0) ).\n",
      "        fprime_ieqcons : callable ``f(x,*args)``, optional\n",
      "            A function of the form ``f(x, *args)`` that returns the m by n\n",
      "            array of inequality constraint normals. If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_ieqcons should be sized as ( len(ieqcons), len(x0) ).\n",
      "        args : sequence, optional\n",
      "            Additional arguments passed to func and fprime.\n",
      "        iter : int, optional\n",
      "            The maximum number of iterations.\n",
      "        acc : float, optional\n",
      "            Requested accuracy.\n",
      "        iprint : int, optional\n",
      "            The verbosity of fmin_slsqp :\n",
      "\n",
      "            * iprint <= 0 : Silent operation\n",
      "            * iprint == 1 : Print summary upon completion (default)\n",
      "            * iprint >= 2 : Print status of each iterate and summary\n",
      "        disp : int, optional\n",
      "            Overrides the iprint interface (preferred).\n",
      "        full_output : bool, optional\n",
      "            If False, return only the minimizer of func (default).\n",
      "            Otherwise, output final objective function and summary\n",
      "            information.\n",
      "        epsilon : float, optional\n",
      "            The step size for finite-difference derivative estimates.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(x)``, where ``x`` is the\n",
      "            current parameter vector.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray of float\n",
      "            The final minimizer of func.\n",
      "        fx : ndarray of float, if full_output is true\n",
      "            The final value of the objective function.\n",
      "        its : int, if full_output is true\n",
      "            The number of iterations.\n",
      "        imode : int, if full_output is true\n",
      "            The exit mode from the optimizer (see below).\n",
      "        smode : string, if full_output is true\n",
      "            Message describing the exit mode from the optimizer.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'SLSQP' `method` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Exit modes are defined as follows:\n",
      "\n",
      "        - ``-1`` : Gradient evaluation required (g & a)\n",
      "        - ``0`` : Optimization terminated successfully\n",
      "        - ``1`` : Function evaluation required (f & c)\n",
      "        - ``2`` : More equality constraints than independent variables\n",
      "        - ``3`` : More than 3*n iterations in LSQ subproblem\n",
      "        - ``4`` : Inequality constraints incompatible\n",
      "        - ``5`` : Singular matrix E in LSQ subproblem\n",
      "        - ``6`` : Singular matrix C in LSQ subproblem\n",
      "        - ``7`` : Rank-deficient equality constraint subproblem HFTI\n",
      "        - ``8`` : Positive directional derivative for linesearch\n",
      "        - ``9`` : Iteration limit reached\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Examples are given :ref:`in the tutorial <tutorial-sqlsp>`.\n",
      "\n",
      "    fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)\n",
      "        Minimize a function with variables subject to bounds, using\n",
      "        gradient information in a truncated Newton algorithm. This\n",
      "        method wraps a C implementation of the algorithm.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x, *args)``\n",
      "            Function to minimize.  Must do one of:\n",
      "\n",
      "            1. Return f and g, where f is the value of the function and g its\n",
      "               gradient (a list of floats).\n",
      "\n",
      "            2. Return the function value but supply gradient function\n",
      "               separately as `fprime`.\n",
      "\n",
      "            3. Return the function value and set ``approx_grad=True``.\n",
      "\n",
      "            If the function returns None, the minimization\n",
      "            is aborted.\n",
      "        x0 : array_like\n",
      "            Initial estimate of minimum.\n",
      "        fprime : callable ``fprime(x, *args)``, optional\n",
      "            Gradient of `func`. If None, then either `func` must return the\n",
      "            function value and the gradient (``f,g = func(x, *args)``)\n",
      "            or `approx_grad` must be True.\n",
      "        args : tuple, optional\n",
      "            Arguments to pass to function.\n",
      "        approx_grad : bool, optional\n",
      "            If true, approximate the gradient numerically.\n",
      "        bounds : list, optional\n",
      "            (min, max) pairs for each element in x0, defining the\n",
      "            bounds on that parameter. Use None or +/-inf for one of\n",
      "            min or max when there is no bound in that direction.\n",
      "        epsilon : float, optional\n",
      "            Used if approx_grad is True. The stepsize in a finite\n",
      "            difference approximation for fprime.\n",
      "        scale : array_like, optional\n",
      "            Scaling factors to apply to each variable. If None, the\n",
      "            factors are up-low for interval bounded variables and\n",
      "            1+|x| for the others. Defaults to None.\n",
      "        offset : array_like, optional\n",
      "            Value to subtract from each variable. If None, the\n",
      "            offsets are (up+low)/2 for interval bounded variables\n",
      "            and x for the others.\n",
      "        messages : int, optional\n",
      "            Bit mask used to select messages display during\n",
      "            minimization values defined in the MSGS dict. Defaults to\n",
      "            MGS_ALL.\n",
      "        disp : int, optional\n",
      "            Integer interface to messages. 0 = no message, 5 = all messages\n",
      "        maxCGit : int, optional\n",
      "            Maximum number of hessian*vector evaluations per main\n",
      "            iteration. If maxCGit == 0, the direction chosen is\n",
      "            -gradient if maxCGit < 0, maxCGit is set to\n",
      "            max(1,min(50,n/2)). Defaults to -1.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluation. If None, maxfun is\n",
      "            set to max(100, 10*len(x0)). Defaults to None. Note that this function\n",
      "            may violate the limit because of evaluating gradients by numerical\n",
      "            differentiation.\n",
      "        eta : float, optional\n",
      "            Severity of the line search. If < 0 or > 1, set to 0.25.\n",
      "            Defaults to -1.\n",
      "        stepmx : float, optional\n",
      "            Maximum step for the line search. May be increased during\n",
      "            call. If too small, it will be set to 10.0. Defaults to 0.\n",
      "        accuracy : float, optional\n",
      "            Relative precision for finite difference calculations. If\n",
      "            <= machine_precision, set to sqrt(machine_precision).\n",
      "            Defaults to 0.\n",
      "        fmin : float, optional\n",
      "            Minimum function value estimate. Defaults to 0.\n",
      "        ftol : float, optional\n",
      "            Precision goal for the value of f in the stopping criterion.\n",
      "            If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "        xtol : float, optional\n",
      "            Precision goal for the value of x in the stopping\n",
      "            criterion (after applying x scaling factors). If xtol <\n",
      "            0.0, xtol is set to sqrt(machine_precision). Defaults to\n",
      "            -1.\n",
      "        pgtol : float, optional\n",
      "            Precision goal for the value of the projected gradient in\n",
      "            the stopping criterion (after applying x scaling factors).\n",
      "            If pgtol < 0.0, pgtol is set to 1e-2 * sqrt(accuracy).\n",
      "            Setting it to 0.0 is not recommended. Defaults to -1.\n",
      "        rescale : float, optional\n",
      "            Scaling factor (in log10) used to trigger f value\n",
      "            rescaling. If 0, rescale at each iteration. If a large\n",
      "            value, never rescale. If < 0, rescale is set to 1.3.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution.\n",
      "        nfeval : int\n",
      "            The number of function evaluations.\n",
      "        rc : int\n",
      "            Return code, see below\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'TNC' `method` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The underlying algorithm is truncated Newton, also called\n",
      "        Newton Conjugate-Gradient. This method differs from\n",
      "        scipy.optimize.fmin_ncg in that\n",
      "\n",
      "        1. it wraps a C implementation of the algorithm\n",
      "        2. it allows each variable to be given an upper and lower bound.\n",
      "\n",
      "        The algorithm incorporates the bound constraints by determining\n",
      "        the descent direction as in an unconstrained truncated Newton,\n",
      "        but never taking a step-size large enough to leave the space\n",
      "        of feasible x's. The algorithm keeps track of a set of\n",
      "        currently active constraints, and ignores them when computing\n",
      "        the minimum allowable step size. (The x's associated with the\n",
      "        active constraint are kept fixed.) If the maximum allowable\n",
      "        step size is zero then a new constraint is added. At the end\n",
      "        of each iteration one of the constraints may be deemed no\n",
      "        longer active and removed. A constraint is considered\n",
      "        no longer active is if it is currently active\n",
      "        but the gradient for that variable points inward from the\n",
      "        constraint. The specific constraint removed is the one\n",
      "        associated with the variable of largest index whose\n",
      "        constraint is no longer active.\n",
      "\n",
      "        Return codes are defined as follows:\n",
      "\n",
      "        - ``-1`` : Infeasible (lower bound > upper bound)\n",
      "        - ``0`` : Local minimum reached (:math:`|pg| \\approx 0`)\n",
      "        - ``1`` : Converged (:math:`|f_n-f_(n-1)| \\approx 0`)\n",
      "        - ``2`` : Converged (:math:`|x_n-x_(n-1)| \\approx 0`)\n",
      "        - ``3`` : Max. number of function evaluations reached\n",
      "        - ``4`` : Linear search failed\n",
      "        - ``5`` : All lower bounds are equal to the upper bounds\n",
      "        - ``6`` : Unable to progress\n",
      "        - ``7`` : User requested end of minimization\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        Wright S., Nocedal J. (2006), 'Numerical Optimization'\n",
      "\n",
      "        Nash S.G. (1984), \"Newton-Type Minimization Via the Lanczos Method\",\n",
      "        SIAM Journal of Numerical Analysis 21, pp. 770-778\n",
      "\n",
      "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
      "        Bounded minimization for scalar functions.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized (must accept and return scalars).\n",
      "        x1, x2 : float or array scalar\n",
      "            Finite optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to function.\n",
      "        xtol : float, optional\n",
      "            The convergence tolerance.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations allowed.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        disp: int, optional\n",
      "            If non-zero, print messages.\n",
      "\n",
      "            ``0`` : no message printing.\n",
      "\n",
      "            ``1`` : non-convergence notification messages only.\n",
      "\n",
      "            ``2`` : print a message on convergence too.\n",
      "\n",
      "            ``3`` : print iteration results.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters (over given interval) which minimize the\n",
      "            objective function.\n",
      "        fval : number\n",
      "            (Optional output) The function value evaluated at the minimizer.\n",
      "        ierr : int\n",
      "            (Optional output) An error flag (0 if converged, 1 if maximum number of\n",
      "            function calls reached).\n",
      "        numfunc : int\n",
      "            (Optional output) The number of function calls made.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Bounded' `method` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Finds a local minimizer of the scalar function `func` in the\n",
      "        interval x1 < xopt < x2 using Brent's method. (See `brent`\n",
      "        for auto-bracketing.)\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Forsythe, G.E., M. A. Malcolm, and C. B. Moler. \"Computer Methods\n",
      "               for Mathematical Computations.\" Prentice-Hall Series in Automatic\n",
      "               Computation 259 (1977).\n",
      "        .. [2] Brent, Richard P. Algorithms for Minimization Without Derivatives.\n",
      "               Courier Corporation, 2013.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        `fminbound` finds the minimizer of the function in the given range.\n",
      "        The following examples illustrate this.\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "        >>> def f(x):\n",
      "        ...     return (x-1)**2\n",
      "        >>> minimizer = optimize.fminbound(f, -4, 4)\n",
      "        >>> minimizer\n",
      "        1.0\n",
      "        >>> minimum = f(minimizer)\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> res = optimize.fminbound(f, 3, 4, full_output=True)\n",
      "        >>> minimizer, fval, ierr, numfunc = res\n",
      "        >>> minimizer\n",
      "        3.000005960860986\n",
      "        >>> minimum = f(minimizer)\n",
      "        >>> minimum, fval\n",
      "        (4.000023843479476, 4.000023843479476)\n",
      "\n",
      "    fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None)\n",
      "        Find the roots of a function.\n",
      "\n",
      "        Return the roots of the (non-linear) equations defined by\n",
      "        ``func(x) = 0`` given a starting estimate.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            A function that takes at least one (possibly vector) argument,\n",
      "            and returns a value of the same length.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the roots of ``func(x) = 0``.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to `func`.\n",
      "        fprime : callable ``f(x, *args)``, optional\n",
      "            A function to compute the Jacobian of `func` with derivatives\n",
      "            across the rows. By default, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            Specify whether the Jacobian function computes derivatives down\n",
      "            the columns (faster, because there is no transpose operation).\n",
      "        xtol : float, optional\n",
      "            The calculation will terminate if the relative error between two\n",
      "            consecutive iterates is at most `xtol`.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If zero, then\n",
      "            ``100*(N+1)`` is the maximum where N is the number of elements\n",
      "            in `x0`.\n",
      "        band : tuple, optional\n",
      "            If set to a two-sequence containing the number of sub- and\n",
      "            super-diagonals within the band of the Jacobi matrix, the\n",
      "            Jacobi matrix is considered banded (only for ``fprime=None``).\n",
      "        epsfcn : float, optional\n",
      "            A suitable step length for the forward-difference\n",
      "            approximation of the Jacobian (for ``fprime=None``). If\n",
      "            `epsfcn` is less than the machine precision, it is assumed\n",
      "            that the relative errors in the functions are of the order of\n",
      "            the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``). Should be in the interval\n",
      "            ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the\n",
      "            variables.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for\n",
      "            an unsuccessful call).\n",
      "        infodict : dict\n",
      "            A dictionary of optional outputs with the keys:\n",
      "\n",
      "            ``nfev``\n",
      "                number of function calls\n",
      "            ``njev``\n",
      "                number of Jacobian calls\n",
      "            ``fvec``\n",
      "                function evaluated at the output\n",
      "            ``fjac``\n",
      "                the orthogonal matrix, q, produced by the QR\n",
      "                factorization of the final approximate Jacobian\n",
      "                matrix, stored column wise\n",
      "            ``r``\n",
      "                upper triangular matrix produced by QR factorization\n",
      "                of the same matrix\n",
      "            ``qtf``\n",
      "                the vector ``(transpose(q) * fvec)``\n",
      "\n",
      "        ier : int\n",
      "            An integer flag.  Set to 1 if a solution was found, otherwise refer\n",
      "            to `mesg` for more information.\n",
      "        mesg : str\n",
      "            If no solution is found, `mesg` details the cause of failure.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See the ``method='hybr'`` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        ``fsolve`` is a wrapper around MINPACK's hybrd and hybrj algorithms.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Find a solution to the system of equations:\n",
      "        ``x0*cos(x1) = 4,  x1*x0 - x1 = 5``.\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import fsolve\n",
      "        >>> def func(x):\n",
      "        ...     return [x[0] * np.cos(x[1]) - 4,\n",
      "        ...             x[1] * x[0] - x[1] - 5]\n",
      "        >>> root = fsolve(func, [1, 1])\n",
      "        >>> root\n",
      "        array([6.50409711, 0.90841421])\n",
      "        >>> np.isclose(func(root), [0.0, 0.0])  # func(root) should be almost 0.0.\n",
      "        array([ True,  True])\n",
      "\n",
      "    golden(func, args=(), brack=None, tol=np.float64(1.4901161193847656e-08), full_output=0, maxiter=5000)\n",
      "        Return the minimizer of a function of one variable using the golden section\n",
      "        method.\n",
      "\n",
      "        Given a function of one variable and a possible bracketing interval,\n",
      "        return a minimizer of the function isolated to a fractional precision of\n",
      "        tol.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            Objective function to minimize.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to func.\n",
      "        brack : tuple, optional\n",
      "            Either a triple ``(xa, xb, xc)`` where ``xa < xb < xc`` and\n",
      "            ``func(xb) < func(xa) and  func(xb) < func(xc)``, or a pair (xa, xb)\n",
      "            to be used as initial points for a downhill bracket search (see\n",
      "            `scipy.optimize.bracket`).\n",
      "            The minimizer ``x`` will not necessarily satisfy ``xa <= x <= xb``.\n",
      "        tol : float, optional\n",
      "            x tolerance stop criterion\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        xmin : ndarray\n",
      "            Optimum point.\n",
      "        fval : float\n",
      "            (Optional output) Optimum function value.\n",
      "        funcalls : int\n",
      "            (Optional output) Number of objective function evaluations made.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Golden' `method` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Uses analog of bisection method to decrease the bracketed\n",
      "        interval.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3, respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range ``(xa, xb)``.\n",
      "\n",
      "        >>> def f(x):\n",
      "        ...     return (x-1)**2\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "\n",
      "        >>> minimizer = optimize.golden(f, brack=(1, 2))\n",
      "        >>> minimizer\n",
      "        1\n",
      "        >>> res = optimize.golden(f, brack=(-1, 0.5, 2), full_output=True)\n",
      "        >>> xmin, fval, funcalls = res\n",
      "        >>> f(xmin), fval\n",
      "        (9.925165290385052e-18, 9.925165290385052e-18)\n",
      "\n",
      "    isotonic_regression(y: 'npt.ArrayLike', *, weights: 'npt.ArrayLike | None' = None, increasing: bool = True) -> scipy.optimize._optimize.OptimizeResult\n",
      "        Nonparametric isotonic regression.\n",
      "\n",
      "        A (not strictly) monotonically increasing array `x` with the same length\n",
      "        as `y` is calculated by the pool adjacent violators algorithm (PAVA), see\n",
      "        [1]_. See the Notes section for more details.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        y : (N,) array_like\n",
      "            Response variable.\n",
      "        weights : (N,) array_like or None\n",
      "            Case weights.\n",
      "        increasing : bool\n",
      "            If True, fit monotonic increasing, i.e. isotonic, regression.\n",
      "            If False, fit a monotonic decreasing, i.e. antitonic, regression.\n",
      "            Default is True.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are:\n",
      "\n",
      "            - ``x``: The isotonic regression solution, i.e. an increasing (or\n",
      "              decreasing) array of the same length than y, with elements in the\n",
      "              range from min(y) to max(y).\n",
      "            - ``weights`` : Array with the sum of case weights for each block\n",
      "              (or pool) B.\n",
      "            - ``blocks``: Array of length B+1 with the indices of the start\n",
      "              positions of each block (or pool) B. The j-th block is given by\n",
      "              ``x[blocks[j]:blocks[j+1]]`` for which all values are the same.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Given data :math:`y` and case weights :math:`w`, the isotonic regression\n",
      "        solves the following optimization problem:\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            \\operatorname{argmin}_{x_i} \\sum_i w_i (y_i - x_i)^2 \\quad\n",
      "            \\text{subject to } x_i \\leq x_j \\text{ whenever } i \\leq j \\,.\n",
      "\n",
      "        For every input value :math:`y_i`, it generates a value :math:`x_i` such\n",
      "        that :math:`x` is increasing (but not strictly), i.e.\n",
      "        :math:`x_i \\leq x_{i+1}`. This is accomplished by the PAVA.\n",
      "        The solution consists of pools or blocks, i.e. neighboring elements of\n",
      "        :math:`x`, e.g. :math:`x_i` and :math:`x_{i+1}`, that all have the same\n",
      "        value.\n",
      "\n",
      "        Most interestingly, the solution stays the same if the squared loss is\n",
      "        replaced by the wide class of Bregman functions which are the unique\n",
      "        class of strictly consistent scoring functions for the mean, see [2]_\n",
      "        and references therein.\n",
      "\n",
      "        The implemented version of PAVA according to [1]_ has a computational\n",
      "        complexity of O(N) with input size N.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Busing, F. M. T. A. (2022).\n",
      "               Monotone Regression: A Simple and Fast O(n) PAVA Implementation.\n",
      "               Journal of Statistical Software, Code Snippets, 102(1), 1-25.\n",
      "               :doi:`10.18637/jss.v102.c01`\n",
      "        .. [2] Jordan, A.I., Mühlemann, A. & Ziegel, J.F.\n",
      "               Characterizing the optimal solutions to the isotonic regression\n",
      "               problem for identifiable functionals.\n",
      "               Ann Inst Stat Math 74, 489-514 (2022).\n",
      "               :doi:`10.1007/s10463-021-00808-0`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        This example demonstrates that ``isotonic_regression`` really solves a\n",
      "        constrained optimization problem.\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import isotonic_regression, minimize\n",
      "        >>> y = [1.5, 1.0, 4.0, 6.0, 5.7, 5.0, 7.8, 9.0, 7.5, 9.5, 9.0]\n",
      "        >>> def objective(yhat, y):\n",
      "        ...     return np.sum((yhat - y)**2)\n",
      "        >>> def constraint(yhat, y):\n",
      "        ...     # This is for a monotonically increasing regression.\n",
      "        ...     return np.diff(yhat)\n",
      "        >>> result = minimize(objective, x0=y, args=(y,),\n",
      "        ...                   constraints=[{'type': 'ineq',\n",
      "        ...                                 'fun': lambda x: constraint(x, y)}])\n",
      "        >>> result.x\n",
      "        array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\n",
      "               5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\n",
      "               9.25      ])\n",
      "        >>> result = isotonic_regression(y)\n",
      "        >>> result.x\n",
      "        array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\n",
      "               5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\n",
      "               9.25      ])\n",
      "\n",
      "        The big advantage of ``isotonic_regression`` compared to calling\n",
      "        ``minimize`` is that it is more user friendly, i.e. one does not need to\n",
      "        define objective and constraint functions, and that it is orders of\n",
      "        magnitudes faster. On commodity hardware (in 2023), for normal distributed\n",
      "        input y of length 1000, the minimizer takes about 4 seconds, while\n",
      "        ``isotonic_regression`` takes about 200 microseconds.\n",
      "\n",
      "    least_squares(fun, x0, jac='2-point', bounds=(-inf, inf), method='trf', ftol=1e-08, xtol=1e-08, gtol=1e-08, x_scale=None, loss='linear', f_scale=1.0, diff_step=None, tr_solver=None, tr_options=None, jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs=None, callback=None, workers=None)\n",
      "        Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "\n",
      "        Given the residuals f(x) (an m-D real function of n real\n",
      "        variables) and the loss function rho(s) (a scalar function), `least_squares`\n",
      "        finds a local minimum of the cost function F(x)::\n",
      "\n",
      "            minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\n",
      "            subject to lb <= x <= ub\n",
      "\n",
      "        The purpose of the loss function rho(s) is to reduce the influence of\n",
      "        outliers on the solution.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Function which computes the vector of residuals, with the signature\n",
      "            ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\n",
      "            respect to its first argument. The argument ``x`` passed to this\n",
      "            function is an ndarray of shape (n,) (never a scalar, even for n=1).\n",
      "            It must allocate and return a 1-D array_like of shape (m,) or a scalar.\n",
      "            If the argument ``x`` is complex or the function ``fun`` returns\n",
      "            complex residuals, it must be wrapped in a real function of real\n",
      "            arguments, as shown at the end of the Examples section.\n",
      "        x0 : array_like with shape (n,) or float\n",
      "            Initial guess on independent variables. If float, it will be treated\n",
      "            as a 1-D array with one element. When `method` is 'trf', the initial\n",
      "            guess might be slightly adjusted to lie sufficiently within the given\n",
      "            `bounds`.\n",
      "        jac : {'2-point', '3-point', 'cs', callable}, optional\n",
      "            Method of computing the Jacobian matrix (an m-by-n matrix, where\n",
      "            element (i, j) is the partial derivative of f[i] with respect to\n",
      "            x[j]). The keywords select a finite difference scheme for numerical\n",
      "            estimation. The scheme '3-point' is more accurate, but requires\n",
      "            twice as many operations as '2-point' (default). The scheme 'cs'\n",
      "            uses complex steps, and while potentially the most accurate, it is\n",
      "            applicable only when `fun` correctly handles complex inputs and\n",
      "            can be analytically continued to the complex plane. If callable, it is used as\n",
      "            ``jac(x, *args, **kwargs)`` and should return a good approximation\n",
      "            (or the exact value) for the Jacobian as an array_like (np.atleast_2d\n",
      "            is applied), a sparse array (csr_array preferred for performance) or\n",
      "            a `scipy.sparse.linalg.LinearOperator`.\n",
      "\n",
      "            .. versionchanged:: 1.16.0\n",
      "                An ability to use the '3-point', 'cs' keywords with the 'lm' method.\n",
      "                Previously 'lm' was limited to '2-point' and callable.\n",
      "\n",
      "        bounds : 2-tuple of array_like or `Bounds`, optional\n",
      "            There are two ways to specify bounds:\n",
      "\n",
      "            1. Instance of `Bounds` class\n",
      "            2. Lower and upper bounds on independent variables. Defaults to no\n",
      "               bounds. Each array must match the size of `x0` or be a scalar,\n",
      "               in the latter case a bound will be the same for all variables.\n",
      "               Use ``np.inf`` with an appropriate sign to disable bounds on all\n",
      "               or some variables.\n",
      "\n",
      "        method : {'trf', 'dogbox', 'lm'}, optional\n",
      "            Algorithm to perform minimization.\n",
      "\n",
      "            * 'trf' : Trust Region Reflective algorithm, particularly suitable\n",
      "              for large sparse problems with bounds. Generally robust method.\n",
      "            * 'dogbox' : dogleg algorithm with rectangular trust regions,\n",
      "              typical use case is small problems with bounds. Not recommended\n",
      "              for problems with rank-deficient Jacobian.\n",
      "            * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\n",
      "              Doesn't handle bounds and sparse Jacobians. Usually the most\n",
      "              efficient method for small unconstrained problems.\n",
      "\n",
      "            Default is 'trf'. See Notes for more information.\n",
      "        ftol : float or None, optional\n",
      "            Tolerance for termination by the change of the cost function. Default\n",
      "            is 1e-8. The optimization process is stopped when ``dF < ftol * F``,\n",
      "            and there was an adequate agreement between a local quadratic model and\n",
      "            the true model in the last step.\n",
      "\n",
      "            If None and 'method' is not 'lm', the termination by this condition is\n",
      "            disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "            machine epsilon.\n",
      "        xtol : float or None, optional\n",
      "            Tolerance for termination by the change of the independent variables.\n",
      "            Default is 1e-8. The exact condition depends on the `method` used:\n",
      "\n",
      "            * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``.\n",
      "            * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\n",
      "              a trust-region radius and ``xs`` is the value of ``x``\n",
      "              scaled according to `x_scale` parameter (see below).\n",
      "\n",
      "            If None and 'method' is not 'lm', the termination by this condition is\n",
      "            disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "            machine epsilon.\n",
      "        gtol : float or None, optional\n",
      "            Tolerance for termination by the norm of the gradient. Default is 1e-8.\n",
      "            The exact condition depends on a `method` used:\n",
      "\n",
      "            * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\n",
      "              ``g_scaled`` is the value of the gradient scaled to account for\n",
      "              the presence of the bounds [STIR]_.\n",
      "            * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\n",
      "              ``g_free`` is the gradient with respect to the variables which\n",
      "              are not in the optimal state on the boundary.\n",
      "            * For 'lm' : the maximum absolute value of the cosine of angles\n",
      "              between columns of the Jacobian and the residual vector is less\n",
      "              than `gtol`, or the residual vector is zero.\n",
      "\n",
      "            If None and 'method' is not 'lm', the termination by this condition is\n",
      "            disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "            machine epsilon.\n",
      "        x_scale : {None, array_like, 'jac'}, optional\n",
      "            Characteristic scale of each variable. Setting `x_scale` is equivalent\n",
      "            to reformulating the problem in scaled variables ``xs = x / x_scale``.\n",
      "            An alternative view is that the size of a trust region along jth\n",
      "            dimension is proportional to ``x_scale[j]``. Improved convergence may\n",
      "            be achieved by setting `x_scale` such that a step of a given size\n",
      "            along any of the scaled variables has a similar effect on the cost\n",
      "            function. If set to 'jac', the scale is iteratively updated using the\n",
      "            inverse norms of the columns of the Jacobian matrix (as described in\n",
      "            [JJMore]_). The default scaling for each method (i.e.\n",
      "            if ``x_scale is None``) is as follows:\n",
      "\n",
      "            * For 'trf'    : ``x_scale == 1``\n",
      "            * For 'dogbox' : ``x_scale == 1``\n",
      "            * For 'lm'     : ``x_scale == 'jac'``\n",
      "\n",
      "            .. versionchanged:: 1.16.0\n",
      "                The default keyword value is changed from 1 to None to indicate that\n",
      "                a default approach to scaling is used.\n",
      "                For the 'lm' method the default scaling is changed from 1 to 'jac'.\n",
      "                This has been found to give better performance, and is the same\n",
      "                scaling as performed by ``leastsq``.\n",
      "\n",
      "        loss : str or callable, optional\n",
      "            Determines the loss function. The following keyword values are allowed:\n",
      "\n",
      "            * 'linear' (default) : ``rho(z) = z``. Gives a standard\n",
      "              least-squares problem.\n",
      "            * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\n",
      "              approximation of l1 (absolute value) loss. Usually a good\n",
      "              choice for robust least squares.\n",
      "            * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\n",
      "              similarly to 'soft_l1'.\n",
      "            * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\n",
      "              influence, but may cause difficulties in optimization process.\n",
      "            * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\n",
      "              a single residual, has properties similar to 'cauchy'.\n",
      "\n",
      "            If callable, it must take a 1-D ndarray ``z=f**2`` and return an\n",
      "            array_like with shape (3, m) where row 0 contains function values,\n",
      "            row 1 contains first derivatives and row 2 contains second\n",
      "            derivatives. Method 'lm' supports only 'linear' loss.\n",
      "        f_scale : float, optional\n",
      "            Value of soft margin between inlier and outlier residuals, default\n",
      "            is 1.0. The loss function is evaluated as follows\n",
      "            ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\n",
      "            and ``rho`` is determined by `loss` parameter. This parameter has\n",
      "            no effect with ``loss='linear'``, but for other `loss` values it is\n",
      "            of crucial importance.\n",
      "        max_nfev : None or int, optional\n",
      "            For all methods this parameter controls the maximum number of function\n",
      "            evaluations used by each method, separate to those used in numerical\n",
      "            approximation of the jacobian.\n",
      "            If None (default), the value is chosen automatically as 100 * n.\n",
      "\n",
      "            .. versionchanged:: 1.16.0\n",
      "                The default for the 'lm' method is changed to 100 * n, for both a callable\n",
      "                and a numerically estimated jacobian. Previously the default when using an\n",
      "                estimated jacobian was 100 * n * (n + 1), because the method included\n",
      "                evaluations used in the estimation.\n",
      "\n",
      "        diff_step : None or array_like, optional\n",
      "            Determines the relative step size for the finite difference\n",
      "            approximation of the Jacobian. The actual step is computed as\n",
      "            ``x * diff_step``. If None (default), then `diff_step` is taken to be\n",
      "            a conventional \"optimal\" power of machine epsilon for the finite\n",
      "            difference scheme used [NR]_.\n",
      "        tr_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method for solving trust-region subproblems, relevant only for 'trf'\n",
      "            and 'dogbox' methods.\n",
      "\n",
      "            * 'exact' is suitable for not very large problems with dense\n",
      "              Jacobian matrices. The computational complexity per iteration is\n",
      "              comparable to a singular value decomposition of the Jacobian\n",
      "              matrix.\n",
      "            * 'lsmr' is suitable for problems with sparse and large Jacobian\n",
      "              matrices. It uses the iterative procedure\n",
      "              `scipy.sparse.linalg.lsmr` for finding a solution of a linear\n",
      "              least-squares problem and only requires matrix-vector product\n",
      "              evaluations.\n",
      "\n",
      "            If None (default), the solver is chosen based on the type of Jacobian\n",
      "            returned on the first iteration.\n",
      "        tr_options : dict, optional\n",
      "            Keyword options passed to trust-region solver.\n",
      "\n",
      "            * ``tr_solver='exact'``: `tr_options` are ignored.\n",
      "            * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\n",
      "              Additionally,  ``method='trf'`` supports  'regularize' option\n",
      "              (bool, default is True), which adds a regularization term to the\n",
      "              normal equation, which improves convergence if the Jacobian is\n",
      "              rank-deficient [Byrd]_ (eq. 3.4).\n",
      "\n",
      "        jac_sparsity : {None, array_like, sparse array}, optional\n",
      "            Defines the sparsity structure of the Jacobian matrix for finite\n",
      "            difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "            only few non-zero elements in *each* row, providing the sparsity\n",
      "            structure will greatly speed up the computations [Curtis]_. A zero\n",
      "            entry means that a corresponding element in the Jacobian is identically\n",
      "            zero. If provided, forces the use of 'lsmr' trust-region solver.\n",
      "            If None (default), then dense differencing will be used. Has no effect\n",
      "            for 'lm' method.\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "\n",
      "            * 0 (default) : work silently.\n",
      "            * 1 : display a termination report.\n",
      "            * 2 : display progress during iterations (not supported by 'lm'\n",
      "              method).\n",
      "\n",
      "        args, kwargs : tuple and dict, optional\n",
      "            Additional arguments passed to `fun` and `jac`. Both empty by default.\n",
      "            The calling signature is ``fun(x, *args, **kwargs)`` and the same for\n",
      "            `jac`.\n",
      "        callback : None or callable, optional\n",
      "            Callback function that is called by the algorithm on each iteration.\n",
      "            This can be used to print or plot the optimization results at each\n",
      "            step, and to stop the optimization algorithm based on some user-defined\n",
      "            condition.  Only implemented for the `trf` and `dogbox` methods.\n",
      "\n",
      "            The signature is ``callback(intermediate_result: OptimizeResult)``\n",
      "\n",
      "            `intermediate_result is a `scipy.optimize.OptimizeResult`\n",
      "            which contains the intermediate results of the optimization at the\n",
      "            current iteration.\n",
      "\n",
      "            The callback also supports a signature like: ``callback(x)``\n",
      "\n",
      "            Introspection is used to determine which of the signatures is invoked.\n",
      "\n",
      "            If the `callback` function raises `StopIteration` the optimization algorithm\n",
      "            will stop and return with status code -2.\n",
      "\n",
      "            .. versionadded:: 1.16.0\n",
      "        workers : map-like callable, optional\n",
      "            A map-like callable, such as `multiprocessing.Pool.map` for evaluating\n",
      "            any numerical differentiation in parallel.\n",
      "            This evaluation is carried out as ``workers(fun, iterable)``.\n",
      "\n",
      "            .. versionadded:: 1.16.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        result : OptimizeResult\n",
      "            `OptimizeResult` with the following fields defined:\n",
      "\n",
      "            x : ndarray, shape (n,)\n",
      "                Solution found.\n",
      "            cost : float\n",
      "                Value of the cost function at the solution.\n",
      "            fun : ndarray, shape (m,)\n",
      "                Vector of residuals at the solution.\n",
      "            jac : ndarray, sparse array or LinearOperator, shape (m, n)\n",
      "                Modified Jacobian matrix at the solution, in the sense that J^T J\n",
      "                is a Gauss-Newton approximation of the Hessian of the cost function.\n",
      "                The type is the same as the one used by the algorithm.\n",
      "            grad : ndarray, shape (m,)\n",
      "                Gradient of the cost function at the solution.\n",
      "            optimality : float\n",
      "                First-order optimality measure. In unconstrained problems, it is\n",
      "                always the uniform norm of the gradient. In constrained problems,\n",
      "                it is the quantity which was compared with `gtol` during iterations.\n",
      "            active_mask : ndarray of int, shape (n,)\n",
      "                Each component shows whether a corresponding constraint is active\n",
      "                (that is, whether a variable is at the bound):\n",
      "\n",
      "                *  0 : a constraint is not active.\n",
      "                * -1 : a lower bound is active.\n",
      "                *  1 : an upper bound is active.\n",
      "\n",
      "                Might be somewhat arbitrary for 'trf' method as it generates a\n",
      "                sequence of strictly feasible iterates and `active_mask` is\n",
      "                determined within a tolerance threshold.\n",
      "            nfev : int\n",
      "                Number of function evaluations done. This number does not include\n",
      "                the function calls used for numerical Jacobian approximation.\n",
      "\n",
      "                .. versionchanged:: 1.16.0\n",
      "                    For the 'lm' method the number of function calls used in numerical\n",
      "                    Jacobian approximation is no longer included. This is to bring all\n",
      "                    methods into line.\n",
      "\n",
      "            njev : int or None\n",
      "                Number of Jacobian evaluations done. If numerical Jacobian\n",
      "                approximation is used in 'lm' method, it is set to None.\n",
      "            status : int\n",
      "                The reason for algorithm termination:\n",
      "\n",
      "                * -2 : terminated because callback raised StopIteration.\n",
      "                * -1 : improper input parameters status returned from MINPACK.\n",
      "                *  0 : the maximum number of function evaluations is exceeded.\n",
      "                *  1 : `gtol` termination condition is satisfied.\n",
      "                *  2 : `ftol` termination condition is satisfied.\n",
      "                *  3 : `xtol` termination condition is satisfied.\n",
      "                *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\n",
      "\n",
      "            message : str\n",
      "                Verbal description of the termination reason.\n",
      "            success : bool\n",
      "                True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        leastsq : A legacy wrapper for the MINPACK implementation of the\n",
      "                  Levenberg-Marquadt algorithm.\n",
      "        curve_fit : Least-squares minimization applied to a curve-fitting problem.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Method 'lm' (Levenberg-Marquardt) calls a wrapper over a least-squares\n",
      "        algorithm implemented in MINPACK (lmder). It runs the\n",
      "        Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\n",
      "        The implementation is based on paper [JJMore]_, it is very robust and\n",
      "        efficient with a lot of smart tricks. It should be your first choice\n",
      "        for unconstrained problems. Note that it doesn't support bounds. Also,\n",
      "        it doesn't work when m < n.\n",
      "\n",
      "        Method 'trf' (Trust Region Reflective) is motivated by the process of\n",
      "        solving a system of equations, which constitute the first-order optimality\n",
      "        condition for a bound-constrained minimization problem as formulated in\n",
      "        [STIR]_. The algorithm iteratively solves trust-region subproblems\n",
      "        augmented by a special diagonal quadratic term and with trust-region shape\n",
      "        determined by the distance from the bounds and the direction of the\n",
      "        gradient. This enhancements help to avoid making steps directly into bounds\n",
      "        and efficiently explore the whole space of variables. To further improve\n",
      "        convergence, the algorithm considers search directions reflected from the\n",
      "        bounds. To obey theoretical requirements, the algorithm keeps iterates\n",
      "        strictly feasible. With dense Jacobians trust-region subproblems are\n",
      "        solved by an exact method very similar to the one described in [JJMore]_\n",
      "        (and implemented in MINPACK). The difference from the MINPACK\n",
      "        implementation is that a singular value decomposition of a Jacobian\n",
      "        matrix is done once per iteration, instead of a QR decomposition and series\n",
      "        of Givens rotation eliminations. For large sparse Jacobians a 2-D subspace\n",
      "        approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\n",
      "        The subspace is spanned by a scaled gradient and an approximate\n",
      "        Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\n",
      "        constraints are imposed the algorithm is very similar to MINPACK and has\n",
      "        generally comparable performance. The algorithm works quite robust in\n",
      "        unbounded and bounded problems, thus it is chosen as a default algorithm.\n",
      "\n",
      "        Method 'dogbox' operates in a trust-region framework, but considers\n",
      "        rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\n",
      "        The intersection of a current trust region and initial bounds is again\n",
      "        rectangular, so on each iteration a quadratic minimization problem subject\n",
      "        to bound constraints is solved approximately by Powell's dogleg method\n",
      "        [NumOpt]_. The required Gauss-Newton step can be computed exactly for\n",
      "        dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\n",
      "        sparse Jacobians. The algorithm is likely to exhibit slow convergence when\n",
      "        the rank of Jacobian is less than the number of variables. The algorithm\n",
      "        often outperforms 'trf' in bounded problems with a small number of\n",
      "        variables.\n",
      "\n",
      "        Robust loss functions are implemented as described in [BA]_. The idea\n",
      "        is to modify a residual vector and a Jacobian matrix on each iteration\n",
      "        such that computed gradient and Gauss-Newton Hessian approximation match\n",
      "        the true gradient and Hessian approximation of the cost function. Then\n",
      "        the algorithm proceeds in a normal way, i.e., robust loss functions are\n",
      "        implemented as a simple wrapper over standard least-squares algorithms.\n",
      "\n",
      "        .. versionadded:: 0.17.0\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\n",
      "                Computing. 3rd edition\", Sec. 5.7.\n",
      "        .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\n",
      "                  solution of the trust region problem by minimization over\n",
      "                  two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\n",
      "                  1988.\n",
      "        .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\n",
      "                    sparse Jacobian matrices\", Journal of the Institute of\n",
      "                    Mathematics and its Applications, 13, pp. 117-120, 1974.\n",
      "        .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\n",
      "                    and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\n",
      "                    Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\n",
      "        .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\n",
      "                    Dogleg Approach for Unconstrained and Bound Constrained\n",
      "                    Nonlinear Optimization\", WSEAS International Conference on\n",
      "                    Applied Mathematics, Corfu, Greece, 2004.\n",
      "        .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\n",
      "                    2nd edition\", Chapter 4.\n",
      "        .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\n",
      "                Proceedings of the International Workshop on Vision Algorithms:\n",
      "                Theory and Practice, pp. 298-372, 1999.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        In this example we find a minimum of the Rosenbrock function without bounds\n",
      "        on independent variables.\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> def fun_rosenbrock(x):\n",
      "        ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\n",
      "\n",
      "        Notice that we only provide the vector of the residuals. The algorithm\n",
      "        constructs the cost function as a sum of squares of the residuals, which\n",
      "        gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\n",
      "\n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> x0_rosenbrock = np.array([2, 2])\n",
      "        >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\n",
      "        >>> res_1.x\n",
      "        array([ 1.,  1.])\n",
      "        >>> res_1.cost\n",
      "        9.8669242910846867e-30\n",
      "        >>> res_1.optimality\n",
      "        8.8928864934219529e-14\n",
      "\n",
      "        We now constrain the variables, in such a way that the previous solution\n",
      "        becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\n",
      "        ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\n",
      "        to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\n",
      "\n",
      "        We also provide the analytic Jacobian:\n",
      "\n",
      "        >>> def jac_rosenbrock(x):\n",
      "        ...     return np.array([\n",
      "        ...         [-20 * x[0], 10],\n",
      "        ...         [-1, 0]])\n",
      "\n",
      "        Putting this all together, we see that the new solution lies on the bound:\n",
      "\n",
      "        >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\n",
      "        ...                       bounds=([-np.inf, 1.5], np.inf))\n",
      "        >>> res_2.x\n",
      "        array([ 1.22437075,  1.5       ])\n",
      "        >>> res_2.cost\n",
      "        0.025213093946805685\n",
      "        >>> res_2.optimality\n",
      "        1.5885401433157753e-07\n",
      "\n",
      "        Now we solve a system of equations (i.e., the cost function should be zero\n",
      "        at a minimum) for a Broyden tridiagonal vector-valued function of 100000\n",
      "        variables:\n",
      "\n",
      "        >>> def fun_broyden(x):\n",
      "        ...     f = (3 - x) * x + 1\n",
      "        ...     f[1:] -= x[:-1]\n",
      "        ...     f[:-1] -= 2 * x[1:]\n",
      "        ...     return f\n",
      "\n",
      "        The corresponding Jacobian matrix is sparse. We tell the algorithm to\n",
      "        estimate it by finite differences and provide the sparsity structure of\n",
      "        Jacobian to significantly speed up this process.\n",
      "\n",
      "        >>> from scipy.sparse import lil_array\n",
      "        >>> def sparsity_broyden(n):\n",
      "        ...     sparsity = lil_array((n, n), dtype=int)\n",
      "        ...     i = np.arange(n)\n",
      "        ...     sparsity[i, i] = 1\n",
      "        ...     i = np.arange(1, n)\n",
      "        ...     sparsity[i, i - 1] = 1\n",
      "        ...     i = np.arange(n - 1)\n",
      "        ...     sparsity[i, i + 1] = 1\n",
      "        ...     return sparsity\n",
      "        ...\n",
      "        >>> n = 100000\n",
      "        >>> x0_broyden = -np.ones(n)\n",
      "        ...\n",
      "        >>> res_3 = least_squares(fun_broyden, x0_broyden,\n",
      "        ...                       jac_sparsity=sparsity_broyden(n))\n",
      "        >>> res_3.cost\n",
      "        4.5687069299604613e-23\n",
      "        >>> res_3.optimality\n",
      "        1.1650454296851518e-11\n",
      "\n",
      "        Let's also solve a curve fitting problem using robust loss function to\n",
      "        take care of outliers in the data. Define the model function as\n",
      "        ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\n",
      "        observation and a, b, c are parameters to estimate.\n",
      "\n",
      "        First, define the function which generates the data with noise and\n",
      "        outliers, define the model parameters, and generate data:\n",
      "\n",
      "        >>> from numpy.random import default_rng\n",
      "        >>> rng = default_rng()\n",
      "        >>> def gen_data(t, a, b, c, noise=0., n_outliers=0, seed=None):\n",
      "        ...     rng = default_rng(seed)\n",
      "        ...\n",
      "        ...     y = a + b * np.exp(t * c)\n",
      "        ...\n",
      "        ...     error = noise * rng.standard_normal(t.size)\n",
      "        ...     outliers = rng.integers(0, t.size, n_outliers)\n",
      "        ...     error[outliers] *= 10\n",
      "        ...\n",
      "        ...     return y + error\n",
      "        ...\n",
      "        >>> a = 0.5\n",
      "        >>> b = 2.0\n",
      "        >>> c = -1\n",
      "        >>> t_min = 0\n",
      "        >>> t_max = 10\n",
      "        >>> n_points = 15\n",
      "        ...\n",
      "        >>> t_train = np.linspace(t_min, t_max, n_points)\n",
      "        >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\n",
      "\n",
      "        Define function for computing residuals and initial estimate of\n",
      "        parameters.\n",
      "\n",
      "        >>> def fun(x, t, y):\n",
      "        ...     return x[0] + x[1] * np.exp(x[2] * t) - y\n",
      "        ...\n",
      "        >>> x0 = np.array([1.0, 1.0, 0.0])\n",
      "\n",
      "        Compute a standard least-squares solution:\n",
      "\n",
      "        >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\n",
      "\n",
      "        Now compute two solutions with two different robust loss functions. The\n",
      "        parameter `f_scale` is set to 0.1, meaning that inlier residuals should\n",
      "        not significantly exceed 0.1 (the noise level used).\n",
      "\n",
      "        >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\n",
      "        ...                             args=(t_train, y_train))\n",
      "        >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\n",
      "        ...                         args=(t_train, y_train))\n",
      "\n",
      "        And, finally, plot all the curves. We see that by selecting an appropriate\n",
      "        `loss`  we can get estimates close to optimal even in the presence of\n",
      "        strong outliers. But keep in mind that generally it is recommended to try\n",
      "        'soft_l1' or 'huber' losses first (if at all necessary) as the other two\n",
      "        options may cause difficulties in optimization process.\n",
      "\n",
      "        >>> t_test = np.linspace(t_min, t_max, n_points * 10)\n",
      "        >>> y_true = gen_data(t_test, a, b, c)\n",
      "        >>> y_lsq = gen_data(t_test, *res_lsq.x)\n",
      "        >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
      "        >>> y_log = gen_data(t_test, *res_log.x)\n",
      "        ...\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.plot(t_train, y_train, 'o')\n",
      "        >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\n",
      "        >>> plt.plot(t_test, y_lsq, label='linear loss')\n",
      "        >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\n",
      "        >>> plt.plot(t_test, y_log, label='cauchy loss')\n",
      "        >>> plt.xlabel(\"t\")\n",
      "        >>> plt.ylabel(\"y\")\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "\n",
      "        In the next example, we show how complex-valued residual functions of\n",
      "        complex variables can be optimized with ``least_squares()``. Consider the\n",
      "        following function:\n",
      "\n",
      "        >>> def f(z):\n",
      "        ...     return z - (0.5 + 0.5j)\n",
      "\n",
      "        We wrap it into a function of real variables that returns real residuals\n",
      "        by simply handling the real and imaginary parts as independent variables:\n",
      "\n",
      "        >>> def f_wrap(x):\n",
      "        ...     fx = f(x[0] + 1j*x[1])\n",
      "        ...     return np.array([fx.real, fx.imag])\n",
      "\n",
      "        Thus, instead of the original m-D complex function of n complex\n",
      "        variables we optimize a 2m-D real function of 2n real variables:\n",
      "\n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\n",
      "        >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\n",
      "        >>> z\n",
      "        (0.49999999999925893+0.49999999999925893j)\n",
      "\n",
      "    leastsq(func, x0, args=(), Dfun=None, full_output=False, col_deriv=False, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n",
      "        Minimize the sum of squares of a set of equations.\n",
      "\n",
      "        ::\n",
      "\n",
      "            x = arg min(sum(func(y)**2,axis=0))\n",
      "                     y\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            Should take at least one (possibly length ``N`` vector) argument and\n",
      "            returns ``M`` floating point numbers. It must not return NaNs or\n",
      "            fitting might fail. ``M`` must be greater than or equal to ``N``.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the minimization.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to func are placed in this tuple.\n",
      "        Dfun : callable, optional\n",
      "            A function or method to compute the Jacobian of func with derivatives\n",
      "            across the rows. If this is None, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            If ``True``, return all optional outputs (not just `x` and `ier`).\n",
      "        col_deriv : bool, optional\n",
      "            If ``True``, specify that the Jacobian function computes derivatives\n",
      "            down the columns (faster, because there is no transpose operation).\n",
      "        ftol : float, optional\n",
      "            Relative error desired in the sum of squares.\n",
      "        xtol : float, optional\n",
      "            Relative error desired in the approximate solution.\n",
      "        gtol : float, optional\n",
      "            Orthogonality desired between the function vector and the columns of\n",
      "            the Jacobian.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If `Dfun` is provided,\n",
      "            then the default `maxfev` is 100*(N+1) where N is the number of elements\n",
      "            in x0, otherwise the default `maxfev` is 200*(N+1).\n",
      "        epsfcn : float, optional\n",
      "            A variable used in determining a suitable step length for the forward-\n",
      "            difference approximation of the Jacobian (for Dfun=None).\n",
      "            Normally the actual step length will be sqrt(epsfcn)*x\n",
      "            If epsfcn is less than the machine precision, it is assumed that the\n",
      "            relative errors are of the order of the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the variables.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for an unsuccessful\n",
      "            call).\n",
      "        cov_x : ndarray\n",
      "            The inverse of the Hessian. `fjac` and `ipvt` are used to construct an\n",
      "            estimate of the Hessian. A value of None indicates a singular matrix,\n",
      "            which means the curvature in parameters `x` is numerically flat. To\n",
      "            obtain the covariance matrix of the parameters `x`, `cov_x` must be\n",
      "            multiplied by the variance of the residuals -- see curve_fit. Only\n",
      "            returned if `full_output` is ``True``.\n",
      "        infodict : dict\n",
      "            a dictionary of optional outputs with the keys:\n",
      "\n",
      "            ``nfev``\n",
      "                The number of function calls\n",
      "            ``fvec``\n",
      "                The function evaluated at the output\n",
      "            ``fjac``\n",
      "                A permutation of the R matrix of a QR\n",
      "                factorization of the final approximate\n",
      "                Jacobian matrix, stored column wise.\n",
      "                Together with ipvt, the covariance of the\n",
      "                estimate can be approximated.\n",
      "            ``ipvt``\n",
      "                An integer array of length N which defines\n",
      "                a permutation matrix, p, such that\n",
      "                fjac*p = q*r, where r is upper triangular\n",
      "                with diagonal elements of nonincreasing\n",
      "                magnitude. Column j of p is column ipvt(j)\n",
      "                of the identity matrix.\n",
      "            ``qtf``\n",
      "                The vector (transpose(q) * fvec).\n",
      "\n",
      "            Only returned if `full_output` is ``True``.\n",
      "        mesg : str\n",
      "            A string message giving information about the cause of failure.\n",
      "            Only returned if `full_output` is ``True``.\n",
      "        ier : int\n",
      "            An integer flag. If it is equal to 1, 2, 3 or 4, the solution was\n",
      "            found. Otherwise, the solution was not found. In either case, the\n",
      "            optional output variable 'mesg' gives more information.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Newer interface to solve nonlinear least-squares problems\n",
      "            with bounds on the variables. See ``method='lm'`` in particular.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        \"leastsq\" is a wrapper around MINPACK's lmdif and lmder algorithms.\n",
      "\n",
      "        cov_x is a Jacobian approximation to the Hessian of the least squares\n",
      "        objective function.\n",
      "        This approximation assumes that the objective function is based on the\n",
      "        difference between some observed target data (ydata) and a (non-linear)\n",
      "        function of the parameters `f(xdata, params)` ::\n",
      "\n",
      "               func(params) = ydata - f(xdata, params)\n",
      "\n",
      "        so that the objective function is ::\n",
      "\n",
      "               min   sum((ydata - f(xdata, params))**2, axis=0)\n",
      "             params\n",
      "\n",
      "        The solution, `x`, is always a 1-D array, regardless of the shape of `x0`,\n",
      "        or whether `x0` is a scalar.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import leastsq\n",
      "        >>> def func(x):\n",
      "        ...     return 2*(x-3)**2+1\n",
      "        >>> leastsq(func, 0)\n",
      "        (array([2.99999999]), 1)\n",
      "\n",
      "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=None, extra_condition=None, maxiter=10)\n",
      "        Find alpha that satisfies strong Wolfe conditions.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function.\n",
      "        myfprime : callable f'(x,*args)\n",
      "            Objective function gradient.\n",
      "        xk : ndarray\n",
      "            Starting point.\n",
      "        pk : ndarray\n",
      "            Search direction. The search direction must be a descent direction\n",
      "            for the algorithm to converge.\n",
      "        gfk : ndarray, optional\n",
      "            Gradient value for x=xk (xk being the current parameter\n",
      "            estimate). Will be recomputed if omitted.\n",
      "        old_fval : float, optional\n",
      "            Function value for x=xk. Will be recomputed if omitted.\n",
      "        old_old_fval : float, optional\n",
      "            Function value for the point preceding x=xk.\n",
      "        args : tuple, optional\n",
      "            Additional arguments passed to objective function.\n",
      "        c1 : float, optional\n",
      "            Parameter for Armijo condition rule.\n",
      "        c2 : float, optional\n",
      "            Parameter for curvature condition rule.\n",
      "        amax : float, optional\n",
      "            Maximum step size\n",
      "        extra_condition : callable, optional\n",
      "            A callable of the form ``extra_condition(alpha, x, f, g)``\n",
      "            returning a boolean. Arguments are the proposed step ``alpha``\n",
      "            and the corresponding ``x``, ``f`` and ``g`` values. The line search\n",
      "            accepts the value of ``alpha`` only if this\n",
      "            callable returns ``True``. If the callable returns ``False``\n",
      "            for the step length, the algorithm will continue with\n",
      "            new iterates. The callable is only called for iterates\n",
      "            satisfying the strong Wolfe conditions.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        alpha : float or None\n",
      "            Alpha for which ``x_new = x0 + alpha * pk``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        fc : int\n",
      "            Number of function evaluations made.\n",
      "        gc : int\n",
      "            Number of gradient evaluations made.\n",
      "        new_fval : float or None\n",
      "            New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        old_fval : float\n",
      "            Old function value ``f(x0)``.\n",
      "        new_slope : float or None\n",
      "            The local slope along the search direction at the\n",
      "            new value ``<myfprime(x_new), pk>``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Uses the line search algorithm to enforce strong Wolfe\n",
      "        conditions. See Wright and Nocedal, 'Numerical Optimization',\n",
      "        1999, pp. 59-61.\n",
      "\n",
      "        The search direction `pk` must be a descent direction (e.g.\n",
      "        ``-myfprime(xk)``) to find a step length that satisfies the strong Wolfe\n",
      "        conditions. If the search direction is not a descent direction (e.g.\n",
      "        ``myfprime(xk)``), then `alpha`, `new_fval`, and `new_slope` will be None.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import line_search\n",
      "\n",
      "        A objective function and its gradient are defined.\n",
      "\n",
      "        >>> def obj_func(x):\n",
      "        ...     return (x[0])**2+(x[1])**2\n",
      "        >>> def obj_grad(x):\n",
      "        ...     return [2*x[0], 2*x[1]]\n",
      "\n",
      "        We can find alpha that satisfies strong Wolfe conditions.\n",
      "\n",
      "        >>> start_point = np.array([1.8, 1.7])\n",
      "        >>> search_gradient = np.array([-1.0, -1.0])\n",
      "        >>> line_search(obj_func, obj_grad, start_point, search_gradient)\n",
      "        (1.0, 2, 1, 1.1300000000000001, 6.13, [1.6, 1.4])\n",
      "\n",
      "    linear_sum_assignment(...)\n",
      "        Solve the linear sum assignment problem.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cost_matrix : array\n",
      "            The cost matrix of the bipartite graph.\n",
      "\n",
      "        maximize : bool (default: False)\n",
      "            Calculates a maximum weight matching if true.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        row_ind, col_ind : array\n",
      "            An array of row indices and one of corresponding column indices giving\n",
      "            the optimal assignment. The cost of the assignment can be computed\n",
      "            as ``cost_matrix[row_ind, col_ind].sum()``. The row indices will be\n",
      "            sorted; in the case of a square cost matrix they will be equal to\n",
      "            ``numpy.arange(cost_matrix.shape[0])``.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        scipy.sparse.csgraph.min_weight_full_bipartite_matching : for sparse inputs\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "\n",
      "        The linear sum assignment problem [1]_ is also known as minimum weight\n",
      "        matching in bipartite graphs. A problem instance is described by a matrix\n",
      "        C, where each C[i,j] is the cost of matching vertex i of the first partite\n",
      "        set (a 'worker') and vertex j of the second set (a 'job'). The goal is to\n",
      "        find a complete assignment of workers to jobs of minimal cost.\n",
      "\n",
      "        Formally, let X be a boolean matrix where :math:`X[i,j] = 1` iff row i is\n",
      "        assigned to column j. Then the optimal assignment has cost\n",
      "\n",
      "        .. math::\n",
      "            \\min \\sum_i \\sum_j C_{i,j} X_{i,j}\n",
      "\n",
      "        where, in the case where the matrix X is square, each row is assigned to\n",
      "        exactly one column, and each column to exactly one row.\n",
      "\n",
      "        This function can also solve a generalization of the classic assignment\n",
      "        problem where the cost matrix is rectangular. If it has more rows than\n",
      "        columns, then not every row needs to be assigned to a column, and vice\n",
      "        versa.\n",
      "\n",
      "        This implementation is a modified Jonker-Volgenant algorithm with no\n",
      "        initialization, described in ref. [2]_.\n",
      "\n",
      "        .. versionadded:: 0.17.0\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "\n",
      "        .. [1] https://en.wikipedia.org/wiki/Assignment_problem\n",
      "\n",
      "        .. [2] DF Crouse. On implementing 2D rectangular assignment algorithms.\n",
      "               *IEEE Transactions on Aerospace and Electronic Systems*,\n",
      "               52(4):1679-1696, August 2016, :doi:`10.1109/TAES.2016.140952`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> cost = np.array([[4, 1, 3], [2, 0, 5], [3, 2, 2]])\n",
      "        >>> from scipy.optimize import linear_sum_assignment\n",
      "        >>> row_ind, col_ind = linear_sum_assignment(cost)\n",
      "        >>> col_ind\n",
      "        array([1, 0, 2])\n",
      "        >>> cost[row_ind, col_ind].sum()\n",
      "        5\n",
      "\n",
      "    linearmixing(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a scalar Jacobian approximation.\n",
      "\n",
      "        .. warning::\n",
      "\n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            The Jacobian approximation is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method='linearmixing'`` in particular.\n",
      "\n",
      "    linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=(0, None), method='highs', callback=None, options=None, x0=None, integrality=None)\n",
      "        Linear programming: minimize a linear objective function subject to linear\n",
      "        equality and inequality constraints.\n",
      "\n",
      "        Linear programming solves problems of the following form:\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            \\min_x \\ & c^T x \\\\\n",
      "            \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "            & A_{eq} x = b_{eq},\\\\\n",
      "            & l \\leq x \\leq u ,\n",
      "\n",
      "        where :math:`x` is a vector of decision variables; :math:`c`,\n",
      "        :math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      "        :math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "\n",
      "        Alternatively, that's:\n",
      "\n",
      "        - minimize ::\n",
      "\n",
      "            c @ x\n",
      "\n",
      "        - such that ::\n",
      "\n",
      "            A_ub @ x <= b_ub\n",
      "            A_eq @ x == b_eq\n",
      "            lb <= x <= ub\n",
      "\n",
      "        Note that by default ``lb = 0`` and ``ub = None``. Other bounds can be\n",
      "        specified with ``bounds``.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        c : 1-D array\n",
      "            The coefficients of the linear objective function to be minimized.\n",
      "        A_ub : 2-D array, optional\n",
      "            The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "            coefficients of a linear inequality constraint on ``x``.\n",
      "        b_ub : 1-D array, optional\n",
      "            The inequality constraint vector. Each element represents an\n",
      "            upper bound on the corresponding value of ``A_ub @ x``.\n",
      "        A_eq : 2-D array, optional\n",
      "            The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "            coefficients of a linear equality constraint on ``x``.\n",
      "        b_eq : 1-D array, optional\n",
      "            The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "            the corresponding element of ``b_eq``.\n",
      "        bounds : sequence, optional\n",
      "            A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the minimum and maximum values of that decision variable.\n",
      "            If a single tuple ``(min, max)`` is provided, then ``min`` and ``max``\n",
      "            will serve as bounds for all decision variables.\n",
      "            Use ``None`` to indicate that there is no bound. For instance, the\n",
      "            default bound ``(0, None)`` means that all decision variables are\n",
      "            non-negative, and the pair ``(None, None)`` means no bounds at all,\n",
      "            i.e. all variables are allowed to be any real.\n",
      "        method : str, optional\n",
      "            The algorithm used to solve the standard form problem.\n",
      "            The following are supported.\n",
      "\n",
      "            - :ref:`'highs' <optimize.linprog-highs>` (default)\n",
      "            - :ref:`'highs-ds' <optimize.linprog-highs-ds>`\n",
      "            - :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`\n",
      "            - :ref:`'interior-point' <optimize.linprog-interior-point>` (legacy)\n",
      "            - :ref:`'revised simplex' <optimize.linprog-revised_simplex>` (legacy)\n",
      "            - :ref:`'simplex' <optimize.linprog-simplex>` (legacy)\n",
      "\n",
      "            The legacy methods are deprecated and will be removed in SciPy 1.11.0.\n",
      "        callback : callable, optional\n",
      "            If a callback function is provided, it will be called at least once per\n",
      "            iteration of the algorithm. The callback function must accept a single\n",
      "            `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "\n",
      "            x : 1-D array\n",
      "                The current solution vector.\n",
      "            fun : float\n",
      "                The current value of the objective function ``c @ x``.\n",
      "            success : bool\n",
      "                ``True`` when the algorithm has completed successfully.\n",
      "            slack : 1-D array\n",
      "                The (nominally positive) values of the slack,\n",
      "                ``b_ub - A_ub @ x``.\n",
      "            con : 1-D array\n",
      "                The (nominally zero) residuals of the equality constraints,\n",
      "                ``b_eq - A_eq @ x``.\n",
      "            phase : int\n",
      "                The phase of the algorithm being executed.\n",
      "            status : int\n",
      "                An integer representing the status of the algorithm.\n",
      "\n",
      "                ``0`` : Optimization proceeding nominally.\n",
      "\n",
      "                ``1`` : Iteration limit reached.\n",
      "\n",
      "                ``2`` : Problem appears to be infeasible.\n",
      "\n",
      "                ``3`` : Problem appears to be unbounded.\n",
      "\n",
      "                ``4`` : Numerical difficulties encountered.\n",
      "\n",
      "            nit : int\n",
      "                The current iteration number.\n",
      "            message : str\n",
      "                A string descriptor of the algorithm status.\n",
      "\n",
      "            Callback functions are not currently supported by the HiGHS methods.\n",
      "\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            options:\n",
      "\n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "                Default: see method-specific documentation.\n",
      "            disp : bool\n",
      "                Set to ``True`` to print convergence messages.\n",
      "                Default: ``False``.\n",
      "            presolve : bool\n",
      "                Set to ``False`` to disable automatic presolve.\n",
      "                Default: ``True``.\n",
      "\n",
      "            All methods except the HiGHS solvers also accept:\n",
      "\n",
      "            tol : float\n",
      "                A tolerance which determines when a residual is \"close enough\" to\n",
      "                zero to be considered exactly zero.\n",
      "            autoscale : bool\n",
      "                Set to ``True`` to automatically perform equilibration.\n",
      "                Consider using this option if the numerical values in the\n",
      "                constraints are separated by several orders of magnitude.\n",
      "                Default: ``False``.\n",
      "            rr : bool\n",
      "                Set to ``False`` to disable automatic redundancy removal.\n",
      "                Default: ``True``.\n",
      "            rr_method : string\n",
      "                Method used to identify and remove redundant rows from the\n",
      "                equality constraint matrix after presolve. For problems with\n",
      "                dense input, the available methods for redundancy removal are:\n",
      "\n",
      "                ``SVD``:\n",
      "                    Repeatedly performs singular value decomposition on\n",
      "                    the matrix, detecting redundant rows based on nonzeros\n",
      "                    in the left singular vectors that correspond with\n",
      "                    zero singular values. May be fast when the matrix is\n",
      "                    nearly full rank.\n",
      "                ``pivot``:\n",
      "                    Uses the algorithm presented in [5]_ to identify\n",
      "                    redundant rows.\n",
      "                ``ID``:\n",
      "                    Uses a randomized interpolative decomposition.\n",
      "                    Identifies columns of the matrix transpose not used in\n",
      "                    a full-rank interpolative decomposition of the matrix.\n",
      "                ``None``:\n",
      "                    Uses ``svd`` if the matrix is nearly full rank, that is,\n",
      "                    the difference between the matrix rank and the number\n",
      "                    of rows is less than five. If not, uses ``pivot``. The\n",
      "                    behavior of this default is subject to change without\n",
      "                    prior notice.\n",
      "\n",
      "                Default: None.\n",
      "                For problems with sparse input, this option is ignored, and the\n",
      "                pivot-based algorithm presented in [5]_ is used.\n",
      "\n",
      "            For method-specific options, see\n",
      "            :func:`show_options('linprog') <show_options>`.\n",
      "\n",
      "        x0 : 1-D array, optional\n",
      "            Guess values of the decision variables, which will be refined by\n",
      "            the optimization algorithm. This argument is currently used only by the\n",
      "            :ref:`'revised simplex' <optimize.linprog-revised_simplex>` method,\n",
      "            and can only be used if `x0` represents a basic feasible solution.\n",
      "\n",
      "        integrality : 1-D array or int, optional\n",
      "            Indicates the type of integrality constraint on each decision variable.\n",
      "\n",
      "            ``0`` : Continuous variable; no integrality constraint.\n",
      "\n",
      "            ``1`` : Integer variable; decision variable must be an integer\n",
      "            within `bounds`.\n",
      "\n",
      "            ``2`` : Semi-continuous variable; decision variable must be within\n",
      "            `bounds` or take value ``0``.\n",
      "\n",
      "            ``3`` : Semi-integer variable; decision variable must be an integer\n",
      "            within `bounds` or take value ``0``.\n",
      "\n",
      "            By default, all variables are continuous.\n",
      "\n",
      "            For mixed integrality constraints, supply an array of shape ``c.shape``.\n",
      "            To infer a constraint on each decision variable from shorter inputs,\n",
      "            the argument will be broadcast to ``c.shape`` using `numpy.broadcast_to`.\n",
      "\n",
      "            This argument is currently used only by the\n",
      "            :ref:`'highs' <optimize.linprog-highs>` method and is ignored otherwise.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            A :class:`scipy.optimize.OptimizeResult` consisting of the fields\n",
      "            below. Note that the return types of the fields may depend on whether\n",
      "            the optimization was successful, therefore it is recommended to check\n",
      "            `OptimizeResult.status` before relying on the other fields:\n",
      "\n",
      "            x : 1-D array\n",
      "                The values of the decision variables that minimizes the\n",
      "                objective function while satisfying the constraints.\n",
      "            fun : float\n",
      "                The optimal value of the objective function ``c @ x``.\n",
      "            slack : 1-D array\n",
      "                The (nominally positive) values of the slack variables,\n",
      "                ``b_ub - A_ub @ x``.\n",
      "            con : 1-D array\n",
      "                The (nominally zero) residuals of the equality constraints,\n",
      "                ``b_eq - A_eq @ x``.\n",
      "            success : bool\n",
      "                ``True`` when the algorithm succeeds in finding an optimal\n",
      "                solution.\n",
      "            status : int\n",
      "                An integer representing the exit status of the algorithm.\n",
      "\n",
      "                ``0`` : Optimization terminated successfully.\n",
      "\n",
      "                ``1`` : Iteration limit reached.\n",
      "\n",
      "                ``2`` : Problem appears to be infeasible.\n",
      "\n",
      "                ``3`` : Problem appears to be unbounded.\n",
      "\n",
      "                ``4`` : Numerical difficulties encountered.\n",
      "\n",
      "            nit : int\n",
      "                The total number of iterations performed in all phases.\n",
      "            message : str\n",
      "                A string descriptor of the exit status of the algorithm.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter.\n",
      "\n",
      "        :ref:`'highs-ds' <optimize.linprog-highs-ds>`, and\n",
      "        :ref:`'highs-ipm' <optimize.linprog-highs-ipm>` are interfaces to the\n",
      "        HiGHS simplex and interior-point method solvers [13]_, respectively.\n",
      "        :ref:`'highs' <optimize.linprog-highs>` (default) chooses between\n",
      "        the two automatically. These are the fastest linear\n",
      "        programming solvers in SciPy, especially for large, sparse problems;\n",
      "        which of these two is faster is problem-dependent.\n",
      "        The other solvers are legacy methods and will be removed when `callback` is\n",
      "        supported by the HiGHS methods.\n",
      "\n",
      "        Method :ref:`'highs-ds' <optimize.linprog-highs-ds>`, is a wrapper of the C++ high\n",
      "        performance dual revised simplex implementation (HSOL) [13]_, [14]_.\n",
      "        Method :ref:`'highs-ipm' <optimize.linprog-highs-ipm>` is a wrapper of a C++\n",
      "        implementation of an **i**\\ nterior-\\ **p**\\ oint **m**\\ ethod [13]_; it\n",
      "        features a crossover routine, so it is as accurate as a simplex solver.\n",
      "        Method :ref:`'highs' <optimize.linprog-highs>` chooses between the two\n",
      "        automatically.\n",
      "        For new code involving `linprog`, we recommend explicitly choosing one of\n",
      "        these three method values.\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        Method :ref:`'interior-point' <optimize.linprog-interior-point>`\n",
      "        uses the primal-dual path following algorithm\n",
      "        as outlined in [4]_. This algorithm supports sparse constraint matrices and\n",
      "        is typically faster than the simplex methods, especially for large, sparse\n",
      "        problems. Note, however, that the solution returned may be slightly less\n",
      "        accurate than those of the simplex methods and will not, in general,\n",
      "        correspond with a vertex of the polytope defined by the constraints.\n",
      "\n",
      "        .. versionadded:: 1.0.0\n",
      "\n",
      "        Method :ref:`'revised simplex' <optimize.linprog-revised_simplex>`\n",
      "        uses the revised simplex method as described in\n",
      "        [9]_, except that a factorization [11]_ of the basis matrix, rather than\n",
      "        its inverse, is efficiently maintained and used to solve the linear systems\n",
      "        at each iteration of the algorithm.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Method :ref:`'simplex' <optimize.linprog-simplex>` uses a traditional,\n",
      "        full-tableau implementation of\n",
      "        Dantzig's simplex algorithm [1]_, [2]_ (*not* the\n",
      "        Nelder-Mead simplex). This algorithm is included for backwards\n",
      "        compatibility and educational purposes.\n",
      "\n",
      "        .. versionadded:: 0.15.0\n",
      "\n",
      "        Before applying :ref:`'interior-point' <optimize.linprog-interior-point>`,\n",
      "        :ref:`'revised simplex' <optimize.linprog-revised_simplex>`, or\n",
      "        :ref:`'simplex' <optimize.linprog-simplex>`,\n",
      "        a presolve procedure based on [8]_ attempts\n",
      "        to identify trivial infeasibilities, trivial unboundedness, and potential\n",
      "        problem simplifications. Specifically, it checks for:\n",
      "\n",
      "        - rows of zeros in ``A_eq`` or ``A_ub``, representing trivial constraints;\n",
      "        - columns of zeros in ``A_eq`` `and` ``A_ub``, representing unconstrained\n",
      "          variables;\n",
      "        - column singletons in ``A_eq``, representing fixed variables; and\n",
      "        - column singletons in ``A_ub``, representing simple bounds.\n",
      "\n",
      "        If presolve reveals that the problem is unbounded (e.g. an unconstrained\n",
      "        and unbounded variable has negative cost) or infeasible (e.g., a row of\n",
      "        zeros in ``A_eq`` corresponds with a nonzero in ``b_eq``), the solver\n",
      "        terminates with the appropriate status code. Note that presolve terminates\n",
      "        as soon as any sign of unboundedness is detected; consequently, a problem\n",
      "        may be reported as unbounded when in reality the problem is infeasible\n",
      "        (but infeasibility has not been detected yet). Therefore, if it is\n",
      "        important to know whether the problem is actually infeasible, solve the\n",
      "        problem again with option ``presolve=False``.\n",
      "\n",
      "        If neither infeasibility nor unboundedness are detected in a single pass\n",
      "        of the presolve, bounds are tightened where possible and fixed\n",
      "        variables are removed from the problem. Then, linearly dependent rows\n",
      "        of the ``A_eq`` matrix are removed, (unless they represent an\n",
      "        infeasibility) to avoid numerical difficulties in the primary solve\n",
      "        routine. Note that rows that are nearly linearly dependent (within a\n",
      "        prescribed tolerance) may also be removed, which can change the optimal\n",
      "        solution in rare cases. If this is a concern, eliminate redundancy from\n",
      "        your problem formulation and run with option ``rr=False`` or\n",
      "        ``presolve=False``.\n",
      "\n",
      "        Several potential improvements can be made here: additional presolve\n",
      "        checks outlined in [8]_ should be implemented, the presolve routine should\n",
      "        be run multiple times (until no further simplifications can be made), and\n",
      "        more of the efficiency improvements from [5]_ should be implemented in the\n",
      "        redundancy removal routines.\n",
      "\n",
      "        After presolve, the problem is transformed to standard form by converting\n",
      "        the (tightened) simple bounds to upper bound constraints, introducing\n",
      "        non-negative slack variables for inequality constraints, and expressing\n",
      "        unbounded variables as the difference between two non-negative variables.\n",
      "        Optionally, the problem is automatically scaled via equilibration [12]_.\n",
      "        The selected algorithm solves the standard form problem, and a\n",
      "        postprocessing routine converts the result to a solution to the original\n",
      "        problem.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "               Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "               1963\n",
      "        .. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "               Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      "        .. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "               Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "        .. [4] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "               optimizer for linear programming: an implementation of the\n",
      "               homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "               2000. 197-232.\n",
      "        .. [5] Andersen, Erling D. \"Finding all linearly dependent rows in\n",
      "               large-scale linear programming.\" Optimization Methods and Software\n",
      "               6.3 (1995): 219-227.\n",
      "        .. [6] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "               Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "               March 2004. Available 2/25/2017 at\n",
      "               https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      "        .. [7] Fourer, Robert. \"Solving Linear Programs by Interior-Point Methods.\"\n",
      "               Unpublished Course Notes, August 26, 2005. Available 2/25/2017 at\n",
      "               http://www.4er.org/CourseNotes/Book%20B/B-III.pdf\n",
      "        .. [8] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "               programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      "        .. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "               programming.\" Athena Scientific 1 (1997): 997.\n",
      "        .. [10] Andersen, Erling D., et al. Implementation of interior point\n",
      "                methods for large scale linear programming. HEC/Universite de\n",
      "                Geneve, 1996.\n",
      "        .. [11] Bartels, Richard H. \"A stabilization of the simplex method.\"\n",
      "                Journal in  Numerische Mathematik 16.5 (1971): 414-434.\n",
      "        .. [12] Tomlin, J. A. \"On scaling linear programming problems.\"\n",
      "                Mathematical Programming Study 4 (1975): 146-166.\n",
      "        .. [13] Huangfu, Q., Galabova, I., Feldmeier, M., and Hall, J. A. J.\n",
      "                \"HiGHS - high performance software for linear optimization.\"\n",
      "                https://highs.dev/\n",
      "        .. [14] Huangfu, Q. and Hall, J. A. J. \"Parallelizing the dual revised\n",
      "                simplex method.\" Mathematical Programming Computation, 10 (1),\n",
      "                119-142, 2018. DOI: 10.1007/s12532-017-0130-5\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Consider the following problem:\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            \\min_{x_0, x_1} \\ -x_0 + 4x_1 & \\\\\n",
      "            \\mbox{such that} \\ -3x_0 + x_1 & \\leq 6,\\\\\n",
      "            -x_0 - 2x_1 & \\geq -4,\\\\\n",
      "            x_1 & \\geq -3.\n",
      "\n",
      "        The problem is not presented in the form accepted by `linprog`. This is\n",
      "        easily remedied by converting the \"greater than\" inequality\n",
      "        constraint to a \"less than\" inequality constraint by\n",
      "        multiplying both sides by a factor of :math:`-1`. Note also that the last\n",
      "        constraint is really the simple bound :math:`-3 \\leq x_1 \\leq \\infty`.\n",
      "        Finally, since there are no bounds on :math:`x_0`, we must explicitly\n",
      "        specify the bounds :math:`-\\infty \\leq x_0 \\leq \\infty`, as the\n",
      "        default is for variables to be non-negative. After collecting coeffecients\n",
      "        into arrays and tuples, the input for this problem is:\n",
      "\n",
      "        >>> from scipy.optimize import linprog\n",
      "        >>> c = [-1, 4]\n",
      "        >>> A = [[-3, 1], [1, 2]]\n",
      "        >>> b = [6, 4]\n",
      "        >>> x0_bounds = (None, None)\n",
      "        >>> x1_bounds = (-3, None)\n",
      "        >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds])\n",
      "        >>> res.fun\n",
      "        -22.0\n",
      "        >>> res.x\n",
      "        array([10., -3.])\n",
      "        >>> res.message\n",
      "        'Optimization terminated successfully. (HiGHS Status 7: Optimal)'\n",
      "\n",
      "        The marginals (AKA dual values / shadow prices / Lagrange multipliers)\n",
      "        and residuals (slacks) are also available.\n",
      "\n",
      "        >>> res.ineqlin\n",
      "          residual: [ 3.900e+01  0.000e+00]\n",
      "         marginals: [-0.000e+00 -1.000e+00]\n",
      "\n",
      "        For example, because the marginal associated with the second inequality\n",
      "        constraint is -1, we expect the optimal value of the objective function\n",
      "        to decrease by ``eps`` if we add a small amount ``eps`` to the right hand\n",
      "        side of the second inequality constraint:\n",
      "\n",
      "        >>> eps = 0.05\n",
      "        >>> b[1] += eps\n",
      "        >>> linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds]).fun\n",
      "        -22.05\n",
      "\n",
      "        Also, because the residual on the first inequality constraint is 39, we\n",
      "        can decrease the right hand side of the first constraint by 39 without\n",
      "        affecting the optimal solution.\n",
      "\n",
      "        >>> b = [6, 4]  # reset to original values\n",
      "        >>> b[0] -= 39\n",
      "        >>> linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds]).fun\n",
      "        -22.0\n",
      "\n",
      "    linprog_verbose_callback(res)\n",
      "        A sample callback function demonstrating the linprog callback interface.\n",
      "        This callback produces detailed output to sys.stdout before each iteration\n",
      "        and after the final iteration of the simplex algorithm.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        res : A `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "\n",
      "            x : 1-D array\n",
      "                The independent variable vector which optimizes the linear\n",
      "                programming problem.\n",
      "            fun : float\n",
      "                Value of the objective function.\n",
      "            success : bool\n",
      "                True if the algorithm succeeded in finding an optimal solution.\n",
      "            slack : 1-D array\n",
      "                The values of the slack variables. Each slack variable corresponds\n",
      "                to an inequality constraint. If the slack is zero, then the\n",
      "                corresponding constraint is active.\n",
      "            con : 1-D array\n",
      "                The (nominally zero) residuals of the equality constraints, that is,\n",
      "                ``b - A_eq @ x``\n",
      "            phase : int\n",
      "                The phase of the optimization being executed. In phase 1 a basic\n",
      "                feasible solution is sought and the T has an additional row\n",
      "                representing an alternate objective function.\n",
      "            status : int\n",
      "                An integer representing the exit status of the optimization:\n",
      "\n",
      "                ``0`` : Optimization terminated successfully\n",
      "\n",
      "                ``1`` : Iteration limit reached\n",
      "\n",
      "                ``2`` : Problem appears to be infeasible\n",
      "\n",
      "                ``3`` : Problem appears to be unbounded\n",
      "\n",
      "                ``4`` : Serious numerical difficulties encountered\n",
      "\n",
      "            nit : int\n",
      "                The number of iterations performed.\n",
      "            message : str\n",
      "                A string descriptor of the exit status of the optimization.\n",
      "\n",
      "    lsq_linear(A, b, bounds=(-inf, inf), method='trf', tol=1e-10, lsq_solver=None, lsmr_tol=None, max_iter=None, verbose=0, *, lsmr_maxiter=None)\n",
      "        Solve a linear least-squares problem with bounds on the variables.\n",
      "\n",
      "        Given a m-by-n design matrix A and a target vector b with m elements,\n",
      "        `lsq_linear` solves the following optimization problem::\n",
      "\n",
      "            minimize 0.5 * ||A x - b||**2\n",
      "            subject to lb <= x <= ub\n",
      "\n",
      "        This optimization problem is convex, hence a found minimum (if iterations\n",
      "        have converged) is guaranteed to be global.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        A : array_like, sparse array or LinearOperator, shape (m, n)\n",
      "            Design matrix. Can be `scipy.sparse.linalg.LinearOperator`.\n",
      "        b : array_like, shape (m,)\n",
      "            Target vector.\n",
      "        bounds : 2-tuple of array_like or `Bounds`, optional\n",
      "            Lower and upper bounds on parameters. Defaults to no bounds.\n",
      "            There are two ways to specify the bounds:\n",
      "\n",
      "            - Instance of `Bounds` class.\n",
      "            - 2-tuple of array_like: Each element of the tuple must be either\n",
      "              an array with the length equal to the number of parameters, or a\n",
      "              scalar (in which case the bound is taken to be the same for all\n",
      "              parameters). Use ``np.inf`` with an appropriate sign to disable\n",
      "              bounds on all or some parameters.\n",
      "\n",
      "        method : 'trf' or 'bvls', optional\n",
      "            Method to perform minimization.\n",
      "\n",
      "            * 'trf' : Trust Region Reflective algorithm adapted for a linear\n",
      "              least-squares problem. This is an interior-point-like method\n",
      "              and the required number of iterations is weakly correlated with\n",
      "              the number of variables.\n",
      "            * 'bvls' : Bounded-variable least-squares algorithm. This is\n",
      "              an active set method, which requires the number of iterations\n",
      "              comparable to the number of variables. Can't be used when `A` is\n",
      "              sparse or LinearOperator.\n",
      "\n",
      "            Default is 'trf'.\n",
      "        tol : float, optional\n",
      "            Tolerance parameter. The algorithm terminates if a relative change\n",
      "            of the cost function is less than `tol` on the last iteration.\n",
      "            Additionally, the first-order optimality measure is considered:\n",
      "\n",
      "            * ``method='trf'`` terminates if the uniform norm of the gradient,\n",
      "              scaled to account for the presence of the bounds, is less than\n",
      "              `tol`.\n",
      "            * ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n",
      "              are satisfied within `tol` tolerance.\n",
      "\n",
      "        lsq_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method of solving unbounded least-squares problems throughout\n",
      "            iterations:\n",
      "\n",
      "            * 'exact' : Use dense QR or SVD decomposition approach. Can't be\n",
      "              used when `A` is sparse or LinearOperator.\n",
      "            * 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n",
      "              which requires only matrix-vector product evaluations. Can't\n",
      "              be used with ``method='bvls'``.\n",
      "\n",
      "            If None (default), the solver is chosen based on type of `A`.\n",
      "        lsmr_tol : None, float or 'auto', optional\n",
      "            Tolerance parameters 'atol' and 'btol' for `scipy.sparse.linalg.lsmr`\n",
      "            If None (default), it is set to ``1e-2 * tol``. If 'auto', the\n",
      "            tolerance will be adjusted based on the optimality of the current\n",
      "            iterate, which can speed up the optimization process, but is not always\n",
      "            reliable.\n",
      "        max_iter : None or int, optional\n",
      "            Maximum number of iterations before termination. If None (default), it\n",
      "            is set to 100 for ``method='trf'`` or to the number of variables for\n",
      "            ``method='bvls'`` (not counting iterations for 'bvls' initialization).\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "\n",
      "            * 0 : work silently (default).\n",
      "            * 1 : display a termination report.\n",
      "            * 2 : display progress during iterations.\n",
      "\n",
      "        lsmr_maxiter : None or int, optional\n",
      "            Maximum number of iterations for the lsmr least squares solver,\n",
      "            if it is used (by setting ``lsq_solver='lsmr'``). If None (default), it\n",
      "            uses lsmr's default of ``min(m, n)`` where ``m`` and ``n`` are the\n",
      "            number of rows and columns of `A`, respectively. Has no effect if\n",
      "            ``lsq_solver='exact'``.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        OptimizeResult with the following fields defined:\n",
      "        x : ndarray, shape (n,)\n",
      "            Solution found.\n",
      "        cost : float\n",
      "            Value of the cost function at the solution.\n",
      "        fun : ndarray, shape (m,)\n",
      "            Vector of residuals at the solution.\n",
      "        optimality : float\n",
      "            First-order optimality measure. The exact meaning depends on `method`,\n",
      "            refer to the description of `tol` parameter.\n",
      "        active_mask : ndarray of int, shape (n,)\n",
      "            Each component shows whether a corresponding constraint is active\n",
      "            (that is, whether a variable is at the bound):\n",
      "\n",
      "            *  0 : a constraint is not active.\n",
      "            * -1 : a lower bound is active.\n",
      "            *  1 : an upper bound is active.\n",
      "\n",
      "            Might be somewhat arbitrary for the `trf` method as it generates a\n",
      "            sequence of strictly feasible iterates and active_mask is determined\n",
      "            within a tolerance threshold.\n",
      "        unbounded_sol : tuple\n",
      "            Unbounded least squares solution tuple returned by the least squares\n",
      "            solver (set with `lsq_solver` option). If `lsq_solver` is not set or is\n",
      "            set to ``'exact'``, the tuple contains an ndarray of shape (n,) with\n",
      "            the unbounded solution, an ndarray with the sum of squared residuals,\n",
      "            an int with the rank of `A`, and an ndarray with the singular values\n",
      "            of `A` (see NumPy's ``linalg.lstsq`` for more information). If\n",
      "            `lsq_solver` is set to ``'lsmr'``, the tuple contains an ndarray of\n",
      "            shape (n,) with the unbounded solution, an int with the exit code,\n",
      "            an int with the number of iterations, and five floats with\n",
      "            various norms and the condition number of `A` (see SciPy's\n",
      "            ``sparse.linalg.lsmr`` for more information). This output can be\n",
      "            useful for determining the convergence of the least squares solver,\n",
      "            particularly the iterative ``'lsmr'`` solver. The unbounded least\n",
      "            squares problem is to minimize ``0.5 * ||A x - b||**2``.\n",
      "        nit : int\n",
      "            Number of iterations. Zero if the unconstrained solution is optimal.\n",
      "        status : int\n",
      "            Reason for algorithm termination:\n",
      "\n",
      "            * -1 : the algorithm was not able to make progress on the last\n",
      "              iteration.\n",
      "            *  0 : the maximum number of iterations is exceeded.\n",
      "            *  1 : the first-order optimality measure is less than `tol`.\n",
      "            *  2 : the relative change of the cost function is less than `tol`.\n",
      "            *  3 : the unconstrained solution is optimal.\n",
      "\n",
      "        message : str\n",
      "            Verbal description of the termination reason.\n",
      "        success : bool\n",
      "            True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        nnls : Linear least squares with non-negativity constraint.\n",
      "        least_squares : Nonlinear least squares with bounds on the variables.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm first computes the unconstrained least-squares solution by\n",
      "        `numpy.linalg.lstsq` or `scipy.sparse.linalg.lsmr` depending on\n",
      "        `lsq_solver`. This solution is returned as optimal if it lies within the\n",
      "        bounds.\n",
      "\n",
      "        Method 'trf' runs the adaptation of the algorithm described in [STIR]_ for\n",
      "        a linear least-squares problem. The iterations are essentially the same as\n",
      "        in the nonlinear least-squares algorithm, but as the quadratic function\n",
      "        model is always accurate, we don't need to track or modify the radius of\n",
      "        a trust region. The line search (backtracking) is used as a safety net\n",
      "        when a selected step does not decrease the cost function. Read more\n",
      "        detailed description of the algorithm in `scipy.optimize.least_squares`.\n",
      "\n",
      "        Method 'bvls' runs a Python implementation of the algorithm described in\n",
      "        [BVLS]_. The algorithm maintains active and free sets of variables, on\n",
      "        each iteration chooses a new variable to move from the active set to the\n",
      "        free set and then solves the unconstrained least-squares problem on free\n",
      "        variables. This algorithm is guaranteed to give an accurate solution\n",
      "        eventually, but may require up to n iterations for a problem with n\n",
      "        variables. Additionally, an ad-hoc initialization procedure is\n",
      "        implemented, that determines which variables to set free or active\n",
      "        initially. It takes some number of iterations before actual BVLS starts,\n",
      "        but can significantly reduce the number of further iterations.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [BVLS] P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:\n",
      "                  an Algorithm and Applications\", Computational Statistics, 10,\n",
      "                  129-141, 1995.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        In this example, a problem with a large sparse arrays and bounds on the\n",
      "        variables is solved.\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.sparse import random_array\n",
      "        >>> from scipy.optimize import lsq_linear\n",
      "        >>> rng = np.random.default_rng()\n",
      "        ...\n",
      "        >>> m = 2000\n",
      "        >>> n = 1000\n",
      "        ...\n",
      "        >>> A = random_array((m, n), density=1e-4, random_state=rng)\n",
      "        >>> b = rng.standard_normal(m)\n",
      "        ...\n",
      "        >>> lb = rng.standard_normal(n)\n",
      "        >>> ub = lb + 1\n",
      "        ...\n",
      "        >>> res = lsq_linear(A, b, bounds=(lb, ub), lsmr_tol='auto', verbose=1)\n",
      "        The relative change of the cost function is less than `tol`.\n",
      "        Number of iterations 10, initial cost 1.0070e+03, final cost 9.6602e+02,\n",
      "        first-order optimality 2.21e-09.        # may vary\n",
      "\n",
      "    milp(c, *, integrality=None, bounds=None, constraints=None, options=None)\n",
      "        Mixed-integer linear programming\n",
      "\n",
      "        Solves problems of the following form:\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            \\min_x \\ & c^T x \\\\\n",
      "            \\mbox{such that} \\ & b_l \\leq A x \\leq b_u,\\\\\n",
      "            & l \\leq x \\leq u, \\\\\n",
      "            & x_i \\in \\mathbb{Z}, i \\in X_i\n",
      "\n",
      "        where :math:`x` is a vector of decision variables;\n",
      "        :math:`c`, :math:`b_l`, :math:`b_u`, :math:`l`, and :math:`u` are vectors;\n",
      "        :math:`A` is a matrix, and :math:`X_i` is the set of indices of\n",
      "        decision variables that must be integral. (In this context, a\n",
      "        variable that can assume only integer values is said to be \"integral\";\n",
      "        it has an \"integrality\" constraint.)\n",
      "\n",
      "        Alternatively, that's:\n",
      "\n",
      "        minimize::\n",
      "\n",
      "            c @ x\n",
      "\n",
      "        such that::\n",
      "\n",
      "            b_l <= A @ x <= b_u\n",
      "            l <= x <= u\n",
      "            Specified elements of x must be integers\n",
      "\n",
      "        By default, ``l = 0`` and ``u = np.inf`` unless specified with\n",
      "        ``bounds``.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        c : 1D dense array_like\n",
      "            The coefficients of the linear objective function to be minimized.\n",
      "            `c` is converted to a double precision array before the problem is\n",
      "            solved.\n",
      "        integrality : 1D dense array_like, optional\n",
      "            Indicates the type of integrality constraint on each decision variable.\n",
      "\n",
      "            ``0`` : Continuous variable; no integrality constraint.\n",
      "\n",
      "            ``1`` : Integer variable; decision variable must be an integer\n",
      "            within `bounds`.\n",
      "\n",
      "            ``2`` : Semi-continuous variable; decision variable must be within\n",
      "            `bounds` or take value ``0``.\n",
      "\n",
      "            ``3`` : Semi-integer variable; decision variable must be an integer\n",
      "            within `bounds` or take value ``0``.\n",
      "\n",
      "            By default, all variables are continuous. `integrality` is converted\n",
      "            to an array of integers before the problem is solved.\n",
      "\n",
      "        bounds : scipy.optimize.Bounds, optional\n",
      "            Bounds on the decision variables. Lower and upper bounds are converted\n",
      "            to double precision arrays before the problem is solved. The\n",
      "            ``keep_feasible`` parameter of the `Bounds` object is ignored. If\n",
      "            not specified, all decision variables are constrained to be\n",
      "            non-negative.\n",
      "        constraints : sequence of scipy.optimize.LinearConstraint, optional\n",
      "            Linear constraints of the optimization problem. Arguments may be\n",
      "            one of the following:\n",
      "\n",
      "            1. A single `LinearConstraint` object\n",
      "            2. A single tuple that can be converted to a `LinearConstraint` object\n",
      "               as ``LinearConstraint(*constraints)``\n",
      "            3. A sequence composed entirely of objects of type 1. and 2.\n",
      "\n",
      "            Before the problem is solved, all values are converted to double\n",
      "            precision, and the matrices of constraint coefficients are converted to\n",
      "            instances of `scipy.sparse.csc_array`. The ``keep_feasible`` parameter\n",
      "            of `LinearConstraint` objects is ignored.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. The following keys are recognized.\n",
      "\n",
      "            disp : bool (default: ``False``)\n",
      "                Set to ``True`` if indicators of optimization status are to be\n",
      "                printed to the console during optimization.\n",
      "            node_limit : int, optional\n",
      "                The maximum number of nodes (linear program relaxations) to solve\n",
      "                before stopping. Default is no maximum number of nodes.\n",
      "            presolve : bool (default: ``True``)\n",
      "                Presolve attempts to identify trivial infeasibilities,\n",
      "                identify trivial unboundedness, and simplify the problem before\n",
      "                sending it to the main solver.\n",
      "            time_limit : float, optional\n",
      "                The maximum number of seconds allotted to solve the problem.\n",
      "                Default is no time limit.\n",
      "            mip_rel_gap : float, optional\n",
      "                Termination criterion for MIP solver: solver will terminate when\n",
      "                the gap between the primal objective value and the dual objective\n",
      "                bound, scaled by the primal objective value, is <= mip_rel_gap.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            An instance of :class:`scipy.optimize.OptimizeResult`. The object\n",
      "            is guaranteed to have the following attributes.\n",
      "\n",
      "            status : int\n",
      "                An integer representing the exit status of the algorithm.\n",
      "\n",
      "                ``0`` : Optimal solution found.\n",
      "\n",
      "                ``1`` : Iteration or time limit reached.\n",
      "\n",
      "                ``2`` : Problem is infeasible.\n",
      "\n",
      "                ``3`` : Problem is unbounded.\n",
      "\n",
      "                ``4`` : Other; see message for details.\n",
      "\n",
      "            success : bool\n",
      "                ``True`` when an optimal solution is found and ``False`` otherwise.\n",
      "\n",
      "            message : str\n",
      "                A string descriptor of the exit status of the algorithm.\n",
      "\n",
      "            The following attributes will also be present, but the values may be\n",
      "            ``None``, depending on the solution status.\n",
      "\n",
      "            x : ndarray\n",
      "                The values of the decision variables that minimize the\n",
      "                objective function while satisfying the constraints.\n",
      "            fun : float\n",
      "                The optimal value of the objective function ``c @ x``.\n",
      "            mip_node_count : int\n",
      "                The number of subproblems or \"nodes\" solved by the MILP solver.\n",
      "            mip_dual_bound : float\n",
      "                The MILP solver's final estimate of the lower bound on the optimal\n",
      "                solution.\n",
      "            mip_gap : float\n",
      "                The difference between the primal objective value and the dual\n",
      "                objective bound, scaled by the primal objective value.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        `milp` is a wrapper of the HiGHS linear optimization software [1]_. The\n",
      "        algorithm is deterministic, and it typically finds the global optimum of\n",
      "        moderately challenging mixed-integer linear programs (when it exists).\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Huangfu, Q., Galabova, I., Feldmeier, M., and Hall, J. A. J.\n",
      "               \"HiGHS - high performance software for linear optimization.\"\n",
      "               https://highs.dev/\n",
      "        .. [2] Huangfu, Q. and Hall, J. A. J. \"Parallelizing the dual revised\n",
      "               simplex method.\" Mathematical Programming Computation, 10 (1),\n",
      "               119-142, 2018. DOI: 10.1007/s12532-017-0130-5\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Consider the problem at\n",
      "        https://en.wikipedia.org/wiki/Integer_programming#Example, which is\n",
      "        expressed as a maximization problem of two variables. Since `milp` requires\n",
      "        that the problem be expressed as a minimization problem, the objective\n",
      "        function coefficients on the decision variables are:\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> c = -np.array([0, 1])\n",
      "\n",
      "        Note the negative sign: we maximize the original objective function\n",
      "        by minimizing the negative of the objective function.\n",
      "\n",
      "        We collect the coefficients of the constraints into arrays like:\n",
      "\n",
      "        >>> A = np.array([[-1, 1], [3, 2], [2, 3]])\n",
      "        >>> b_u = np.array([1, 12, 12])\n",
      "        >>> b_l = np.full_like(b_u, -np.inf, dtype=float)\n",
      "\n",
      "        Because there is no lower limit on these constraints, we have defined a\n",
      "        variable ``b_l`` full of values representing negative infinity. This may\n",
      "        be unfamiliar to users of `scipy.optimize.linprog`, which only accepts\n",
      "        \"less than\" (or \"upper bound\") inequality constraints of the form\n",
      "        ``A_ub @ x <= b_u``. By accepting both ``b_l`` and ``b_u`` of constraints\n",
      "        ``b_l <= A_ub @ x <= b_u``, `milp` makes it easy to specify \"greater than\"\n",
      "        inequality constraints, \"less than\" inequality constraints, and equality\n",
      "        constraints concisely.\n",
      "\n",
      "        These arrays are collected into a single `LinearConstraint` object like:\n",
      "\n",
      "        >>> from scipy.optimize import LinearConstraint\n",
      "        >>> constraints = LinearConstraint(A, b_l, b_u)\n",
      "\n",
      "        The non-negativity bounds on the decision variables are enforced by\n",
      "        default, so we do not need to provide an argument for `bounds`.\n",
      "\n",
      "        Finally, the problem states that both decision variables must be integers:\n",
      "\n",
      "        >>> integrality = np.ones_like(c)\n",
      "\n",
      "        We solve the problem like:\n",
      "\n",
      "        >>> from scipy.optimize import milp\n",
      "        >>> res = milp(c=c, constraints=constraints, integrality=integrality)\n",
      "        >>> res.x\n",
      "        [2.0, 2.0]\n",
      "\n",
      "        Note that had we solved the relaxed problem (without integrality\n",
      "        constraints):\n",
      "\n",
      "        >>> res = milp(c=c, constraints=constraints)  # OR:\n",
      "        >>> # from scipy.optimize import linprog; res = linprog(c, A, b_u)\n",
      "        >>> res.x\n",
      "        [1.8, 2.8]\n",
      "\n",
      "        we would not have obtained the correct solution by rounding to the nearest\n",
      "        integers.\n",
      "\n",
      "        Other examples are given :ref:`in the tutorial <tutorial-optimize_milp>`.\n",
      "\n",
      "    minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "        Minimization of scalar function of one or more variables.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            The objective function to be minimized::\n",
      "\n",
      "                fun(x, *args) -> float\n",
      "\n",
      "            where ``x`` is a 1-D array with shape (n,) and ``args``\n",
      "            is a tuple of the fixed parameters needed to completely\n",
      "            specify the function.\n",
      "\n",
      "            Suppose the callable has signature ``f0(x, *my_args, **my_kwargs)``, where\n",
      "            ``my_args`` and ``my_kwargs`` are required positional and keyword arguments.\n",
      "            Rather than passing ``f0`` as the callable, wrap it to accept\n",
      "            only ``x``; e.g., pass ``fun=lambda x: f0(x, *my_args, **my_kwargs)`` as the\n",
      "            callable, where ``my_args`` (tuple) and ``my_kwargs`` (dict) have been\n",
      "            gathered before invoking this function.\n",
      "        x0 : ndarray, shape (n,)\n",
      "            Initial guess. Array of real elements of size (n,),\n",
      "            where ``n`` is the number of independent variables.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its\n",
      "            derivatives (`fun`, `jac` and `hess` functions).\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of\n",
      "\n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'COBYQA'      :ref:`(see here) <optimize.minimize-cobyqa>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "            - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "            - custom - a callable object, see below for description.\n",
      "\n",
      "            If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "            depending on whether or not the problem has constraints or bounds.\n",
      "        jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "            Method for computing the gradient vector. Only for CG, BFGS,\n",
      "            Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "            trust-exact and trust-constr.\n",
      "            If it is a callable, it should be a function that returns the gradient\n",
      "            vector::\n",
      "\n",
      "                jac(x, *args) -> array_like, shape (n,)\n",
      "\n",
      "            where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n",
      "            the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n",
      "            assumed to return a tuple ``(f, g)`` containing the objective\n",
      "            function and the gradient.\n",
      "            Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n",
      "            'trust-krylov' require that either a callable be supplied, or that\n",
      "            `fun` return the objective and gradient.\n",
      "            If None or False, the gradient will be estimated using 2-point finite\n",
      "            difference estimation with an absolute step size.\n",
      "            Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n",
      "            to select a finite difference scheme for numerical estimation of the\n",
      "            gradient with a relative step size. These finite difference schemes\n",
      "            obey any specified `bounds`.\n",
      "        hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n",
      "            Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "            trust-ncg, trust-krylov, trust-exact and trust-constr.\n",
      "            If it is callable, it should return the Hessian matrix::\n",
      "\n",
      "                hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)\n",
      "\n",
      "            where ``x`` is a (n,) ndarray and ``args`` is a tuple with the fixed\n",
      "            parameters.\n",
      "            The keywords {'2-point', '3-point', 'cs'} can also be used to select\n",
      "            a finite difference scheme for numerical estimation of the hessian.\n",
      "            Alternatively, objects implementing the `HessianUpdateStrategy`\n",
      "            interface can be used to approximate the Hessian. Available\n",
      "            quasi-Newton methods implementing this interface are:\n",
      "\n",
      "            - `BFGS`\n",
      "            - `SR1`\n",
      "\n",
      "            Not all of the options are available for each of the methods; for\n",
      "            availability refer to the notes.\n",
      "        hessp : callable, optional\n",
      "            Hessian of objective function times an arbitrary vector p. Only for\n",
      "            Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "            Only one of `hessp` or `hess` needs to be given. If `hess` is\n",
      "            provided, then `hessp` will be ignored. `hessp` must compute the\n",
      "            Hessian times an arbitrary vector::\n",
      "\n",
      "                hessp(x, p, *args) ->  ndarray shape (n,)\n",
      "\n",
      "            where ``x`` is a (n,) ndarray, ``p`` is an arbitrary vector with\n",
      "            dimension (n,) and ``args`` is a tuple with the fixed\n",
      "            parameters.\n",
      "        bounds : sequence or `Bounds`, optional\n",
      "            Bounds on variables for Nelder-Mead, L-BFGS-B, TNC, SLSQP, Powell,\n",
      "            trust-constr, COBYLA, and COBYQA methods. There are two ways to specify\n",
      "            the bounds:\n",
      "\n",
      "            1. Instance of `Bounds` class.\n",
      "            2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "               is used to specify no bound.\n",
      "\n",
      "        constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "            Constraints definition. Only for COBYLA, COBYQA, SLSQP and trust-constr.\n",
      "\n",
      "            Constraints for 'trust-constr', 'cobyqa', and 'cobyla' are defined as a single\n",
      "            object or a list of objects specifying constraints to the optimization problem.\n",
      "            Available constraints are:\n",
      "\n",
      "            - `LinearConstraint`\n",
      "            - `NonlinearConstraint`\n",
      "\n",
      "            Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "            Each dictionary with fields:\n",
      "\n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "\n",
      "            Equality constraint means that the constraint function result is to\n",
      "            be zero whereas inequality means that it is to be non-negative.\n",
      "\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. When `tol` is specified, the selected\n",
      "            minimization algorithm sets some relevant solver-specific tolerance(s)\n",
      "            equal to `tol`. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods except `TNC` accept the\n",
      "            following generic options:\n",
      "\n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform. Depending on the\n",
      "                method each iteration may use several function evaluations.\n",
      "\n",
      "                For `TNC` use `maxfun` instead of `maxiter`.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "\n",
      "            For method-specific options, see :func:`show_options()`.\n",
      "        callback : callable, optional\n",
      "            A callable called after each iteration.\n",
      "\n",
      "            All methods except TNC and SLSQP support a callable with\n",
      "            the signature::\n",
      "\n",
      "                callback(intermediate_result: OptimizeResult)\n",
      "\n",
      "            where ``intermediate_result`` is a keyword parameter containing an\n",
      "            `OptimizeResult` with attributes ``x`` and ``fun``, the present values\n",
      "            of the parameter vector and objective function. Not all attributes of\n",
      "            `OptimizeResult` may be present. The name of the parameter must be\n",
      "            ``intermediate_result`` for the callback to be passed an `OptimizeResult`.\n",
      "            These methods will also terminate if the callback raises ``StopIteration``.\n",
      "\n",
      "            All methods except trust-constr (also) support a signature like::\n",
      "\n",
      "                callback(xk)\n",
      "\n",
      "            where ``xk`` is the current parameter vector.\n",
      "\n",
      "            Introspection is used to determine which of the signatures above to\n",
      "            invoke.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar : Interface to minimization algorithms for scalar\n",
      "            univariate functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *BFGS*.\n",
      "\n",
      "        **Unconstrained minimization**\n",
      "\n",
      "        Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "        gradient algorithm by Polak and Ribiere, a variant of the\n",
      "        Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n",
      "        first derivatives are used.\n",
      "\n",
      "        Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "        method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "        pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "        performance even for non-smooth optimizations. This method also\n",
      "        returns an approximation of the Hessian inverse, stored as\n",
      "        `hess_inv` in the OptimizeResult object.\n",
      "\n",
      "        Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "        Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "        Newton method). It uses a CG method to the compute the search\n",
      "        direction. See also *TNC* method for a box-constrained\n",
      "        minimization with a similar algorithm. Suitable for large-scale\n",
      "        problems.\n",
      "\n",
      "        Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "        trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "        algorithm requires the gradient and Hessian; furthermore the\n",
      "        Hessian is required to be positive definite.\n",
      "\n",
      "        Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "        Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "        unconstrained minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector. Suitable for large-scale problems.\n",
      "\n",
      "        Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "        the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "        minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector. Suitable for large-scale problems.\n",
      "        On indefinite problems it requires usually less iterations than the\n",
      "        `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "\n",
      "        Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "        is a trust-region method for unconstrained minimization in which\n",
      "        quadratic subproblems are solved almost exactly [13]_. This\n",
      "        algorithm requires the gradient and the Hessian (which is\n",
      "        *not* required to be positive definite). It is, in many\n",
      "        situations, the Newton method to converge in fewer iterations\n",
      "        and the most recommended for small and medium-size problems.\n",
      "\n",
      "        **Bound-Constrained minimization**\n",
      "\n",
      "        Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "        Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "        applications. However, if numerical computation of derivative can be\n",
      "        trusted, other algorithms using the first and/or second derivatives\n",
      "        information might be preferred for their better performance in\n",
      "        general.\n",
      "\n",
      "        Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "        algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "\n",
      "        Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "        of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "        method. It performs sequential one-dimensional minimizations along\n",
      "        each vector of the directions set (`direc` field in `options` and\n",
      "        `info`), which is updated at each iteration of the main\n",
      "        minimization loop. The function need not be differentiable, and no\n",
      "        derivatives are taken. If bounds are not provided, then an\n",
      "        unbounded line search will be used. If bounds are provided and\n",
      "        the initial guess is within the bounds, then every function\n",
      "        evaluation throughout the minimization procedure will be within\n",
      "        the bounds. If bounds are provided, the initial guess is outside\n",
      "        the bounds, and `direc` is full rank (default has full rank), then\n",
      "        some function evaluations during the first iteration may be\n",
      "        outside the bounds, but every function evaluation after the first\n",
      "        iteration will be within the bounds. If `direc` is not full rank,\n",
      "        then some parameters may not be optimized and the solution is not\n",
      "        guaranteed to be within the bounds.\n",
      "\n",
      "        Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "        algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "        to bounds. This algorithm uses gradient information; it is also\n",
      "        called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "        method described above as it wraps a C implementation and allows\n",
      "        each variable to be given upper and lower bounds.\n",
      "\n",
      "        **Constrained Minimization**\n",
      "\n",
      "        Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the PRIMA\n",
      "        implementation [19]_ of the\n",
      "        Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "        [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "        approximations to the objective function and each constraint.\n",
      "\n",
      "        Method :ref:`COBYQA <optimize.minimize-cobyqa>` uses the Constrained\n",
      "        Optimization BY Quadratic Approximations (COBYQA) method [18]_. The\n",
      "        algorithm is a derivative-free trust-region SQP method based on quadratic\n",
      "        approximations to the objective function and each nonlinear constraint. The\n",
      "        bounds are treated as unrelaxable constraints, in the sense that the\n",
      "        algorithm always respects them throughout the optimization process.\n",
      "\n",
      "        Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "        Least SQuares Programming to minimize a function of several\n",
      "        variables with any combination of bounds, equality and inequality\n",
      "        constraints. The method wraps the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft [12]_. Note that the\n",
      "        wrapper handles infinite values in bounds by converting them into\n",
      "        large floating values.\n",
      "\n",
      "        Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "        trust-region algorithm for constrained optimization. It switches\n",
      "        between two implementations depending on the problem definition.\n",
      "        It is the most versatile constrained minimization algorithm\n",
      "        implemented in SciPy and the most appropriate for large-scale problems.\n",
      "        For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "        Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "        inequality constraints are imposed as well, it switches to the trust-region\n",
      "        interior point method described in [16]_. This interior point algorithm,\n",
      "        in turn, solves inequality constraints by introducing slack variables\n",
      "        and solving a sequence of equality-constrained barrier problems\n",
      "        for progressively smaller values of the barrier parameter.\n",
      "        The previously described equality constrained SQP method is\n",
      "        used to solve the subproblems with increasing levels of accuracy\n",
      "        as the iterate gets closer to a solution.\n",
      "\n",
      "        **Finite-Difference Options**\n",
      "\n",
      "        For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "        the gradient and the Hessian may be approximated using\n",
      "        three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "        The scheme 'cs' is, potentially, the most accurate but it\n",
      "        requires the function to correctly handle complex inputs and to\n",
      "        be differentiable in the complex plane. The scheme '3-point' is more\n",
      "        accurate than '2-point' but requires twice as many operations. If the\n",
      "        gradient is estimated via finite-differences the Hessian must be\n",
      "        estimated using one of the quasi-Newton strategies.\n",
      "\n",
      "        **Method specific options for the** `hess` **keyword**\n",
      "\n",
      "        +--------------+------+----------+-------------------------+-----+\n",
      "        | method/Hess  | None | callable | '2-point/'3-point'/'cs' | HUS |\n",
      "        +==============+======+==========+=========================+=====+\n",
      "        | Newton-CG    | x    | (n, n)   | x                       | x   |\n",
      "        |              |      | LO       |                         |     |\n",
      "        +--------------+------+----------+-------------------------+-----+\n",
      "        | dogleg       |      | (n, n)   |                         |     |\n",
      "        +--------------+------+----------+-------------------------+-----+\n",
      "        | trust-ncg    |      | (n, n)   | x                       | x   |\n",
      "        +--------------+------+----------+-------------------------+-----+\n",
      "        | trust-krylov |      | (n, n)   | x                       | x   |\n",
      "        +--------------+------+----------+-------------------------+-----+\n",
      "        | trust-exact  |      | (n, n)   |                         |     |\n",
      "        +--------------+------+----------+-------------------------+-----+\n",
      "        | trust-constr | x    | (n, n)   |  x                      | x   |\n",
      "        |              |      | LO       |                         |     |\n",
      "        |              |      | sp       |                         |     |\n",
      "        +--------------+------+----------+-------------------------+-----+\n",
      "\n",
      "        where LO=LinearOperator, sp=Sparse matrix, HUS=HessianUpdateStrategy\n",
      "\n",
      "        **Custom minimizers**\n",
      "\n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "        or a different library.  You can simply pass a callable as the ``method``\n",
      "        parameter.\n",
      "\n",
      "        The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "        `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "        `fun` returns just the function values and `jac` is converted to a function\n",
      "        returning the Jacobian.  The method shall return an `OptimizeResult`\n",
      "        object.\n",
      "\n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method.  You can find an example in the scipy.optimize tutorial.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "            Minimization. The Computer Journal 7: 308-13.\n",
      "        .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "            respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "            Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "            Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "            191-208.\n",
      "        .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "           a function of several variables without calculating derivatives. The\n",
      "           Computer Journal 7: 155-162.\n",
      "        .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "           Numerical Recipes (any edition), Cambridge University Press.\n",
      "        .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "           Springer New York.\n",
      "        .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "           Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "           Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "        .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "           778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "           optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "           550-560.\n",
      "        .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "           1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "        .. [9] Powell, M J D. A direct search optimization method that models\n",
      "           the objective and constraint functions by linear interpolation.\n",
      "           1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "           and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "        .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "           calculations. 1998. Acta Numerica 7: 287-336.\n",
      "        .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "           derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "           2007/NA03\n",
      "        .. [12] Kraft, D. A software package for sequential quadratic\n",
      "           programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "           Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "        .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "           Trust region methods. 2000. Siam. pp. 169-200.\n",
      "        .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "           implementation of the GLTR method for iterative solution of\n",
      "           the trust region problem\", :arxiv:`1611.04718`\n",
      "        .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "           Trust-Region Subproblem using the Lanczos Method\",\n",
      "           SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "        .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "            An interior point algorithm for large-scale nonlinear  programming.\n",
      "            SIAM Journal on Optimization 9.4: 877-900.\n",
      "        .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. 1998. On the\n",
      "            implementation of an algorithm for large-scale equality constrained\n",
      "            optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "        .. [18] Ragonneau, T. M. *Model-Based Derivative-Free Optimization Methods\n",
      "            and Software*. PhD thesis, Department of Applied Mathematics, The Hong\n",
      "            Kong Polytechnic University, Hong Kong, China, 2022. URL:\n",
      "            https://theses.lib.polyu.edu.hk/handle/200/12294.\n",
      "        .. [19] Zhang, Z. \"PRIMA: Reference Implementation for Powell's Methods with\n",
      "            Modernization and Amelioration\", https://www.libprima.net,\n",
      "            :doi:`10.5281/zenodo.8052654`\n",
      "        .. [20] Karush-Kuhn-Tucker conditions,\n",
      "            https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function (and its respective derivatives) is implemented in `rosen`\n",
      "        (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "\n",
      "        >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "\n",
      "        A simple application of the *Nelder-Mead* method is:\n",
      "\n",
      "        >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "        >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "\n",
      "        Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "        options:\n",
      "\n",
      "        >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "        ...                options={'gtol': 1e-6, 'disp': True})\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 26\n",
      "                 Function evaluations: 31\n",
      "                 Gradient evaluations: 31\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        >>> print(res.message)\n",
      "        Optimization terminated successfully.\n",
      "        >>> res.hess_inv\n",
      "        array([\n",
      "            [ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "            [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "            [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "            [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "            [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]\n",
      "        ])\n",
      "\n",
      "        Next, consider a minimization problem with several constraints (namely\n",
      "        Example 16.4 from [5]_). The objective function is:\n",
      "\n",
      "        >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "\n",
      "        There are three constraints defined as:\n",
      "\n",
      "        >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "\n",
      "        And variables must be positive, hence the following bounds:\n",
      "\n",
      "        >>> bnds = ((0, None), (0, None))\n",
      "\n",
      "        The optimization problem is solved using the SLSQP method as:\n",
      "\n",
      "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds, constraints=cons)\n",
      "\n",
      "        It should converge to the theoretical solution ``[1.4 ,1.7]``. *SLSQP* also\n",
      "        returns the multipliers that are used in the solution of the problem. These\n",
      "        multipliers, when the problem constraints are linear, can be thought of as the\n",
      "        Karush-Kuhn-Tucker (KKT) multipliers, which are a generalization\n",
      "        of Lagrange multipliers to inequality-constrained optimization problems ([20]_).\n",
      "\n",
      "        Notice that at the solution, the first constraint is active. Let's evaluate the\n",
      "        function at solution:\n",
      "\n",
      "        >>> cons[0]['fun'](res.x)\n",
      "        np.float64(1.4901224698604665e-09)\n",
      "\n",
      "        Also, notice that at optimality there is a non-zero multiplier:\n",
      "\n",
      "        >>> res.multipliers\n",
      "        array([0.8, 0. , 0. ])\n",
      "\n",
      "        This can be understood as the local sensitivity of the optimal value of the\n",
      "        objective function with respect to changes in the first constraint. If we\n",
      "        tighten the constraint by a small amount ``eps``:\n",
      "\n",
      "        >>> eps = 0.01\n",
      "        >>> cons[0]['fun'] = lambda x: x[0] - 2 * x[1] + 2 - eps\n",
      "\n",
      "        we expect the optimal value of the objective function to increase by\n",
      "        approximately ``eps * res.multipliers[0]``:\n",
      "\n",
      "        >>> eps * res.multipliers[0]  # Expected change in f0\n",
      "        np.float64(0.008000000027153205)\n",
      "        >>> f0 = res.fun  # Keep track of the previous optimal value\n",
      "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds, constraints=cons)\n",
      "        >>> f1 = res.fun  # New optimal value\n",
      "        >>> f1 - f0\n",
      "        np.float64(0.008019998807885509)\n",
      "\n",
      "    minimize_scalar(fun, bracket=None, bounds=None, args=(), method=None, tol=None, options=None)\n",
      "        Local minimization of scalar function of one variable.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Objective function.\n",
      "            Scalar function, must return a scalar.\n",
      "\n",
      "            Suppose the callable has signature ``f0(x, *my_args, **my_kwargs)``, where\n",
      "            ``my_args`` and ``my_kwargs`` are required positional and keyword arguments.\n",
      "            Rather than passing ``f0`` as the callable, wrap it to accept\n",
      "            only ``x``; e.g., pass ``fun=lambda x: f0(x, *my_args, **my_kwargs)`` as the\n",
      "            callable, where ``my_args`` (tuple) and ``my_kwargs`` (dict) have been\n",
      "            gathered before invoking this function.\n",
      "\n",
      "        bracket : sequence, optional\n",
      "            For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
      "            interval and is required.\n",
      "            Either a triple ``(xa, xb, xc)`` satisfying ``xa < xb < xc`` and\n",
      "            ``func(xb) < func(xa) and  func(xb) < func(xc)``, or a pair\n",
      "            ``(xa, xb)`` to be used as initial points for a downhill bracket search\n",
      "            (see `scipy.optimize.bracket`).\n",
      "            The minimizer ``res.x`` will not necessarily satisfy\n",
      "            ``xa <= res.x <= xb``.\n",
      "        bounds : sequence, optional\n",
      "            For method 'bounded', `bounds` is mandatory and must have two finite\n",
      "            items corresponding to the optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function.\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of:\n",
      "\n",
      "            - :ref:`Brent <optimize.minimize_scalar-brent>`\n",
      "            - :ref:`Bounded <optimize.minimize_scalar-bounded>`\n",
      "            - :ref:`Golden <optimize.minimize_scalar-golden>`\n",
      "            - custom - a callable object (added in version 0.14.0), see below\n",
      "\n",
      "            Default is \"Bounded\" if bounds are provided and \"Brent\" otherwise.\n",
      "            See the 'Notes' section for details of each solver.\n",
      "\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options.\n",
      "\n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "\n",
      "            See :func:`show_options()` for solver-specific options.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        minimize : Interface to minimization algorithms for scalar multivariate\n",
      "            functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is the ``\"Bounded\"`` Brent method if\n",
      "        `bounds` are passed and unbounded ``\"Brent\"`` otherwise.\n",
      "\n",
      "        Method :ref:`Brent <optimize.minimize_scalar-brent>` uses Brent's\n",
      "        algorithm [1]_ to find a local minimum.  The algorithm uses inverse\n",
      "        parabolic interpolation when possible to speed up convergence of\n",
      "        the golden section method.\n",
      "\n",
      "        Method :ref:`Golden <optimize.minimize_scalar-golden>` uses the\n",
      "        golden section search technique [1]_. It uses analog of the bisection\n",
      "        method to decrease the bracketed interval. It is usually\n",
      "        preferable to use the *Brent* method.\n",
      "\n",
      "        Method :ref:`Bounded <optimize.minimize_scalar-bounded>` can\n",
      "        perform bounded minimization [2]_ [3]_. It uses the Brent method to find a\n",
      "        local minimum in the interval x1 < xopt < x2.\n",
      "\n",
      "        Note that the Brent and Golden methods do not guarantee success unless a\n",
      "        valid ``bracket`` triple is provided. If a three-point bracket cannot be\n",
      "        found, consider `scipy.optimize.minimize`. Also, all methods are intended\n",
      "        only for local minimization. When the function of interest has more than\n",
      "        one local minimum, consider :ref:`global_optimization`.\n",
      "\n",
      "        **Custom minimizers**\n",
      "\n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using some library frontend to minimize_scalar. You can simply\n",
      "        pass a callable as the ``method`` parameter.\n",
      "\n",
      "        The callable is called as ``method(fun, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `bracket`, `tol`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  The method\n",
      "        shall return an `OptimizeResult` object.\n",
      "\n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method. You can find an example in the scipy.optimize tutorial.\n",
      "\n",
      "        .. versionadded:: 0.11.0\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Press, W., S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery.\n",
      "               Numerical Recipes in C. Cambridge University Press.\n",
      "        .. [2] Forsythe, G.E., M. A. Malcolm, and C. B. Moler. \"Computer Methods\n",
      "               for Mathematical Computations.\" Prentice-Hall Series in Automatic\n",
      "               Computation 259 (1977).\n",
      "        .. [3] Brent, Richard P. Algorithms for Minimization Without Derivatives.\n",
      "               Courier Corporation, 2013.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Consider the problem of minimizing the following function.\n",
      "\n",
      "        >>> def f(x):\n",
      "        ...     return (x - 2) * x * (x + 2)**2\n",
      "\n",
      "        Using the *Brent* method, we find the local minimum as:\n",
      "\n",
      "        >>> from scipy.optimize import minimize_scalar\n",
      "        >>> res = minimize_scalar(f)\n",
      "        >>> res.fun\n",
      "        -9.9149495908\n",
      "\n",
      "        The minimizer is:\n",
      "\n",
      "        >>> res.x\n",
      "        1.28077640403\n",
      "\n",
      "        Using the *Bounded* method, we find a local minimum with specified\n",
      "        bounds as:\n",
      "\n",
      "        >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
      "        >>> res.fun  # minimum\n",
      "        3.28365179850e-13\n",
      "        >>> res.x  # minimizer\n",
      "        -2.0000002026\n",
      "\n",
      "    newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None, x1=None, rtol=0.0, full_output=False, disp=True)\n",
      "        Find a root of a real or complex function using the Newton-Raphson\n",
      "        (or secant or Halley's) method.\n",
      "\n",
      "        Find a root of the scalar-valued function `func` given a nearby scalar\n",
      "        starting point `x0`.\n",
      "        The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
      "        is provided, otherwise the secant method is used. If the second order\n",
      "        derivative `fprime2` of `func` is also provided, then Halley's method is\n",
      "        used.\n",
      "\n",
      "        If `x0` is a sequence with more than one item, `newton` returns an array:\n",
      "        the roots of the function from each (scalar) starting point in `x0`.\n",
      "        In this case, `func` must be vectorized to return a sequence or array of\n",
      "        the same shape as its first argument. If `fprime` (`fprime2`) is given,\n",
      "        then its return must also have the same shape: each element is the first\n",
      "        (second) derivative of `func` with respect to its only variable evaluated\n",
      "        at each element of its first argument.\n",
      "\n",
      "        `newton` is for finding roots of a scalar-valued functions of a single\n",
      "        variable. For problems involving several variables, see `root`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The function whose root is wanted. It must be a function of a\n",
      "            single variable of the form ``f(x,a,b,c...)``, where ``a,b,c...``\n",
      "            are extra arguments that can be passed in the `args` parameter.\n",
      "        x0 : float, sequence, or ndarray\n",
      "            An initial estimate of the root that should be somewhere near the\n",
      "            actual root. If not scalar, then `func` must be vectorized and return\n",
      "            a sequence or array of the same shape as its first argument.\n",
      "        fprime : callable, optional\n",
      "            The derivative of the function when available and convenient. If it\n",
      "            is None (default), then the secant method is used.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to be used in the function call.\n",
      "        tol : float, optional\n",
      "            The allowable error of the root's value. If `func` is complex-valued,\n",
      "            a larger `tol` is recommended as both the real and imaginary parts\n",
      "            of `x` contribute to ``|x - x0|``.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        fprime2 : callable, optional\n",
      "            The second order derivative of the function when available and\n",
      "            convenient. If it is None (default), then the normal Newton-Raphson\n",
      "            or the secant method is used. If it is not None, then Halley's method\n",
      "            is used.\n",
      "        x1 : float, optional\n",
      "            Another estimate of the root that should be somewhere near the\n",
      "            actual root. Used if `fprime` is not provided.\n",
      "        rtol : float, optional\n",
      "            Tolerance (relative) for termination.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False (default), the root is returned.\n",
      "            If True and `x0` is scalar, the return value is ``(x, r)``, where ``x``\n",
      "            is the root and ``r`` is a `RootResults` object.\n",
      "            If True and `x0` is non-scalar, the return value is ``(x, converged,\n",
      "            zero_der)`` (see Returns section for details).\n",
      "        disp : bool, optional\n",
      "            If True, raise a RuntimeError if the algorithm didn't converge, with\n",
      "            the error message containing the number of iterations and current\n",
      "            function value. Otherwise, the convergence status is recorded in a\n",
      "            `RootResults` return object.\n",
      "            Ignored if `x0` is not scalar.\n",
      "            *Note: this has little to do with displaying, however,\n",
      "            the `disp` keyword cannot be renamed for backwards compatibility.*\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        root : float, sequence, or ndarray\n",
      "            Estimated location where function is zero.\n",
      "        r : `RootResults`, optional\n",
      "            Present if ``full_output=True`` and `x0` is scalar.\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        converged : ndarray of bool, optional\n",
      "            Present if ``full_output=True`` and `x0` is non-scalar.\n",
      "            For vector functions, indicates which elements converged successfully.\n",
      "        zero_der : ndarray of bool, optional\n",
      "            Present if ``full_output=True`` and `x0` is non-scalar.\n",
      "            For vector functions, indicates which elements had a zero derivative.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        root_scalar : interface to root solvers for scalar functions\n",
      "        root : interface to root solvers for multi-input, multi-output functions\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The convergence rate of the Newton-Raphson method is quadratic,\n",
      "        the Halley method is cubic, and the secant method is\n",
      "        sub-quadratic. This means that if the function is well-behaved\n",
      "        the actual error in the estimated root after the nth iteration\n",
      "        is approximately the square (cube for Halley) of the error\n",
      "        after the (n-1)th step. However, the stopping criterion used\n",
      "        here is the step size and there is no guarantee that a root\n",
      "        has been found. Consequently, the result should be verified.\n",
      "        Safer algorithms are brentq, brenth, ridder, and bisect,\n",
      "        but they all require that the root first be bracketed in an\n",
      "        interval where the function changes sign. The brentq algorithm\n",
      "        is recommended for general use in one dimensional problems\n",
      "        when such an interval has been found.\n",
      "\n",
      "        When `newton` is used with arrays, it is best suited for the following\n",
      "        types of problems:\n",
      "\n",
      "        * The initial guesses, `x0`, are all relatively the same distance from\n",
      "          the roots.\n",
      "        * Some or all of the extra arguments, `args`, are also arrays so that a\n",
      "          class of similar problems can be solved together.\n",
      "        * The size of the initial guesses, `x0`, is larger than O(100) elements.\n",
      "          Otherwise, a naive loop may perform as well or better than a vector.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy import optimize\n",
      "\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "\n",
      "        ``fprime`` is not provided, use the secant method:\n",
      "\n",
      "        >>> root = optimize.newton(f, 1.5)\n",
      "        >>> root\n",
      "        1.0000000000000016\n",
      "        >>> root = optimize.newton(f, 1.5, fprime2=lambda x: 6 * x)\n",
      "        >>> root\n",
      "        1.0000000000000016\n",
      "\n",
      "        Only ``fprime`` is provided, use the Newton-Raphson method:\n",
      "\n",
      "        >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2)\n",
      "        >>> root\n",
      "        1.0\n",
      "\n",
      "        Both ``fprime2`` and ``fprime`` are provided, use Halley's method:\n",
      "\n",
      "        >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2,\n",
      "        ...                        fprime2=lambda x: 6 * x)\n",
      "        >>> root\n",
      "        1.0\n",
      "\n",
      "        When we want to find roots for a set of related starting values and/or\n",
      "        function parameters, we can provide both of those as an array of inputs:\n",
      "\n",
      "        >>> f = lambda x, a: x**3 - a\n",
      "        >>> fder = lambda x, a: 3 * x**2\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x = rng.standard_normal(100)\n",
      "        >>> a = np.arange(-50, 50)\n",
      "        >>> vec_res = optimize.newton(f, x, fprime=fder, args=(a, ), maxiter=200)\n",
      "\n",
      "        The above is the equivalent of solving for each value in ``(x, a)``\n",
      "        separately in a for-loop, just faster:\n",
      "\n",
      "        >>> loop_res = [optimize.newton(f, x0, fprime=fder, args=(a0,),\n",
      "        ...                             maxiter=200)\n",
      "        ...             for x0, a0 in zip(x, a)]\n",
      "        >>> np.allclose(vec_res, loop_res)\n",
      "        True\n",
      "\n",
      "        Plot the results found for all values of ``a``:\n",
      "\n",
      "        >>> analytical_result = np.sign(a) * np.abs(a)**(1/3)\n",
      "        >>> fig, ax = plt.subplots()\n",
      "        >>> ax.plot(a, analytical_result, 'o')\n",
      "        >>> ax.plot(a, vec_res, '.')\n",
      "        >>> ax.set_xlabel('$a$')\n",
      "        >>> ax.set_ylabel('$x$ where $f(x, a)=0$')\n",
      "        >>> plt.show()\n",
      "\n",
      "    newton_krylov(F, xin, iter=None, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
      "\n",
      "        This method is suitable for solving large-scale problems.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        rdiff : float, optional\n",
      "            Relative step size to use in numerical differentiation.\n",
      "        method : str or callable, optional\n",
      "            Krylov method to use to approximate the Jacobian.  Can be a string,\n",
      "            or a function implementing the same interface as the iterative\n",
      "            solvers in `scipy.sparse.linalg`. If a string, needs to be one of:\n",
      "            ``'lgmres'``, ``'gmres'``, ``'bicgstab'``, ``'cgs'``, ``'minres'``,\n",
      "            ``'tfqmr'``.\n",
      "\n",
      "            The default is `scipy.sparse.linalg.lgmres`.\n",
      "        inner_maxiter : int, optional\n",
      "            Parameter to pass to the \"inner\" Krylov solver: maximum number of\n",
      "            iterations. Iteration will stop after maxiter steps even if the\n",
      "            specified tolerance has not been achieved.\n",
      "        inner_M : LinearOperator or InverseJacobian\n",
      "            Preconditioner for the inner Krylov iteration.\n",
      "            Note that you can use also inverse Jacobians as (adaptive)\n",
      "            preconditioners. For example,\n",
      "\n",
      "            >>> from scipy.optimize import BroydenFirst, KrylovJacobian\n",
      "            >>> from scipy.optimize import InverseJacobian\n",
      "            >>> jac = BroydenFirst()\n",
      "            >>> kjac = KrylovJacobian(inner_M=InverseJacobian(jac))\n",
      "\n",
      "            If the preconditioner has a method named 'update', it will be called\n",
      "            as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
      "            the current point, and ``f`` the current function value.\n",
      "        outer_k : int, optional\n",
      "            Size of the subspace kept across LGMRES nonlinear iterations.\n",
      "            See `scipy.sparse.linalg.lgmres` for details.\n",
      "        inner_kwargs : kwargs\n",
      "            Keyword parameters for the \"inner\" Krylov solver\n",
      "            (defined with `method`). Parameter names must start with\n",
      "            the `inner_` prefix which will be stripped before passing on\n",
      "            the inner method. See, e.g., `scipy.sparse.linalg.gmres` for details.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method='krylov'`` in particular.\n",
      "        scipy.sparse.linalg.gmres\n",
      "        scipy.sparse.linalg.lgmres\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This function implements a Newton-Krylov solver. The basic idea is\n",
      "        to compute the inverse of the Jacobian with an iterative Krylov\n",
      "        method. These methods require only evaluating the Jacobian-vector\n",
      "        products, which are conveniently approximated by a finite difference:\n",
      "\n",
      "        .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
      "\n",
      "        Due to the use of iterative matrix inverses, these methods can\n",
      "        deal with large nonlinear problems.\n",
      "\n",
      "        SciPy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
      "        solvers to choose from. The default here is `lgmres`, which is a\n",
      "        variant of restarted GMRES iteration that reuses some of the\n",
      "        information obtained in the previous Newton steps to invert\n",
      "        Jacobians in subsequent steps.\n",
      "\n",
      "        For a review on Newton-Krylov methods, see for example [1]_,\n",
      "        and for the LGMRES sparse inverse method, see [2]_.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] C. T. Kelley, Solving Nonlinear Equations with Newton's Method,\n",
      "               SIAM, pp.57-83, 2003.\n",
      "               :doi:`10.1137/1.9780898718898.ch3`\n",
      "        .. [2] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2004).\n",
      "               :doi:`10.1016/j.jcp.2003.08.010`\n",
      "        .. [3] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
      "               SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
      "               :doi:`10.1137/S0895479803422014`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "\n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0] + 0.5 * x[1] - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0]) ** 2]\n",
      "\n",
      "        A solution can be obtained as follows.\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.newton_krylov(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.66731771, 0.66536458])\n",
      "\n",
      "    nnls(A, b, *, maxiter=None, atol=<object object at 0x147f08e20>)\n",
      "        Solve ``argmin_x || Ax - b ||_2`` for ``x>=0``.\n",
      "\n",
      "        This problem, often called as NonNegative Least Squares, is a convex\n",
      "        optimization problem with convex constraints. It typically arises when\n",
      "        the ``x`` models quantities for which only nonnegative values are\n",
      "        attainable; weight of ingredients, component costs and so on.\n",
      "\n",
      "        .. deprecated:: 1.18.0\n",
      "            Use of argument(s) ``{'maxiter'}`` by position is deprecated; beginning in\n",
      "            SciPy 1.18.0, these will be keyword-only. Argument(s) ``{'atol'}`` are deprecated, whether passed by position or keyword; they will be removed in SciPy 1.18.0.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        A : (m, n) ndarray\n",
      "            Coefficient array\n",
      "        b : (m,) ndarray, float\n",
      "            Right-hand side vector.\n",
      "        maxiter: int, optional\n",
      "            Maximum number of iterations, optional. Default value is ``3 * n``.\n",
      "        atol : float, optional\n",
      "            .. deprecated:: 1.18.0\n",
      "                This parameter is deprecated and will be removed in SciPy 1.18.0.\n",
      "                It is not used in the implementation.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Solution vector.\n",
      "        rnorm : float\n",
      "            The 2-norm of the residual, ``|| Ax-b ||_2``.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "\n",
      "        :func:`lsq_linear`\n",
      "            Linear least squares with bounds on the variables\n",
      "\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The code is based on the classical algorithm of [1]_. It utilizes an active\n",
      "        set method and solves the KKK (Karush-Kuhn-Tucker) conditions for the\n",
      "        non-negative least squares problem.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] : Lawson C., Hanson R.J., \"Solving Least Squares Problems\", SIAM,\n",
      "           1995, :doi:`10.1137/1.9781611971217`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import nnls\n",
      "        ...\n",
      "        >>> A = np.array([[1, 0], [1, 0], [0, 1]])\n",
      "        >>> b = np.array([2, 1, 1])\n",
      "        >>> nnls(A, b)\n",
      "        (array([1.5, 1. ]), 0.7071067811865475)\n",
      "\n",
      "        >>> b = np.array([-1, -1, -1])\n",
      "        >>> nnls(A, b)\n",
      "        (array([0., 0.]), 1.7320508075688772)\n",
      "\n",
      "    quadratic_assignment(A, B, method='faq', options=None)\n",
      "        Approximates solution to the quadratic assignment problem and\n",
      "        the graph matching problem.\n",
      "\n",
      "        Quadratic assignment solves problems of the following form:\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            \\min_P & \\ {\\ \\text{trace}(A^T P B P^T)}\\\\\n",
      "            \\mbox{s.t. } & {P \\ \\epsilon \\ \\mathcal{P}}\\\\\n",
      "\n",
      "        where :math:`\\mathcal{P}` is the set of all permutation matrices,\n",
      "        and :math:`A` and :math:`B` are square matrices.\n",
      "\n",
      "        Graph matching tries to *maximize* the same objective function.\n",
      "        This algorithm can be thought of as finding the alignment of the\n",
      "        nodes of two graphs that minimizes the number of induced edge\n",
      "        disagreements, or, in the case of weighted graphs, the sum of squared\n",
      "        edge weight differences.\n",
      "\n",
      "        Note that the quadratic assignment problem is NP-hard. The results given\n",
      "        here are approximations and are not guaranteed to be optimal.\n",
      "\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        A : 2-D array, square\n",
      "            The square matrix :math:`A` in the objective function above.\n",
      "\n",
      "        B : 2-D array, square\n",
      "            The square matrix :math:`B` in the objective function above.\n",
      "\n",
      "        method :  str in {'faq', '2opt'} (default: 'faq')\n",
      "            The algorithm used to solve the problem.\n",
      "            :ref:`'faq' <optimize.qap-faq>` (default) and\n",
      "            :ref:`'2opt' <optimize.qap-2opt>` are available.\n",
      "\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All solvers support the following:\n",
      "\n",
      "            maximize : bool (default: False)\n",
      "                Maximizes the objective function if ``True``.\n",
      "\n",
      "            partial_match : 2-D array of integers, optional (default: None)\n",
      "                Fixes part of the matching. Also known as a \"seed\" [2]_.\n",
      "\n",
      "                Each row of `partial_match` specifies a pair of matched nodes:\n",
      "                node ``partial_match[i, 0]`` of `A` is matched to node\n",
      "                ``partial_match[i, 1]`` of `B`. The array has shape ``(m, 2)``,\n",
      "                where ``m`` is not greater than the number of nodes, :math:`n`.\n",
      "\n",
      "            rng : `numpy.random.Generator`, optional\n",
      "                Pseudorandom number generator state. When `rng` is None, a new\n",
      "                `numpy.random.Generator` is created using entropy from the\n",
      "                operating system. Types other than `numpy.random.Generator` are\n",
      "                passed to `numpy.random.default_rng` to instantiate a ``Generator``.\n",
      "\n",
      "                .. versionchanged:: 1.15.0\n",
      "                    As part of the `SPEC-007 <https://scientific-python.org/specs/spec-0007/>`_\n",
      "                    transition from use of `numpy.random.RandomState` to\n",
      "                    `numpy.random.Generator` is occurring. Supplying\n",
      "                    `np.random.RandomState` to this function will now emit a\n",
      "                    `DeprecationWarning`. In SciPy 1.17 its use will raise an exception.\n",
      "                    In addition relying on global state using `np.random.seed`\n",
      "                    will emit a `FutureWarning`. In SciPy 1.17 the global random number\n",
      "                    generator will no longer be used.\n",
      "                    Use of an int-like seed will raise a `FutureWarning`, in SciPy 1.17 it\n",
      "                    will be normalized via `np.random.default_rng` rather than\n",
      "                    `np.random.RandomState`.\n",
      "\n",
      "            For method-specific options, see\n",
      "            :func:`show_options('quadratic_assignment') <show_options>`.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            `OptimizeResult` containing the following fields.\n",
      "\n",
      "            col_ind : 1-D array\n",
      "                Column indices corresponding to the best permutation found of the\n",
      "                nodes of `B`.\n",
      "            fun : float\n",
      "                The objective value of the solution.\n",
      "            nit : int\n",
      "                The number of iterations performed during optimization.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The default method :ref:`'faq' <optimize.qap-faq>` uses the Fast\n",
      "        Approximate QAP algorithm [1]_; it typically offers the best combination of\n",
      "        speed and accuracy.\n",
      "        Method :ref:`'2opt' <optimize.qap-2opt>` can be computationally expensive,\n",
      "        but may be a useful alternative, or it can be used to refine the solution\n",
      "        returned by another method.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J.T. Vogelstein, J.M. Conroy, V. Lyzinski, L.J. Podrazik,\n",
      "               S.G. Kratzer, E.T. Harley, D.E. Fishkind, R.J. Vogelstein, and\n",
      "               C.E. Priebe, \"Fast approximate quadratic programming for graph\n",
      "               matching,\" PLOS one, vol. 10, no. 4, p. e0121002, 2015,\n",
      "               :doi:`10.1371/journal.pone.0121002`\n",
      "\n",
      "        .. [2] D. Fishkind, S. Adali, H. Patsolic, L. Meng, D. Singh, V. Lyzinski,\n",
      "               C. Priebe, \"Seeded graph matching\", Pattern Recognit. 87 (2019):\n",
      "               203-215, :doi:`10.1016/j.patcog.2018.09.014`\n",
      "\n",
      "        .. [3] \"2-opt,\" Wikipedia.\n",
      "               https://en.wikipedia.org/wiki/2-opt\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import quadratic_assignment\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> A = np.array([[0, 80, 150, 170], [80, 0, 130, 100],\n",
      "        ...               [150, 130, 0, 120], [170, 100, 120, 0]])\n",
      "        >>> B = np.array([[0, 5, 2, 7], [0, 0, 3, 8],\n",
      "        ...               [0, 0, 0, 3], [0, 0, 0, 0]])\n",
      "        >>> res = quadratic_assignment(A, B, options={'rng': rng})\n",
      "        >>> print(res)\n",
      "             fun: 3260\n",
      "         col_ind: [0 3 2 1]\n",
      "             nit: 9\n",
      "\n",
      "        The see the relationship between the returned ``col_ind`` and ``fun``,\n",
      "        use ``col_ind`` to form the best permutation matrix found, then evaluate\n",
      "        the objective function :math:`f(P) = trace(A^T P B P^T )`.\n",
      "\n",
      "        >>> perm = res['col_ind']\n",
      "        >>> P = np.eye(len(A), dtype=int)[perm]\n",
      "        >>> fun = np.trace(A.T @ P @ B @ P.T)\n",
      "        >>> print(fun)\n",
      "        3260\n",
      "\n",
      "        Alternatively, to avoid constructing the permutation matrix explicitly,\n",
      "        directly permute the rows and columns of the distance matrix.\n",
      "\n",
      "        >>> fun = np.trace(A.T @ B[perm][:, perm])\n",
      "        >>> print(fun)\n",
      "        3260\n",
      "\n",
      "        Although not guaranteed in general, ``quadratic_assignment`` happens to\n",
      "        have found the globally optimal solution.\n",
      "\n",
      "        >>> from itertools import permutations\n",
      "        >>> perm_opt, fun_opt = None, np.inf\n",
      "        >>> for perm in permutations([0, 1, 2, 3]):\n",
      "        ...     perm = np.array(perm)\n",
      "        ...     fun = np.trace(A.T @ B[perm][:, perm])\n",
      "        ...     if fun < fun_opt:\n",
      "        ...         fun_opt, perm_opt = fun, perm\n",
      "        >>> print(np.array_equal(perm_opt, res['col_ind']))\n",
      "        True\n",
      "\n",
      "        Here is an example for which the default method,\n",
      "        :ref:`'faq' <optimize.qap-faq>`, does not find the global optimum.\n",
      "\n",
      "        >>> A = np.array([[0, 5, 8, 6], [5, 0, 5, 1],\n",
      "        ...               [8, 5, 0, 2], [6, 1, 2, 0]])\n",
      "        >>> B = np.array([[0, 1, 8, 4], [1, 0, 5, 2],\n",
      "        ...               [8, 5, 0, 5], [4, 2, 5, 0]])\n",
      "        >>> res = quadratic_assignment(A, B, options={'rng': rng})\n",
      "        >>> print(res)\n",
      "             fun: 178\n",
      "         col_ind: [1 0 3 2]\n",
      "             nit: 13\n",
      "\n",
      "        If accuracy is important, consider using  :ref:`'2opt' <optimize.qap-2opt>`\n",
      "        to refine the solution.\n",
      "\n",
      "        >>> guess = np.array([np.arange(len(A)), res.col_ind]).T\n",
      "        >>> res = quadratic_assignment(A, B, method=\"2opt\",\n",
      "        ...     options = {'rng': rng, 'partial_guess': guess})\n",
      "        >>> print(res)\n",
      "             fun: 176\n",
      "         col_ind: [1 2 3 0]\n",
      "             nit: 17\n",
      "\n",
      "    ridder(f, a, b, args=(), xtol=2e-12, rtol=np.float64(8.881784197001252e-16), maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in an interval using Ridder's method.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.isclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be positive.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.isclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        root : float\n",
      "            Root of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.\n",
      "            In particular, ``r.converged`` is True if the routine converged.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton : 1-D root-finding\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        elementwise.find_root : efficient elementwise 1-D root-finder\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Uses [Ridders1979]_ method to find a root of the function `f` between the\n",
      "        arguments `a` and `b`. Ridders' method is faster than bisection, but not\n",
      "        generally as fast as the Brent routines. [Ridders1979]_ provides the\n",
      "        classic description and source of the algorithm. A description can also be\n",
      "        found in any recent edition of Numerical Recipes.\n",
      "\n",
      "        The routine used here diverges slightly from standard presentations in\n",
      "        order to be a bit more careful of tolerance.\n",
      "\n",
      "        As mentioned in the parameter documentation, the computed root ``x0`` will\n",
      "        satisfy ``np.isclose(x, x0, atol=xtol, rtol=rtol)``, where ``x`` is the\n",
      "        exact root. In equation form, this terminating condition is ``abs(x - x0)\n",
      "        <= xtol + rtol * abs(x0)``.\n",
      "\n",
      "        The default value ``xtol=2e-12`` may lead to surprising behavior if one\n",
      "        expects `ridder` to always compute roots with relative error near machine\n",
      "        precision. Care should be taken to select `xtol` for the use case at hand.\n",
      "        Setting ``xtol=5e-324``, the smallest subnormal number, will ensure the\n",
      "        highest level of accuracy. Larger values of `xtol` may be useful for saving\n",
      "        function evaluations when a root is at or near zero in applications where\n",
      "        the tiny absolute differences available between floating point numbers near\n",
      "        zero are not meaningful.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [Ridders1979]\n",
      "           Ridders, C. F. J. \"A New Algorithm for Computing a\n",
      "           Single Root of a Real Continuous Function.\"\n",
      "           IEEE Trans. Circuits Systems 26, 979-980, 1979.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "\n",
      "        >>> root = optimize.ridder(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "\n",
      "        >>> root = optimize.ridder(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "\n",
      "    root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)\n",
      "        Find a root of a vector function.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            A vector function to find a root of.\n",
      "\n",
      "            Suppose the callable has signature ``f0(x, *my_args, **my_kwargs)``, where\n",
      "            ``my_args`` and ``my_kwargs`` are required positional and keyword arguments.\n",
      "            Rather than passing ``f0`` as the callable, wrap it to accept\n",
      "            only ``x``; e.g., pass ``fun=lambda x: f0(x, *my_args, **my_kwargs)`` as the\n",
      "            callable, where ``my_args`` (tuple) and ``my_kwargs`` (dict) have been\n",
      "            gathered before invoking this function.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its Jacobian.\n",
      "        method : str, optional\n",
      "            Type of solver. Should be one of\n",
      "\n",
      "            - 'hybr'             :ref:`(see here) <optimize.root-hybr>`\n",
      "            - 'lm'               :ref:`(see here) <optimize.root-lm>`\n",
      "            - 'broyden1'         :ref:`(see here) <optimize.root-broyden1>`\n",
      "            - 'broyden2'         :ref:`(see here) <optimize.root-broyden2>`\n",
      "            - 'anderson'         :ref:`(see here) <optimize.root-anderson>`\n",
      "            - 'linearmixing'     :ref:`(see here) <optimize.root-linearmixing>`\n",
      "            - 'diagbroyden'      :ref:`(see here) <optimize.root-diagbroyden>`\n",
      "            - 'excitingmixing'   :ref:`(see here) <optimize.root-excitingmixing>`\n",
      "            - 'krylov'           :ref:`(see here) <optimize.root-krylov>`\n",
      "            - 'df-sane'          :ref:`(see here) <optimize.root-dfsane>`\n",
      "\n",
      "        jac : bool or callable, optional\n",
      "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "            value of Jacobian along with the objective function. If False, the\n",
      "            Jacobian will be estimated numerically.\n",
      "            `jac` can also be a callable returning the Jacobian of `fun`. In\n",
      "            this case, it must accept the same arguments as `fun`.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g., `xtol` or `maxiter`, see\n",
      "            :obj:`show_options()` for details.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sol : OptimizeResult\n",
      "            The solution represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the algorithm exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *hybr*.\n",
      "\n",
      "        Method *hybr* uses a modification of the Powell hybrid method as\n",
      "        implemented in MINPACK [1]_.\n",
      "\n",
      "        Method *lm* solves the system of nonlinear equations in a least squares\n",
      "        sense using a modification of the Levenberg-Marquardt algorithm as\n",
      "        implemented in MINPACK [1]_.\n",
      "\n",
      "        Method *df-sane* is a derivative-free spectral method. [3]_\n",
      "\n",
      "        Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
      "        *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
      "        with backtracking or full line searches [2]_. Each method corresponds\n",
      "        to a particular Jacobian approximations.\n",
      "\n",
      "        - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
      "          known as Broyden's good method.\n",
      "        - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
      "          is known as Broyden's bad method.\n",
      "        - Method *anderson* uses (extended) Anderson mixing.\n",
      "        - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
      "          is suitable for large-scale problem.\n",
      "        - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
      "        - Method *linearmixing* uses a scalar Jacobian approximation.\n",
      "        - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
      "          approximation.\n",
      "\n",
      "        .. warning::\n",
      "\n",
      "            The algorithms implemented for methods *diagbroyden*,\n",
      "            *linearmixing* and *excitingmixing* may be useful for specific\n",
      "            problems, but whether they will work may depend strongly on the\n",
      "            problem.\n",
      "\n",
      "        .. versionadded:: 0.11.0\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
      "           1980. User Guide for MINPACK-1.\n",
      "        .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
      "           Equations. Society for Industrial and Applied Mathematics.\n",
      "           <https://archive.siam.org/books/kelley/fr16/>\n",
      "        .. [3] W. La Cruz, J.M. Martinez, M. Raydan. Math. Comp. 75, 1429 (2006).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations and its\n",
      "        jacobian.\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "\n",
      "        >>> def jac(x):\n",
      "        ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
      "        ...                       -1.5 * (x[0] - x[1])**2],\n",
      "        ...                      [-1.5 * (x[1] - x[0])**2,\n",
      "        ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
      "\n",
      "        A solution can be obtained as follows.\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
      "        >>> sol.x\n",
      "        array([ 0.8411639,  0.1588361])\n",
      "\n",
      "        **Large problem**\n",
      "\n",
      "        Suppose that we needed to solve the following integrodifferential\n",
      "        equation on the square :math:`[0,1]\\times[0,1]`:\n",
      "\n",
      "        .. math::\n",
      "\n",
      "           \\nabla^2 P = 10 \\left(\\int_0^1\\int_0^1\\cosh(P)\\,dx\\,dy\\right)^2\n",
      "\n",
      "        with :math:`P(x,1) = 1` and :math:`P=0` elsewhere on the boundary of\n",
      "        the square.\n",
      "\n",
      "        The solution can be found using the ``method='krylov'`` solver:\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "        >>> # parameters\n",
      "        >>> nx, ny = 75, 75\n",
      "        >>> hx, hy = 1./(nx-1), 1./(ny-1)\n",
      "\n",
      "        >>> P_left, P_right = 0, 0\n",
      "        >>> P_top, P_bottom = 1, 0\n",
      "\n",
      "        >>> def residual(P):\n",
      "        ...    d2x = np.zeros_like(P)\n",
      "        ...    d2y = np.zeros_like(P)\n",
      "        ...\n",
      "        ...    d2x[1:-1] = (P[2:]   - 2*P[1:-1] + P[:-2]) / hx/hx\n",
      "        ...    d2x[0]    = (P[1]    - 2*P[0]    + P_left)/hx/hx\n",
      "        ...    d2x[-1]   = (P_right - 2*P[-1]   + P[-2])/hx/hx\n",
      "        ...\n",
      "        ...    d2y[:,1:-1] = (P[:,2:] - 2*P[:,1:-1] + P[:,:-2])/hy/hy\n",
      "        ...    d2y[:,0]    = (P[:,1]  - 2*P[:,0]    + P_bottom)/hy/hy\n",
      "        ...    d2y[:,-1]   = (P_top   - 2*P[:,-1]   + P[:,-2])/hy/hy\n",
      "        ...\n",
      "        ...    return d2x + d2y - 10*np.cosh(P).mean()**2\n",
      "\n",
      "        >>> guess = np.zeros((nx, ny), float)\n",
      "        >>> sol = optimize.root(residual, guess, method='krylov')\n",
      "        >>> print('Residual: %g' % abs(residual(sol.x)).max())\n",
      "        Residual: 5.7972e-06  # may vary\n",
      "\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x, y = np.mgrid[0:1:(nx*1j), 0:1:(ny*1j)]\n",
      "        >>> plt.pcolormesh(x, y, sol.x, shading='gouraud')\n",
      "        >>> plt.colorbar()\n",
      "        >>> plt.show()\n",
      "\n",
      "    root_scalar(f, args=(), method=None, bracket=None, fprime=None, fprime2=None, x0=None, x1=None, xtol=None, rtol=None, maxiter=None, options=None)\n",
      "        Find a root of a scalar function.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            A function to find a root of.\n",
      "\n",
      "            Suppose the callable has signature ``f0(x, *my_args, **my_kwargs)``, where\n",
      "            ``my_args`` and ``my_kwargs`` are required positional and keyword arguments.\n",
      "            Rather than passing ``f0`` as the callable, wrap it to accept\n",
      "            only ``x``; e.g., pass ``fun=lambda x: f0(x, *my_args, **my_kwargs)`` as the\n",
      "            callable, where ``my_args`` (tuple) and ``my_kwargs`` (dict) have been\n",
      "            gathered before invoking this function.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its derivative(s).\n",
      "        method : str, optional\n",
      "            Type of solver.  Should be one of\n",
      "\n",
      "            - 'bisect'    :ref:`(see here) <optimize.root_scalar-bisect>`\n",
      "            - 'brentq'    :ref:`(see here) <optimize.root_scalar-brentq>`\n",
      "            - 'brenth'    :ref:`(see here) <optimize.root_scalar-brenth>`\n",
      "            - 'ridder'    :ref:`(see here) <optimize.root_scalar-ridder>`\n",
      "            - 'toms748'    :ref:`(see here) <optimize.root_scalar-toms748>`\n",
      "            - 'newton'    :ref:`(see here) <optimize.root_scalar-newton>`\n",
      "            - 'secant'    :ref:`(see here) <optimize.root_scalar-secant>`\n",
      "            - 'halley'    :ref:`(see here) <optimize.root_scalar-halley>`\n",
      "\n",
      "        bracket: A sequence of 2 floats, optional\n",
      "            An interval bracketing a root.  ``f(x, *args)`` must have different\n",
      "            signs at the two endpoints.\n",
      "        x0 : float, optional\n",
      "            Initial guess.\n",
      "        x1 : float, optional\n",
      "            A second guess.\n",
      "        fprime : bool or callable, optional\n",
      "            If `fprime` is a boolean and is True, `f` is assumed to return the\n",
      "            value of the objective function and of the derivative.\n",
      "            `fprime` can also be a callable returning the derivative of `f`. In\n",
      "            this case, it must accept the same arguments as `f`.\n",
      "        fprime2 : bool or callable, optional\n",
      "            If `fprime2` is a boolean and is True, `f` is assumed to return the\n",
      "            value of the objective function and of the\n",
      "            first and second derivatives.\n",
      "            `fprime2` can also be a callable returning the second derivative of `f`.\n",
      "            In this case, it must accept the same arguments as `f`.\n",
      "        xtol : float, optional\n",
      "            Tolerance (absolute) for termination.\n",
      "        rtol : float, optional\n",
      "            Tolerance (relative) for termination.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g., ``k``, see\n",
      "            :obj:`show_options()` for details.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sol : RootResults\n",
      "            The solution represented as a ``RootResults`` object.\n",
      "            Important attributes are: ``root`` the solution , ``converged`` a\n",
      "            boolean flag indicating if the algorithm exited successfully and\n",
      "            ``flag`` which describes the cause of the termination. See\n",
      "            `RootResults` for a description of other attributes.\n",
      "\n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        root : Find a root of a vector function.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter.\n",
      "\n",
      "        The default is to use the best method available for the situation\n",
      "        presented.\n",
      "        If a bracket is provided, it may use one of the bracketing methods.\n",
      "        If a derivative and an initial value are specified, it may\n",
      "        select one of the derivative-based methods.\n",
      "        If no method is judged applicable, it will raise an Exception.\n",
      "\n",
      "        Arguments for each method are as follows (x=required, o=optional).\n",
      "\n",
      "        +-----------------------------------------------+---+------+---------+----+----+--------+---------+------+------+---------+---------+\n",
      "        |                    method                     | f | args | bracket | x0 | x1 | fprime | fprime2 | xtol | rtol | maxiter | options |\n",
      "        +===============================================+===+======+=========+====+====+========+=========+======+======+=========+=========+\n",
      "        | :ref:`bisect <optimize.root_scalar-bisect>`   | x |  o   |    x    |    |    |        |         |  o   |  o   |    o    |   o     |\n",
      "        +-----------------------------------------------+---+------+---------+----+----+--------+---------+------+------+---------+---------+\n",
      "        | :ref:`brentq <optimize.root_scalar-brentq>`   | x |  o   |    x    |    |    |        |         |  o   |  o   |    o    |   o     |\n",
      "        +-----------------------------------------------+---+------+---------+----+----+--------+---------+------+------+---------+---------+\n",
      "        | :ref:`brenth <optimize.root_scalar-brenth>`   | x |  o   |    x    |    |    |        |         |  o   |  o   |    o    |   o     |\n",
      "        +-----------------------------------------------+---+------+---------+----+----+--------+---------+------+------+---------+---------+\n",
      "        | :ref:`ridder <optimize.root_scalar-ridder>`   | x |  o   |    x    |    |    |        |         |  o   |  o   |    o    |   o     |\n",
      "        +-----------------------------------------------+---+------+---------+----+----+--------+---------+------+------+---------+---------+\n",
      "        | :ref:`toms748 <optimize.root_scalar-toms748>` | x |  o   |    x    |    |    |        |         |  o   |  o   |    o    |   o     |\n",
      "        +-----------------------------------------------+---+------+---------+----+----+--------+---------+------+------+---------+---------+\n",
      "        | :ref:`secant <optimize.root_scalar-secant>`   | x |  o   |         | x  | o  |        |         |  o   |  o   |    o    |   o     |\n",
      "        +-----------------------------------------------+---+------+---------+----+----+--------+---------+------+------+---------+---------+\n",
      "        | :ref:`newton <optimize.root_scalar-newton>`   | x |  o   |         | x  |    |   o    |         |  o   |  o   |    o    |   o     |\n",
      "        +-----------------------------------------------+---+------+---------+----+----+--------+---------+------+------+---------+---------+\n",
      "        | :ref:`halley <optimize.root_scalar-halley>`   | x |  o   |         | x  |    |   x    |    x    |  o   |  o   |    o    |   o     |\n",
      "        +-----------------------------------------------+---+------+---------+----+----+--------+---------+------+------+---------+---------+\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "\n",
      "        Find the root of a simple cubic\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "\n",
      "        >>> def fprime(x):\n",
      "        ...     return 3*x**2\n",
      "\n",
      "        The `brentq` method takes as input a bracket\n",
      "\n",
      "        >>> sol = optimize.root_scalar(f, bracket=[0, 3], method='brentq')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 10, 11)\n",
      "\n",
      "        The `newton` method takes as input a single point and uses the\n",
      "        derivative(s).\n",
      "\n",
      "        >>> sol = optimize.root_scalar(f, x0=0.2, fprime=fprime, method='newton')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 11, 22)\n",
      "\n",
      "        The function can provide the value and derivative(s) in a single call.\n",
      "\n",
      "        >>> def f_p_pp(x):\n",
      "        ...     return (x**3 - 1), 3*x**2, 6*x\n",
      "\n",
      "        >>> sol = optimize.root_scalar(\n",
      "        ...     f_p_pp, x0=0.2, fprime=True, method='newton'\n",
      "        ... )\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 11, 11)\n",
      "\n",
      "        >>> sol = optimize.root_scalar(\n",
      "        ...     f_p_pp, x0=0.2, fprime=True, fprime2=True, method='halley'\n",
      "        ... )\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 7, 8)\n",
      "\n",
      "    rosen(x)\n",
      "        The Rosenbrock function.\n",
      "\n",
      "        The function computed is::\n",
      "\n",
      "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Rosenbrock function is to be computed.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        f : float\n",
      "            The value of the Rosenbrock function.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "\n",
      "        :func:`rosen_der`, :func:`rosen_hess`, :func:`rosen_hess_prod`\n",
      "            ..\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "\n",
      "        `rosen` has experimental support for Python Array API Standard compatible\n",
      "        backends in addition to NumPy. Please consider testing these features\n",
      "        by setting an environment variable ``SCIPY_ARRAY_API=1`` and providing\n",
      "        CuPy, PyTorch, JAX, or Dask arrays as array arguments. The following\n",
      "        combinations of backend and device (or other capability) are supported.\n",
      "\n",
      "        ====================  ====================  ====================\n",
      "        Library               CPU                   GPU\n",
      "        ====================  ====================  ====================\n",
      "        NumPy                 ✅                     n/a\n",
      "        CuPy                  n/a                   ✅\n",
      "        PyTorch               ✅                     ✅\n",
      "        JAX                   ✅                     ✅\n",
      "        Dask                  ✅                     n/a\n",
      "        ====================  ====================  ====================\n",
      "\n",
      "        See :ref:`dev-arrayapi` for more information.\n",
      "\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import rosen\n",
      "        >>> X = 0.1 * np.arange(10)\n",
      "        >>> rosen(X)\n",
      "        76.56\n",
      "\n",
      "        For higher-dimensional input ``rosen`` broadcasts.\n",
      "        In the following example, we use this to plot a 2D landscape.\n",
      "        Note that ``rosen_hess`` does not broadcast in this manner.\n",
      "\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from mpl_toolkits.mplot3d import Axes3D\n",
      "        >>> x = np.linspace(-1, 1, 50)\n",
      "        >>> X, Y = np.meshgrid(x, x)\n",
      "        >>> ax = plt.subplot(111, projection='3d')\n",
      "        >>> ax.plot_surface(X, Y, rosen([X, Y]))\n",
      "        >>> plt.show()\n",
      "\n",
      "    rosen_der(x)\n",
      "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the derivative is to be computed.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        rosen_der : (N,) ndarray\n",
      "            The gradient of the Rosenbrock function at `x`.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "\n",
      "        :func:`rosen`, :func:`rosen_hess`, :func:`rosen_hess_prod`\n",
      "            ..\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "\n",
      "        `rosen_der` has experimental support for Python Array API Standard compatible\n",
      "        backends in addition to NumPy. Please consider testing these features\n",
      "        by setting an environment variable ``SCIPY_ARRAY_API=1`` and providing\n",
      "        CuPy, PyTorch, JAX, or Dask arrays as array arguments. The following\n",
      "        combinations of backend and device (or other capability) are supported.\n",
      "\n",
      "        ====================  ====================  ====================\n",
      "        Library               CPU                   GPU\n",
      "        ====================  ====================  ====================\n",
      "        NumPy                 ✅                     n/a\n",
      "        CuPy                  n/a                   ✅\n",
      "        PyTorch               ✅                     ✅\n",
      "        JAX                   ⛔                     ⛔\n",
      "        Dask                  ✅                     n/a\n",
      "        ====================  ====================  ====================\n",
      "\n",
      "        See :ref:`dev-arrayapi` for more information.\n",
      "\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import rosen_der\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> rosen_der(X)\n",
      "        array([ -2. ,  10.6,  15.6,  13.4,   6.4,  -3. , -12.4, -19.4,  62. ])\n",
      "\n",
      "    rosen_hess(x)\n",
      "        The Hessian matrix of the Rosenbrock function.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x`.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "\n",
      "        :func:`rosen`, :func:`rosen_der`, :func:`rosen_hess_prod`\n",
      "            ..\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "\n",
      "        `rosen_hess` has experimental support for Python Array API Standard compatible\n",
      "        backends in addition to NumPy. Please consider testing these features\n",
      "        by setting an environment variable ``SCIPY_ARRAY_API=1`` and providing\n",
      "        CuPy, PyTorch, JAX, or Dask arrays as array arguments. The following\n",
      "        combinations of backend and device (or other capability) are supported.\n",
      "\n",
      "        ====================  ====================  ====================\n",
      "        Library               CPU                   GPU\n",
      "        ====================  ====================  ====================\n",
      "        NumPy                 ✅                     n/a\n",
      "        CuPy                  n/a                   ✅\n",
      "        PyTorch               ✅                     ✅\n",
      "        JAX                   ⛔                     ⛔\n",
      "        Dask                  ✅                     n/a\n",
      "        ====================  ====================  ====================\n",
      "\n",
      "        See :ref:`dev-arrayapi` for more information.\n",
      "\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import rosen_hess\n",
      "        >>> X = 0.1 * np.arange(4)\n",
      "        >>> rosen_hess(X)\n",
      "        array([[-38.,   0.,   0.,   0.],\n",
      "               [  0., 134., -40.,   0.],\n",
      "               [  0., -40., 130., -80.],\n",
      "               [  0.,   0., -80., 200.]])\n",
      "\n",
      "    rosen_hess_prod(x, p)\n",
      "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        p : array_like\n",
      "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess_prod : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
      "            by the vector `p`.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "\n",
      "        :func:`rosen`, :func:`rosen_der`, :func:`rosen_hess`\n",
      "            ..\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "\n",
      "        `rosen_hess_prod` has experimental support for Python Array API Standard compatible\n",
      "        backends in addition to NumPy. Please consider testing these features\n",
      "        by setting an environment variable ``SCIPY_ARRAY_API=1`` and providing\n",
      "        CuPy, PyTorch, JAX, or Dask arrays as array arguments. The following\n",
      "        combinations of backend and device (or other capability) are supported.\n",
      "\n",
      "        ====================  ====================  ====================\n",
      "        Library               CPU                   GPU\n",
      "        ====================  ====================  ====================\n",
      "        NumPy                 ✅                     n/a\n",
      "        CuPy                  n/a                   ✅\n",
      "        PyTorch               ✅                     ✅\n",
      "        JAX                   ⛔                     ⛔\n",
      "        Dask                  ✅                     n/a\n",
      "        ====================  ====================  ====================\n",
      "\n",
      "        See :ref:`dev-arrayapi` for more information.\n",
      "\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import rosen_hess_prod\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> p = 0.5 * np.arange(9)\n",
      "        >>> rosen_hess_prod(X, p)\n",
      "        array([  -0.,   27.,  -10.,  -95., -192., -265., -278., -195., -180.])\n",
      "\n",
      "    shgo(func, bounds, args=(), constraints=None, n=100, iters=1, callback=None, minimizer_kwargs=None, options=None, sampling_method='simplicial', *, workers=1)\n",
      "        Finds the global minimum of a function using SHG optimization.\n",
      "\n",
      "        SHGO stands for \"simplicial homology global optimization\".\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized.  Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence or `Bounds`\n",
      "            Bounds for variables. There are two ways to specify the bounds:\n",
      "\n",
      "            1. Instance of `Bounds` class.\n",
      "            2. Sequence of ``(min, max)`` pairs for each element in `x`.\n",
      "\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify the\n",
      "            objective function.\n",
      "        constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "            Constraints definition. Only for COBYLA, COBYQA, SLSQP and trust-constr.\n",
      "            See the tutorial [5]_ for further details on specifying constraints.\n",
      "\n",
      "            .. note::\n",
      "\n",
      "               Only COBYLA, COBYQA, SLSQP, and trust-constr local minimize methods\n",
      "               currently support constraint arguments. If the ``constraints``\n",
      "               sequence used in the local optimization problem is not defined in\n",
      "               ``minimizer_kwargs`` and a constrained method is used then the\n",
      "               global ``constraints`` will be used.\n",
      "               (Defining a ``constraints`` sequence in ``minimizer_kwargs``\n",
      "               means that ``constraints`` will not be added so if equality\n",
      "               constraints and so forth need to be added then the inequality\n",
      "               functions in ``constraints`` need to be added to\n",
      "               ``minimizer_kwargs`` too).\n",
      "               COBYLA only supports inequality constraints.\n",
      "\n",
      "            .. versionchanged:: 1.11.0\n",
      "\n",
      "               ``constraints`` accepts `NonlinearConstraint`, `LinearConstraint`.\n",
      "\n",
      "        n : int, optional\n",
      "            Number of sampling points used in the construction of the simplicial\n",
      "            complex. For the default ``simplicial`` sampling method 2**dim + 1\n",
      "            sampling points are generated instead of the default ``n=100``. For all\n",
      "            other specified values `n` sampling points are generated. For\n",
      "            ``sobol``, ``halton`` and other arbitrary `sampling_methods` ``n=100`` or\n",
      "            another specified number of sampling points are generated.\n",
      "        iters : int, optional\n",
      "            Number of iterations used in the construction of the simplicial\n",
      "            complex. Default is 1.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the minimizer\n",
      "            ``scipy.optimize.minimize``. Some important options could be:\n",
      "\n",
      "            method : str\n",
      "                The minimization method. If not given, chosen to be one of\n",
      "                BFGS, L-BFGS-B, SLSQP, depending on whether or not the\n",
      "                problem has constraints or bounds.\n",
      "            args : tuple\n",
      "                Extra arguments passed to the objective function (``func``) and\n",
      "                its derivatives (Jacobian, Hessian).\n",
      "            options : dict, optional\n",
      "                Note that by default the tolerance is specified as\n",
      "                ``{ftol: 1e-12}``\n",
      "\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. Many of the options specified for the\n",
      "            global routine are also passed to the ``scipy.optimize.minimize``\n",
      "            routine. The options that are also passed to the local routine are\n",
      "            marked with \"(L)\".\n",
      "\n",
      "            Stopping criteria, the algorithm will terminate if any of the specified\n",
      "            criteria are met. However, the default algorithm does not require any\n",
      "            to be specified:\n",
      "\n",
      "            maxfev : int (L)\n",
      "                Maximum number of function evaluations in the feasible domain.\n",
      "                (Note only methods that support this option will terminate\n",
      "                the routine at precisely exact specified value. Otherwise the\n",
      "                criterion will only terminate during a global iteration)\n",
      "            f_min : float\n",
      "                Specify the minimum objective function value, if it is known.\n",
      "            f_tol : float\n",
      "                Precision goal for the value of f in the stopping\n",
      "                criterion. Note that the global routine will also\n",
      "                terminate if a sampling point in the global routine is\n",
      "                within this tolerance.\n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            maxev : int\n",
      "                Maximum number of sampling evaluations to perform (includes\n",
      "                searching in infeasible points).\n",
      "            maxtime : float\n",
      "                Maximum processing runtime allowed\n",
      "            minhgrd : int\n",
      "                Minimum homology group rank differential. The homology group of the\n",
      "                objective function is calculated (approximately) during every\n",
      "                iteration. The rank of this group has a one-to-one correspondence\n",
      "                with the number of locally convex subdomains in the objective\n",
      "                function (after adequate sampling points each of these subdomains\n",
      "                contain a unique global minimum). If the difference in the hgr is 0\n",
      "                between iterations for ``maxhgrd`` specified iterations the\n",
      "                algorithm will terminate.\n",
      "\n",
      "            Objective function knowledge:\n",
      "\n",
      "            symmetry : list or bool\n",
      "                Specify if the objective function contains symmetric variables.\n",
      "                The search space (and therefore performance) is decreased by up to\n",
      "                O(n!) times in the fully symmetric case. If `True` is specified\n",
      "                then all variables will be set symmetric to the first variable.\n",
      "                Default\n",
      "                is set to False.\n",
      "\n",
      "                E.g.  f(x) = (x_1 + x_2 + x_3) + (x_4)**2 + (x_5)**2 + (x_6)**2\n",
      "\n",
      "                In this equation x_2 and x_3 are symmetric to x_1, while x_5 and\n",
      "                x_6 are symmetric to x_4, this can be specified to the solver as::\n",
      "\n",
      "                    symmetry = [0,  # Variable 1\n",
      "                                0,  # symmetric to variable 1\n",
      "                                0,  # symmetric to variable 1\n",
      "                                3,  # Variable 4\n",
      "                                3,  # symmetric to variable 4\n",
      "                                3,  # symmetric to variable 4\n",
      "                                ]\n",
      "\n",
      "            jac : bool or callable, optional\n",
      "                Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
      "                Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. If ``jac`` is a\n",
      "                boolean and is True, ``fun`` is assumed to return the gradient\n",
      "                along with the objective function. If False, the gradient will be\n",
      "                estimated numerically. ``jac`` can also be a callable returning the\n",
      "                gradient of the objective. In this case, it must accept the same\n",
      "                arguments as ``fun``. (Passed to `scipy.optimize.minimize`\n",
      "                automatically)\n",
      "\n",
      "            hess, hessp : callable, optional\n",
      "                Hessian (matrix of second-order derivatives) of objective function\n",
      "                or Hessian of objective function times an arbitrary vector p.\n",
      "                Only for Newton-CG, dogleg, trust-ncg. Only one of ``hessp`` or\n",
      "                ``hess`` needs to be given. If ``hess`` is provided, then\n",
      "                ``hessp`` will be ignored. If neither ``hess`` nor ``hessp`` is\n",
      "                provided, then the Hessian product will be approximated using\n",
      "                finite differences on ``jac``. ``hessp`` must compute the Hessian\n",
      "                times an arbitrary vector. (Passed to `scipy.optimize.minimize`\n",
      "                automatically)\n",
      "\n",
      "            Algorithm settings:\n",
      "\n",
      "            minimize_every_iter : bool\n",
      "                If True then promising global sampling points will be passed to a\n",
      "                local minimization routine every iteration. If True then only the\n",
      "                final minimizer pool will be run. Defaults to True.\n",
      "\n",
      "            local_iter : int\n",
      "                Only evaluate a few of the best minimizer pool candidates every\n",
      "                iteration. If False all potential points are passed to the local\n",
      "                minimization routine.\n",
      "\n",
      "            infty_constraints : bool\n",
      "                If True then any sampling points generated which are outside will\n",
      "                the feasible domain will be saved and given an objective function\n",
      "                value of ``inf``. If False then these points will be discarded.\n",
      "                Using this functionality could lead to higher performance with\n",
      "                respect to function evaluations before the global minimum is found,\n",
      "                specifying False will use less memory at the cost of a slight\n",
      "                decrease in performance. Defaults to True.\n",
      "\n",
      "            Feedback:\n",
      "\n",
      "            disp : bool (L)\n",
      "                Set to True to print convergence messages.\n",
      "\n",
      "        sampling_method : str or function, optional\n",
      "            Current built in sampling method options are ``halton``, ``sobol`` and\n",
      "            ``simplicial``. The default ``simplicial`` provides\n",
      "            the theoretical guarantee of convergence to the global minimum in\n",
      "            finite time. ``halton`` and ``sobol`` method are faster in terms of\n",
      "            sampling point generation at the cost of the loss of\n",
      "            guaranteed convergence. It is more appropriate for most \"easier\"\n",
      "            problems where the convergence is relatively fast.\n",
      "            User defined sampling functions must accept two arguments of ``n``\n",
      "            sampling points of dimension ``dim`` per call and output an array of\n",
      "            sampling points with shape `n x dim`.\n",
      "\n",
      "        workers : int or map-like callable, optional\n",
      "            Sample and run the local serial minimizations in parallel.\n",
      "            Supply -1 to use all available CPU cores, or an int to use\n",
      "            that many Processes (uses `multiprocessing.Pool <multiprocessing>`).\n",
      "\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for parallel evaluation.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            Requires that `func` be pickleable.\n",
      "\n",
      "            .. versionadded:: 1.11.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are:\n",
      "            ``x`` the solution array corresponding to the global minimum,\n",
      "            ``fun`` the function output at the global solution,\n",
      "            ``xl`` an ordered list of local minima solutions,\n",
      "            ``funl`` the function output at the corresponding local solutions,\n",
      "            ``success`` a Boolean flag indicating if the optimizer exited\n",
      "            successfully,\n",
      "            ``message`` which describes the cause of the termination,\n",
      "            ``nfev`` the total number of objective function evaluations including\n",
      "            the sampling calls,\n",
      "            ``nlfev`` the total number of objective function evaluations\n",
      "            culminating from all local search optimizations,\n",
      "            ``nit`` number of iterations performed by the global routine.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Global optimization using simplicial homology global optimization [1]_.\n",
      "        Appropriate for solving general purpose NLP and blackbox optimization\n",
      "        problems to global optimality (low-dimensional problems).\n",
      "\n",
      "        In general, the optimization problems are of the form::\n",
      "\n",
      "            minimize f(x) subject to\n",
      "\n",
      "            g_i(x) >= 0,  i = 1,...,m\n",
      "            h_j(x)  = 0,  j = 1,...,p\n",
      "\n",
      "        where x is a vector of one or more variables. ``f(x)`` is the objective\n",
      "        function ``R^n -> R``, ``g_i(x)`` are the inequality constraints, and\n",
      "        ``h_j(x)`` are the equality constraints.\n",
      "\n",
      "        Optionally, the lower and upper bounds for each element in x can also be\n",
      "        specified using the `bounds` argument.\n",
      "\n",
      "        While most of the theoretical advantages of SHGO are only proven for when\n",
      "        ``f(x)`` is a Lipschitz smooth function, the algorithm is also proven to\n",
      "        converge to the global optimum for the more general case where ``f(x)`` is\n",
      "        non-continuous, non-convex and non-smooth, if the default sampling method\n",
      "        is used [1]_.\n",
      "\n",
      "        The local search method may be specified using the ``minimizer_kwargs``\n",
      "        parameter which is passed on to ``scipy.optimize.minimize``. By default,\n",
      "        the ``SLSQP`` method is used. In general, it is recommended to use the\n",
      "        ``SLSQP``, ``COBYLA``, or ``COBYQA`` local minimization if inequality\n",
      "        constraints are defined for the problem since the other methods do not use\n",
      "        constraints.\n",
      "\n",
      "        The ``halton`` and ``sobol`` method points are generated using\n",
      "        `scipy.stats.qmc`. Any other QMC method could be used.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Endres, SC, Sandrock, C, Focke, WW (2018) \"A simplicial homology\n",
      "               algorithm for lipschitz optimisation\", Journal of Global\n",
      "               Optimization.\n",
      "        .. [2] Joe, SW and Kuo, FY (2008) \"Constructing Sobol' sequences with\n",
      "               better  two-dimensional projections\", SIAM J. Sci. Comput. 30,\n",
      "               2635-2654.\n",
      "        .. [3] Hock, W and Schittkowski, K (1981) \"Test examples for nonlinear\n",
      "               programming codes\", Lecture Notes in Economics and Mathematical\n",
      "               Systems, 187. Springer-Verlag, New York.\n",
      "               http://www.ai7.uni-bayreuth.de/test_problem_coll.pdf\n",
      "        .. [4] Wales, DJ (2015) \"Perspective: Insight into reaction coordinates and\n",
      "               dynamics from the potential energy landscape\",\n",
      "               Journal of Chemical Physics, 142(13), 2015.\n",
      "        .. [5] https://docs.scipy.org/doc/scipy/tutorial/optimize.html#constrained-minimization-of-multivariate-scalar-functions-minimize\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        First consider the problem of minimizing the Rosenbrock function, `rosen`:\n",
      "\n",
      "        >>> from scipy.optimize import rosen, shgo\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = shgo(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 2.920392374190081e-18)\n",
      "\n",
      "        Note that bounds determine the dimensionality of the objective\n",
      "        function and is therefore a required input, however you can specify\n",
      "        empty bounds using ``None`` or objects like ``np.inf`` which will be\n",
      "        converted to large float numbers.\n",
      "\n",
      "        >>> bounds = [(None, None), ]*4\n",
      "        >>> result = shgo(rosen, bounds)\n",
      "        >>> result.x\n",
      "        array([0.99999851, 0.99999704, 0.99999411, 0.9999882 ])\n",
      "\n",
      "        Next, we consider the Eggholder function, a problem with several local\n",
      "        minima and one global minimum. We will demonstrate the use of arguments and\n",
      "        the capabilities of `shgo`.\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization)\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> def eggholder(x):\n",
      "        ...     return (-(x[1] + 47.0)\n",
      "        ...             * np.sin(np.sqrt(abs(x[0]/2.0 + (x[1] + 47.0))))\n",
      "        ...             - x[0] * np.sin(np.sqrt(abs(x[0] - (x[1] + 47.0))))\n",
      "        ...             )\n",
      "        ...\n",
      "        >>> bounds = [(-512, 512), (-512, 512)]\n",
      "\n",
      "        `shgo` has built-in low discrepancy sampling sequences. First, we will\n",
      "        input 64 initial sampling points of the *Sobol'* sequence:\n",
      "\n",
      "        >>> result = shgo(eggholder, bounds, n=64, sampling_method='sobol')\n",
      "        >>> result.x, result.fun\n",
      "        (array([512.        , 404.23180824]), -959.6406627208397)\n",
      "\n",
      "        `shgo` also has a return for any other local minima that was found, these\n",
      "        can be called using:\n",
      "\n",
      "        >>> result.xl\n",
      "        array([[ 512.        ,  404.23180824],\n",
      "               [ 283.0759062 , -487.12565635],\n",
      "               [-294.66820039, -462.01964031],\n",
      "               [-105.87688911,  423.15323845],\n",
      "               [-242.97926   ,  274.38030925],\n",
      "               [-506.25823477,    6.3131022 ],\n",
      "               [-408.71980731, -156.10116949],\n",
      "               [ 150.23207937,  301.31376595],\n",
      "               [  91.00920901, -391.283763  ],\n",
      "               [ 202.89662724, -269.38043241],\n",
      "               [ 361.66623976, -106.96493868],\n",
      "               [-219.40612786, -244.06020508]])\n",
      "\n",
      "        >>> result.funl\n",
      "        array([-959.64066272, -718.16745962, -704.80659592, -565.99778097,\n",
      "               -559.78685655, -557.36868733, -507.87385942, -493.9605115 ,\n",
      "               -426.48799655, -421.15571437, -419.31194957, -410.98477763])\n",
      "\n",
      "        These results are useful in applications where there are many global minima\n",
      "        and the values of other global minima are desired or where the local minima\n",
      "        can provide insight into the system (for example morphologies\n",
      "        in physical chemistry [4]_).\n",
      "\n",
      "        If we want to find a larger number of local minima, we can increase the\n",
      "        number of sampling points or the number of iterations. We'll increase the\n",
      "        number of sampling points to 64 and the number of iterations from the\n",
      "        default of 1 to 3. Using ``simplicial`` this would have given us\n",
      "        64 x 3 = 192 initial sampling points.\n",
      "\n",
      "        >>> result_2 = shgo(eggholder,\n",
      "        ...                 bounds, n=64, iters=3, sampling_method='sobol')\n",
      "        >>> len(result.xl), len(result_2.xl)\n",
      "        (12, 23)\n",
      "\n",
      "        Note the difference between, e.g., ``n=192, iters=1`` and ``n=64,\n",
      "        iters=3``.\n",
      "        In the first case the promising points contained in the minimiser pool\n",
      "        are processed only once. In the latter case it is processed every 64\n",
      "        sampling points for a total of 3 times.\n",
      "\n",
      "        To demonstrate solving problems with non-linear constraints consider the\n",
      "        following example from Hock and Schittkowski problem 73 (cattle-feed)\n",
      "        [3]_::\n",
      "\n",
      "            minimize: f = 24.55 * x_1 + 26.75 * x_2 + 39 * x_3 + 40.50 * x_4\n",
      "\n",
      "            subject to: 2.3 * x_1 + 5.6 * x_2 + 11.1 * x_3 + 1.3 * x_4 - 5    >= 0,\n",
      "\n",
      "                        12 * x_1 + 11.9 * x_2 + 41.8 * x_3 + 52.1 * x_4 - 21\n",
      "                            -1.645 * sqrt(0.28 * x_1**2 + 0.19 * x_2**2 +\n",
      "                                          20.5 * x_3**2 + 0.62 * x_4**2)      >= 0,\n",
      "\n",
      "                        x_1 + x_2 + x_3 + x_4 - 1                             == 0,\n",
      "\n",
      "                        1 >= x_i >= 0 for all i\n",
      "\n",
      "        The approximate answer given in [3]_ is::\n",
      "\n",
      "            f([0.6355216, -0.12e-11, 0.3127019, 0.05177655]) = 29.894378\n",
      "\n",
      "        >>> def f(x):  # (cattle-feed)\n",
      "        ...     return 24.55*x[0] + 26.75*x[1] + 39*x[2] + 40.50*x[3]\n",
      "        ...\n",
      "        >>> def g1(x):\n",
      "        ...     return 2.3*x[0] + 5.6*x[1] + 11.1*x[2] + 1.3*x[3] - 5  # >=0\n",
      "        ...\n",
      "        >>> def g2(x):\n",
      "        ...     return (12*x[0] + 11.9*x[1] +41.8*x[2] + 52.1*x[3] - 21\n",
      "        ...             - 1.645 * np.sqrt(0.28*x[0]**2 + 0.19*x[1]**2\n",
      "        ...                             + 20.5*x[2]**2 + 0.62*x[3]**2)\n",
      "        ...             ) # >=0\n",
      "        ...\n",
      "        >>> def h1(x):\n",
      "        ...     return x[0] + x[1] + x[2] + x[3] - 1  # == 0\n",
      "        ...\n",
      "        >>> cons = ({'type': 'ineq', 'fun': g1},\n",
      "        ...         {'type': 'ineq', 'fun': g2},\n",
      "        ...         {'type': 'eq', 'fun': h1})\n",
      "        >>> bounds = [(0, 1.0),]*4\n",
      "        >>> res = shgo(f, bounds, n=150, constraints=cons)\n",
      "        >>> res\n",
      "         message: Optimization terminated successfully.\n",
      "         success: True\n",
      "             fun: 29.894378159142136\n",
      "            funl: [ 2.989e+01]\n",
      "               x: [ 6.355e-01  1.137e-13  3.127e-01  5.178e-02] # may vary\n",
      "              xl: [[ 6.355e-01  1.137e-13  3.127e-01  5.178e-02]] # may vary\n",
      "             nit: 1\n",
      "            nfev: 142 # may vary\n",
      "           nlfev: 35 # may vary\n",
      "           nljev: 5\n",
      "           nlhev: 0\n",
      "\n",
      "        >>> g1(res.x), g2(res.x), h1(res.x)\n",
      "        (-5.062616992290714e-14, -2.9594104944408173e-12, 0.0)\n",
      "\n",
      "    show_options(solver=None, method=None, disp=True)\n",
      "        Show documentation for additional options of optimization solvers.\n",
      "\n",
      "        These are method-specific options that can be supplied through the\n",
      "        ``options`` dict.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        solver : str\n",
      "            Type of optimization solver. One of 'minimize', 'minimize_scalar',\n",
      "            'root', 'root_scalar', 'linprog', or 'quadratic_assignment'.\n",
      "        method : str, optional\n",
      "            If not given, shows all methods of the specified solver. Otherwise,\n",
      "            show only the options for the specified method. Valid values\n",
      "            corresponds to methods' names of respective solver (e.g., 'BFGS' for\n",
      "            'minimize').\n",
      "        disp : bool, optional\n",
      "            Whether to print the result rather than returning it.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        text\n",
      "            Either None (for disp=True) or the text string (disp=False)\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The solver-specific methods are:\n",
      "\n",
      "        `scipy.optimize.minimize`\n",
      "\n",
      "        - :ref:`Nelder-Mead <optimize.minimize-neldermead>`\n",
      "        - :ref:`Powell      <optimize.minimize-powell>`\n",
      "        - :ref:`CG          <optimize.minimize-cg>`\n",
      "        - :ref:`BFGS        <optimize.minimize-bfgs>`\n",
      "        - :ref:`Newton-CG   <optimize.minimize-newtoncg>`\n",
      "        - :ref:`L-BFGS-B    <optimize.minimize-lbfgsb>`\n",
      "        - :ref:`TNC         <optimize.minimize-tnc>`\n",
      "        - :ref:`COBYLA      <optimize.minimize-cobyla>`\n",
      "        - :ref:`COBYQA      <optimize.minimize-cobyqa>`\n",
      "        - :ref:`SLSQP       <optimize.minimize-slsqp>`\n",
      "        - :ref:`dogleg      <optimize.minimize-dogleg>`\n",
      "        - :ref:`trust-ncg   <optimize.minimize-trustncg>`\n",
      "\n",
      "        `scipy.optimize.root`\n",
      "\n",
      "        - :ref:`hybr              <optimize.root-hybr>`\n",
      "        - :ref:`lm                <optimize.root-lm>`\n",
      "        - :ref:`broyden1          <optimize.root-broyden1>`\n",
      "        - :ref:`broyden2          <optimize.root-broyden2>`\n",
      "        - :ref:`anderson          <optimize.root-anderson>`\n",
      "        - :ref:`linearmixing      <optimize.root-linearmixing>`\n",
      "        - :ref:`diagbroyden       <optimize.root-diagbroyden>`\n",
      "        - :ref:`excitingmixing    <optimize.root-excitingmixing>`\n",
      "        - :ref:`krylov            <optimize.root-krylov>`\n",
      "        - :ref:`df-sane           <optimize.root-dfsane>`\n",
      "\n",
      "        `scipy.optimize.minimize_scalar`\n",
      "\n",
      "        - :ref:`brent       <optimize.minimize_scalar-brent>`\n",
      "        - :ref:`golden      <optimize.minimize_scalar-golden>`\n",
      "        - :ref:`bounded     <optimize.minimize_scalar-bounded>`\n",
      "\n",
      "        `scipy.optimize.root_scalar`\n",
      "\n",
      "        - :ref:`bisect  <optimize.root_scalar-bisect>`\n",
      "        - :ref:`brentq  <optimize.root_scalar-brentq>`\n",
      "        - :ref:`brenth  <optimize.root_scalar-brenth>`\n",
      "        - :ref:`ridder  <optimize.root_scalar-ridder>`\n",
      "        - :ref:`toms748 <optimize.root_scalar-toms748>`\n",
      "        - :ref:`newton  <optimize.root_scalar-newton>`\n",
      "        - :ref:`secant  <optimize.root_scalar-secant>`\n",
      "        - :ref:`halley  <optimize.root_scalar-halley>`\n",
      "\n",
      "        `scipy.optimize.linprog`\n",
      "\n",
      "        - :ref:`simplex           <optimize.linprog-simplex>`\n",
      "        - :ref:`interior-point    <optimize.linprog-interior-point>`\n",
      "        - :ref:`revised simplex   <optimize.linprog-revised_simplex>`\n",
      "        - :ref:`highs             <optimize.linprog-highs>`\n",
      "        - :ref:`highs-ds          <optimize.linprog-highs-ds>`\n",
      "        - :ref:`highs-ipm         <optimize.linprog-highs-ipm>`\n",
      "\n",
      "        `scipy.optimize.quadratic_assignment`\n",
      "\n",
      "        - :ref:`faq             <optimize.qap-faq>`\n",
      "        - :ref:`2opt            <optimize.qap-2opt>`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        We can print documentations of a solver in stdout:\n",
      "\n",
      "        >>> from scipy.optimize import show_options\n",
      "        >>> show_options(solver=\"minimize\")\n",
      "        ...\n",
      "\n",
      "        Specifying a method is possible:\n",
      "\n",
      "        >>> show_options(solver=\"minimize\", method=\"Nelder-Mead\")\n",
      "        ...\n",
      "\n",
      "        We can also get the documentations as a string:\n",
      "\n",
      "        >>> show_options(solver=\"minimize\", method=\"Nelder-Mead\", disp=False)\n",
      "        Minimization of scalar function of one or more variables using the ...\n",
      "\n",
      "    toms748(f, a, b, args=(), k=1, xtol=2e-12, rtol=np.float64(8.881784197001252e-16), maxiter=100, full_output=False, disp=True)\n",
      "        Find a root using TOMS Algorithm 748 method.\n",
      "\n",
      "        Implements the Algorithm 748 method of Alefeld, Potro and Shi to find a\n",
      "        root of the function `f` on the interval ``[a , b]``, where ``f(a)`` and\n",
      "        `f(b)` must have opposite signs.\n",
      "\n",
      "        It uses a mixture of inverse cubic interpolation and\n",
      "        \"Newton-quadratic\" steps. [APS1995].\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a scalar. The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)`\n",
      "            have opposite signs.\n",
      "        a : scalar,\n",
      "            lower boundary of the search interval\n",
      "        b : scalar,\n",
      "            upper boundary of the search interval\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``f(x, *args)``.\n",
      "        k : int, optional\n",
      "            The number of Newton quadratic steps to perform each\n",
      "            iteration. ``k>=1``.\n",
      "        xtol : scalar, optional\n",
      "            The computed root ``x0`` will satisfy ``np.isclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be positive.\n",
      "        rtol : scalar, optional\n",
      "            The computed root ``x0`` will satisfy ``np.isclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in the `RootResults`\n",
      "            return object.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        root : float\n",
      "            Approximate root of `f`\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, ridder, bisect, newton\n",
      "        fsolve : find roots in N dimensions.\n",
      "        elementwise.find_root : efficient elementwise 1-D root-finder\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.\n",
      "        Algorithm 748 with ``k=2`` is asymptotically the most efficient\n",
      "        algorithm known for finding roots of a four times continuously\n",
      "        differentiable function.\n",
      "        In contrast with Brent's algorithm, which may only decrease the length of\n",
      "        the enclosing bracket on the last step, Algorithm 748 decreases it each\n",
      "        iteration with the same asymptotic efficiency as it finds the root.\n",
      "\n",
      "        For easy statement of efficiency indices, assume that `f` has 4\n",
      "        continuous deriviatives.\n",
      "        For ``k=1``, the convergence order is at least 2.7, and with about\n",
      "        asymptotically 2 function evaluations per iteration, the efficiency\n",
      "        index is approximately 1.65.\n",
      "        For ``k=2``, the order is about 4.6 with asymptotically 3 function\n",
      "        evaluations per iteration, and the efficiency index 1.66.\n",
      "        For higher values of `k`, the efficiency index approaches\n",
      "        the kth root of ``(3k-2)``, hence ``k=1`` or ``k=2`` are\n",
      "        usually appropriate.\n",
      "\n",
      "        As mentioned in the parameter documentation, the computed root ``x0`` will\n",
      "        satisfy ``np.isclose(x, x0, atol=xtol, rtol=rtol)``, where ``x`` is the\n",
      "        exact root. In equation form, this terminating condition is ``abs(x - x0)\n",
      "        <= xtol + rtol * abs(x0)``.\n",
      "\n",
      "        The default value ``xtol=2e-12`` may lead to surprising behavior if one\n",
      "        expects `toms748` to always compute roots with relative error near machine\n",
      "        precision. Care should be taken to select `xtol` for the use case at hand.\n",
      "        Setting ``xtol=5e-324``, the smallest subnormal number, will ensure the\n",
      "        highest level of accuracy. Larger values of `xtol` may be useful for saving\n",
      "        function evaluations when a root is at or near zero in applications where\n",
      "        the tiny absolute differences available between floating point numbers near\n",
      "        zero are not meaningful.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [APS1995]\n",
      "           Alefeld, G. E. and Potra, F. A. and Shi, Yixun,\n",
      "           *Algorithm 748: Enclosing Zeros of Continuous Functions*,\n",
      "           ACM Trans. Math. Softw. Volume 221(1995)\n",
      "           doi = {10.1145/210089.210111}\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "\n",
      "        >>> from scipy import optimize\n",
      "        >>> root, results = optimize.toms748(f, 0, 2, full_output=True)\n",
      "        >>> root\n",
      "        1.0\n",
      "        >>> results\n",
      "              converged: True\n",
      "                   flag: converged\n",
      "         function_calls: 11\n",
      "             iterations: 5\n",
      "                   root: 1.0\n",
      "                 method: toms748\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BFGS', 'Bounds', 'BroydenFirst', 'HessianUpdateStrategy', ...\n",
      "\n",
      "FILE\n",
      "    /Users/jason.debacker/anaconda3/envs/usitc-env/lib/python3.12/site-packages/scipy/optimize/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usitc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
